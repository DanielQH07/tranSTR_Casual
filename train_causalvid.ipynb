{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a98c20",
   "metadata": {},
   "source": [
    "## 1. üì¶ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers einops h5py wandb\n",
    "\n",
    "# Login to W&B (uncomment v√† th√™m API key c·ªßa b·∫°n)\n",
    "import wandb\n",
    "wandb.login()  # S·∫Ω prompt nh·∫≠p API key n·∫øu ch∆∞a login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503597db",
   "metadata": {},
   "source": [
    "## 1.5 üìù Patch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f177574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch DataLoader.py ƒë·ªÉ x·ª≠ l√Ω dimension mismatch\n",
    "# Ch·∫°y cell n√†y tr∆∞·ªõc khi import DataLoader\n",
    "\n",
    "patch_code = '''\n",
    "import torch\n",
    "import os\n",
    "import h5py\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle as pkl\n",
    "from torch.utils import data\n",
    "from utils.util import load_file, pause, transform_bb, pkload\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "\n",
    "class VideoQADataset(Dataset):\n",
    "    \"\"\"\n",
    "    DataLoader cho CausalVidQA v·ªõi output format t∆∞∆°ng th√≠ch NextQA\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, split, n_query=5, obj_num=1, \n",
    "                 sample_list_path=None,\n",
    "                 video_feature_path=None,\n",
    "                 text_annotation_path=None,\n",
    "                 qtype=-1,\n",
    "                 max_samples=None):\n",
    "        super(VideoQADataset, self).__init__()\n",
    "        \n",
    "        self.split = split\n",
    "        self.mc = n_query\n",
    "        self.obj_num = obj_num\n",
    "        self.qtype = qtype\n",
    "        self.video_feature_path = video_feature_path\n",
    "        self.text_annotation_path = text_annotation_path\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "        # Load video ids for this split\n",
    "        split_name = split\n",
    "        if split == 'val':\n",
    "            split_file = osp.join(sample_list_path, 'val.pkl')\n",
    "            if not osp.exists(split_file):\n",
    "                split_file = osp.join(sample_list_path, 'valid.pkl')\n",
    "        else:\n",
    "            split_file = osp.join(sample_list_path, f'{split}.pkl')\n",
    "        \n",
    "        if not osp.exists(split_file):\n",
    "            raise FileNotFoundError(f\"Split file not found: {split_file}\")\n",
    "        \n",
    "        self.vids = pkload(split_file)\n",
    "        \n",
    "        if self.vids is None:\n",
    "            raise ValueError(f\"Failed to load split file: {split_file}\")\n",
    "        \n",
    "        if max_samples is not None and max_samples > 0:\n",
    "            self.vids = self.vids[:max_samples]\n",
    "            print(f\"Limited to {len(self.vids)} videos (max_samples={max_samples})\")\n",
    "        else:\n",
    "            print(f\"Loaded {len(self.vids)} videos from {split_file}\")\n",
    "        \n",
    "        # Load video feature index mapping\n",
    "        idx2vid_file = osp.join(video_feature_path, 'idx2vid.pkl')\n",
    "        vf_info = pkload(idx2vid_file)\n",
    "        self.vf_info = dict()\n",
    "        for idx, vid in enumerate(vf_info):\n",
    "            if vid in self.vids:\n",
    "                self.vf_info[vid] = idx\n",
    "        \n",
    "        # Load appearance features\n",
    "        app_file = osp.join(video_feature_path, 'appearance_feat.h5')\n",
    "        print(f'Loading {app_file}...')\n",
    "        self.app_feats = dict()\n",
    "        with h5py.File(app_file, 'r') as fp:\n",
    "            feats = fp['resnet_features']\n",
    "            for vid, idx in self.vf_info.items():\n",
    "                self.app_feats[vid] = feats[idx][...]\n",
    "        \n",
    "        # Load motion features\n",
    "        mot_file = osp.join(video_feature_path, 'motion_feat.h5')\n",
    "        print(f'Loading {mot_file}...')\n",
    "        self.mot_feats = dict()\n",
    "        with h5py.File(mot_file, 'r') as fp:\n",
    "            feats = fp['resnet_features']\n",
    "            for vid, idx in self.vf_info.items():\n",
    "                self.mot_feats[vid] = feats[idx][...]\n",
    "        \n",
    "        self._build_sample_list()\n",
    "\n",
    "    def _build_sample_list(self):\n",
    "        self.samples = []\n",
    "        \n",
    "        if self.qtype == -1:\n",
    "            for vid in self.vids:\n",
    "                for qt in range(6):\n",
    "                    self.samples.append((vid, qt))\n",
    "        elif self.qtype == 0 or self.qtype == 1:\n",
    "            for vid in self.vids:\n",
    "                self.samples.append((vid, self.qtype))\n",
    "        elif self.qtype == 2:\n",
    "            for vid in self.vids:\n",
    "                self.samples.append((vid, 2))\n",
    "                self.samples.append((vid, 3))\n",
    "        elif self.qtype == 3:\n",
    "            for vid in self.vids:\n",
    "                self.samples.append((vid, 4))\n",
    "                self.samples.append((vid, 5))\n",
    "        else:\n",
    "            for vid in self.vids:\n",
    "                self.samples.append((vid, self.qtype))\n",
    "        \n",
    "        print(f\"Total samples: {len(self.samples)}\")\n",
    "\n",
    "    def _load_text(self, vid, qtype):\n",
    "        text_file = osp.join(self.text_annotation_path, vid, 'text.json')\n",
    "        answer_file = osp.join(self.text_annotation_path, vid, 'answer.json')\n",
    "        \n",
    "        if not osp.exists(text_file):\n",
    "            text_file = osp.join(self.text_annotation_path, 'QA', vid, 'text.json')\n",
    "            answer_file = osp.join(self.text_annotation_path, 'QA', vid, 'answer.json')\n",
    "        \n",
    "        if not osp.exists(text_file):\n",
    "            raise FileNotFoundError(f\"Text annotation not found for video: {vid}\")\n",
    "        \n",
    "        with open(text_file, 'r') as f:\n",
    "            text = json.load(f)\n",
    "        with open(answer_file, 'r') as f:\n",
    "            answer = json.load(f)\n",
    "        \n",
    "        if qtype == 0:\n",
    "            qns = text['descriptive']['question']\n",
    "            cand_ans = text['descriptive']['answer']\n",
    "            ans_id = answer['descriptive']['answer']\n",
    "        elif qtype == 1:\n",
    "            qns = text['explanatory']['question']\n",
    "            cand_ans = text['explanatory']['answer']\n",
    "            ans_id = answer['explanatory']['answer']\n",
    "        elif qtype == 2:\n",
    "            qns = text['predictive']['question']\n",
    "            cand_ans = text['predictive']['answer']\n",
    "            ans_id = answer['predictive']['answer']\n",
    "        elif qtype == 3:\n",
    "            qns = text['predictive']['question']\n",
    "            cand_ans = text['predictive']['reason']\n",
    "            ans_id = answer['predictive']['reason']\n",
    "        elif qtype == 4:\n",
    "            qns = text['counterfactual']['question']\n",
    "            cand_ans = text['counterfactual']['answer']\n",
    "            ans_id = answer['counterfactual']['answer']\n",
    "        elif qtype == 5:\n",
    "            qns = text['counterfactual']['question']\n",
    "            cand_ans = text['counterfactual']['reason']\n",
    "            ans_id = answer['counterfactual']['reason']\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid qtype: {qtype}\")\n",
    "        \n",
    "        return qns, cand_ans, ans_id\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid, qtype = self.samples[idx]\n",
    "        \n",
    "        qns_word, cand_ans, ans_id = self._load_text(vid, qtype)\n",
    "        ans_word = ['[CLS] ' + qns_word + ' [SEP] ' + str(cand_ans[i]) for i in range(self.mc)]\n",
    "        \n",
    "        # Load video features\n",
    "        app_feat = self.app_feats[vid]\n",
    "        mot_feat = self.mot_feats[vid]\n",
    "        \n",
    "        # === FIX: Handle different feature shapes ===\n",
    "        # Squeeze or reshape if needed to get (T, D)\n",
    "        if app_feat.ndim == 3:\n",
    "            app_feat = app_feat.mean(axis=1) if app_feat.shape[1] > 1 else app_feat.squeeze(1)\n",
    "        if mot_feat.ndim == 3:\n",
    "            mot_feat = mot_feat.mean(axis=1) if mot_feat.shape[1] > 1 else mot_feat.squeeze(1)\n",
    "        \n",
    "        if app_feat.ndim == 1:\n",
    "            app_feat = app_feat[np.newaxis, :]\n",
    "        if mot_feat.ndim == 1:\n",
    "            mot_feat = mot_feat[np.newaxis, :]\n",
    "        # === END FIX ===\n",
    "        \n",
    "        # Frame feature: concatenate app + mot\n",
    "        frame_feat = np.concatenate([app_feat, mot_feat], axis=-1)\n",
    "        vid_frame_feat = torch.from_numpy(frame_feat).type(torch.float32)\n",
    "        \n",
    "        # Object features\n",
    "        T = app_feat.shape[0]\n",
    "        D_obj = app_feat.shape[-1]\n",
    "        \n",
    "        obj_feat = np.tile(app_feat[:, np.newaxis, :], (1, self.obj_num, 1))\n",
    "        dummy_bbox = np.zeros((T, self.obj_num, 5), dtype=np.float32)\n",
    "        dummy_bbox[:, :, :4] = np.array([0.0, 0.0, 1.0, 1.0])\n",
    "        dummy_bbox[:, :, 4] = 1.0\n",
    "        \n",
    "        obj_feat = np.concatenate([obj_feat, dummy_bbox], axis=-1)\n",
    "        vid_obj_feat = torch.from_numpy(obj_feat).type(torch.float32)\n",
    "        \n",
    "        qns_key = vid + '_' + str(qtype)\n",
    "        \n",
    "        return vid_frame_feat, vid_obj_feat, qns_word, ans_word, ans_id, qns_key\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "'''\n",
    "\n",
    "# Write patched DataLoader.py\n",
    "with open('DataLoader.py', 'w') as f:\n",
    "    f.write(patch_code)\n",
    "\n",
    "print(\"‚úÖ DataLoader.py patched with dimension fix!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17953d4d",
   "metadata": {},
   "source": [
    "## 2. üìÇ Data Paths & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "# ============================================================\n",
    "# DATA PATHS (Kaggle Input)\n",
    "# ============================================================\n",
    "text_feature_path = '/kaggle/input/text-feature'\n",
    "visual_feature_path = '/kaggle/input/visual-feature'\n",
    "split_path = '/kaggle/input/dataset-split-1'\n",
    "text_annotation_path = '/kaggle/input/text-annotation'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìÇ DATA PATHS\")\n",
    "print(\"=\" * 70)\n",
    "for name, path in [(\"Visual features\", visual_feature_path), \n",
    "                   (\"Split files\", split_path), \n",
    "                   (\"Text annotations\", text_annotation_path)]:\n",
    "    status = \"‚úì\" if os.path.exists(path) else \"‚úó\"\n",
    "    print(f\"  {status} {name}: {path}\")\n",
    "\n",
    "# ============================================================\n",
    "# DATA STATISTICS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä DATASET STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Split files\n",
    "print(\"\\nüìÅ Split Files:\")\n",
    "split_stats = {}\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    split_file = f'{split_path}/{split_name}.pkl'\n",
    "    if os.path.exists(split_file):\n",
    "        with open(split_file, 'rb') as f:\n",
    "            vids = pickle.load(f)\n",
    "        split_stats[split_name] = len(vids)\n",
    "        samples = len(vids) * 6  # 6 question types per video\n",
    "        print(f\"  {split_name:>6}: {len(vids):>6} videos ‚Üí {samples:>6} samples\")\n",
    "\n",
    "# 2. Visual features\n",
    "print(\"\\nüé¨ Visual Features:\")\n",
    "idx2vid_file = f'{visual_feature_path}/idx2vid.pkl'\n",
    "if os.path.exists(idx2vid_file):\n",
    "    with open(idx2vid_file, 'rb') as f:\n",
    "        idx2vid = pickle.load(f)\n",
    "    print(f\"  Indexed videos: {len(idx2vid)}\")\n",
    "\n",
    "for feat_name in ['appearance_feat.h5', 'motion_feat.h5']:\n",
    "    feat_file = f'{visual_feature_path}/{feat_name}'\n",
    "    if os.path.exists(feat_file):\n",
    "        with h5py.File(feat_file, 'r') as f:\n",
    "            shape = f['resnet_features'].shape\n",
    "        print(f\"  {feat_name}: {shape}\")\n",
    "\n",
    "# 3. Question types\n",
    "print(\"\\n‚ùì Question Types (qtype):\")\n",
    "qtype_info = [\n",
    "    (\"0\", \"Descriptive\", \"What is happening?\"),\n",
    "    (\"1\", \"Explanatory\", \"Why did it happen?\"),\n",
    "    (\"2\", \"Predictive-Ans\", \"What will happen?\"),\n",
    "    (\"3\", \"Predictive-Reason\", \"Why will it happen?\"),\n",
    "    (\"4\", \"Counterfactual-Ans\", \"What if X didn't happen?\"),\n",
    "    (\"5\", \"Counterfactual-Reason\", \"Why would that result?\"),\n",
    "]\n",
    "for qt, name, desc in qtype_info:\n",
    "    print(f\"  {qt}: {name:<20} - {desc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29601a5f",
   "metadata": {},
   "source": [
    "## 3. ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Training configuration for CausalVidQA\"\"\"\n",
    "    \n",
    "    # Experiment\n",
    "    project_name = \"CausalVidQA-TranSTR\"\n",
    "    run_name = \"causalvid_2gpu\"\n",
    "    \n",
    "    # Data paths\n",
    "    sample_list_path = split_path\n",
    "    video_feature_path = visual_feature_path\n",
    "    text_annotation_path = text_annotation_path\n",
    "    \n",
    "    # Training\n",
    "    bs = 16                    # Batch size (s·∫Ω chia ƒë·ªÅu cho 2 GPU)\n",
    "    lr = 1e-4                  # Learning rate\n",
    "    text_encoder_lr = 1e-5     # Text encoder LR (lower)\n",
    "    epoch = 20\n",
    "    warmup_epochs = 2          # Warmup epochs\n",
    "    \n",
    "    # Dataset\n",
    "    dataset = 'causal-vid'\n",
    "    qtype = -1                 # -1 = all question types\n",
    "    max_samples = None         # None = use all data\n",
    "    \n",
    "    # Model architecture\n",
    "    d_model = 768\n",
    "    word_dim = 768\n",
    "    nheads = 8\n",
    "    num_encoder_layers = 1\n",
    "    num_decoder_layers = 1\n",
    "    dropout = 0.1\n",
    "    encoder_dropout = 0.1\n",
    "    activation = 'relu'\n",
    "    normalize_before = False\n",
    "    \n",
    "    # Video features\n",
    "    objs = 20                  # Objects per frame\n",
    "    topK_frame = 8             # Top-K frames to select\n",
    "    topK_obj = 5               # Top-K objects to select\n",
    "    frame_feat_dim = 4096      # app(2048) + mot(2048)\n",
    "    obj_feat_dim = 2053        # feat(2048) + bbox(5)\n",
    "    n_query = 5                # 5-way multiple choice\n",
    "    \n",
    "    # Text encoder\n",
    "    text_encoder_type = \"microsoft/deberta-base\"\n",
    "    freeze_text_encoder = False\n",
    "    text_pool_mode = 0\n",
    "    hard_eval = False\n",
    "    \n",
    "    # Optimizer\n",
    "    decay = 0.001              # Weight decay\n",
    "    patience = 3               # LR scheduler patience\n",
    "    gamma = 0.5                # LR decay factor\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_patience = 5  # Stop after 5 epochs without improvement\n",
    "    \n",
    "    # Contrastive learning\n",
    "    pos_ratio = 0.7\n",
    "    neg_ratio = 0.3\n",
    "    a = 1\n",
    "    \n",
    "    # Multi-GPU\n",
    "    use_multi_gpu = True       # Enable DataParallel\n",
    "    num_workers = 4            # DataLoader workers\n",
    "    \n",
    "    # Logging\n",
    "    log_interval = 50          # Log every N batches\n",
    "    save_every = 5             # Save checkpoint every N epochs\n",
    "\n",
    "args = Config()\n",
    "\n",
    "# ============================================================\n",
    "# GPU SETUP\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üñ•Ô∏è GPU CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"  Available GPUs: {n_gpus}\")\n",
    "for i in range(n_gpus):\n",
    "    print(f\"    GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    mem = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "    print(f\"           Memory: {mem:.1f} GB\")\n",
    "\n",
    "if n_gpus >= 2 and args.use_multi_gpu:\n",
    "    print(f\"\\n  ‚úì Multi-GPU mode: DataParallel on {n_gpus} GPUs\")\n",
    "    print(f\"  ‚úì Effective batch size: {args.bs} (total)\")\n",
    "else:\n",
    "    print(f\"\\n  ‚Üí Single GPU mode\")\n",
    "    args.use_multi_gpu = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"  Primary device: {device}\")\n",
    "\n",
    "# ============================================================\n",
    "# PRINT CONFIG\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚öôÔ∏è TRAINING CONFIG\")\n",
    "print(\"=\" * 70)\n",
    "config_items = [\n",
    "    (\"Batch size\", args.bs),\n",
    "    (\"Learning rate\", args.lr),\n",
    "    (\"Text encoder LR\", args.text_encoder_lr),\n",
    "    (\"Epochs\", args.epoch),\n",
    "    (\"Early stopping\", f\"{args.early_stopping_patience} epochs\"),\n",
    "    (\"d_model\", args.d_model),\n",
    "    (\"TopK frames\", args.topK_frame),\n",
    "    (\"TopK objects\", args.topK_obj),\n",
    "    (\"Objects/frame\", args.objs),\n",
    "    (\"Text encoder\", args.text_encoder_type),\n",
    "]\n",
    "\n",
    "for name, val in config_items:\n",
    "    print(f\"  {name:<20}: {val}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081795f3",
   "metadata": {},
   "source": [
    "## 4. üìö Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6efa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Local imports\n",
    "from DataLoader import VideoQADataset\n",
    "from networks.model import VideoQAmodel\n",
    "import eval_mc\n",
    "\n",
    "# ============================================================\n",
    "# REPRODUCIBILITY\n",
    "# ============================================================\n",
    "def set_seed(seed=999):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(999)\n",
    "print(\"‚úÖ Modules imported, seed set to 999\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e945a",
   "metadata": {},
   "source": [
    "## 5. üìä Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60fd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating datasets...\")\n",
    "\n",
    "# ============================================================\n",
    "# CREATE DATASETS\n",
    "# ============================================================\n",
    "dataset_kwargs = dict(\n",
    "    n_query=args.n_query,\n",
    "    obj_num=args.objs,\n",
    "    sample_list_path=args.sample_list_path,\n",
    "    video_feature_path=args.video_feature_path,\n",
    "    text_annotation_path=args.text_annotation_path,\n",
    "    qtype=args.qtype,\n",
    "    max_samples=args.max_samples\n",
    ")\n",
    "\n",
    "train_dataset = VideoQADataset(split='train', **dataset_kwargs)\n",
    "val_dataset = VideoQADataset(split='val', **dataset_kwargs)\n",
    "\n",
    "# Test set LU√îN d√πng to√†n b·ªô data (kh√¥ng gi·ªõi h·∫°n max_samples)\n",
    "test_kwargs = dataset_kwargs.copy()\n",
    "test_kwargs['max_samples'] = None  # Force full test set\n",
    "test_dataset = VideoQADataset(split='test', **test_kwargs)\n",
    "\n",
    "# ============================================================\n",
    "# CREATE DATALOADERS (optimized for multi-GPU)\n",
    "# ============================================================\n",
    "loader_kwargs = dict(\n",
    "    batch_size=args.bs,\n",
    "    num_workers=args.num_workers if args.use_multi_gpu else 0,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2 if args.num_workers > 0 else None,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, **loader_kwargs)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, **loader_kwargs)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, **loader_kwargs)\n",
    "\n",
    "# ============================================================\n",
    "# DATASET SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä DATALOADER SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  {'Split':<10} {'Videos':>10} {'Samples':>10} {'Batches':>10}\")\n",
    "print(f\"  {'-'*10} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "for name, dataset, loader in [\n",
    "    (\"Train\", train_dataset, train_loader),\n",
    "    (\"Val\", val_dataset, val_loader),\n",
    "    (\"Test (FULL)\", test_dataset, test_loader)\n",
    "]:\n",
    "    n_vids = len(dataset.vids) if hasattr(dataset, 'vids') else \"?\"\n",
    "    print(f\"  {name:<10} {n_vids:>10} {len(dataset):>10} {len(loader):>10}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  ‚ÑπÔ∏è  Test set always uses ALL data regardless of max_samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229298f3",
   "metadata": {},
   "source": [
    "## 6. üîç Check Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7186de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFY DATA SAMPLE\n",
    "# ============================================================\n",
    "print(\"üîç Verifying data sample...\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    vid_frame_feat, vid_obj_feat, qns_word, ans_word, ans_id, qns_key = batch\n",
    "    \n",
    "    print(f\"\\n  Frame features:  {vid_frame_feat.shape}\")\n",
    "    print(f\"  Object features: {vid_obj_feat.shape}\")\n",
    "    print(f\"  Batch size:      {len(qns_word)}\")\n",
    "    print(f\"\\n  Sample question: {qns_word[0][:80]}...\")\n",
    "    print(f\"  Sample answer:   {ans_word[0][0][:60]}...\")\n",
    "    print(f\"  Ground truth:    {ans_id[0].item()}\")\n",
    "    print(f\"  Question key:    {qns_key[0]}\")\n",
    "    break\n",
    "\n",
    "print(\"\\n‚úÖ Data verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e180162",
   "metadata": {},
   "source": [
    "## 7. üèóÔ∏è Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bfaceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE MODEL\n",
    "# ============================================================\n",
    "print(\"üèóÔ∏è Creating model...\")\n",
    "\n",
    "model_config = {\n",
    "    'd_model': args.d_model,\n",
    "    'word_dim': args.word_dim,\n",
    "    'encoder_dropout': args.encoder_dropout,\n",
    "    'dropout': args.dropout,\n",
    "    'num_encoder_layers': args.num_encoder_layers,\n",
    "    'num_decoder_layers': args.num_decoder_layers,\n",
    "    'nheads': args.nheads,\n",
    "    'normalize_before': args.normalize_before,\n",
    "    'activation': args.activation,\n",
    "    'text_encoder_type': args.text_encoder_type,\n",
    "    'freeze_text_encoder': args.freeze_text_encoder,\n",
    "    'text_pool_mode': args.text_pool_mode,\n",
    "    'n_query': args.n_query,\n",
    "    'objs': args.objs,\n",
    "    'topK_frame': args.topK_frame,\n",
    "    'topK_obj': args.topK_obj,\n",
    "    'hard_eval': args.hard_eval,\n",
    "    'frame_feat_dim': args.frame_feat_dim,\n",
    "    'obj_feat_dim': args.obj_feat_dim,\n",
    "    'device': device,\n",
    "}\n",
    "\n",
    "model = VideoQAmodel(**model_config)\n",
    "\n",
    "# ============================================================\n",
    "# MULTI-GPU SETUP (DataParallel)\n",
    "# ============================================================\n",
    "if args.use_multi_gpu and torch.cuda.device_count() > 1:\n",
    "    print(f\"  ‚Üí Wrapping model with DataParallel ({torch.cuda.device_count()} GPUs)\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# ============================================================\n",
    "# MODEL SUMMARY\n",
    "# ============================================================\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üß† MODEL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Total parameters:     {total_params / 1e6:.2f}M\")\n",
    "print(f\"  Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
    "print(f\"  Multi-GPU:            {args.use_multi_gpu and torch.cuda.device_count() > 1}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb184acc",
   "metadata": {},
   "source": [
    "## 8. üéØ Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e68a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader, criterion, device, epoch, wandb_run=None):\n",
    "    \"\"\"Train for one epoch with detailed logging\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    answers = []\n",
    "    batch_times = []\n",
    "    \n",
    "    # Per question type tracking\n",
    "    qtype_correct = defaultdict(int)\n",
    "    qtype_total = defaultdict(int)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, inputs in enumerate(train_loader):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "        vid_frame_feat = vid_frame_feat.to(device)\n",
    "        vid_obj_feat = vid_obj_feat.to(device)\n",
    "        ans_targets = ans_id.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "        loss = criterion(out, ans_targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        pred = out.max(-1)[1].cpu()\n",
    "        predictions.append(pred)\n",
    "        answers.append(ans_id)\n",
    "        \n",
    "        # Track per question type accuracy\n",
    "        for qkey, p, a in zip(qns_keys, pred.numpy(), ans_id.numpy()):\n",
    "            qtype = int(qkey.split('_')[-1])\n",
    "            qtype_total[qtype] += 1\n",
    "            if p == a:\n",
    "                qtype_correct[qtype] += 1\n",
    "        \n",
    "        batch_times.append(time.time() - batch_start)\n",
    "        \n",
    "        # Logging\n",
    "        if (batch_idx + 1) % args.log_interval == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            avg_time = np.mean(batch_times[-args.log_interval:])\n",
    "            print(f\"    Batch [{batch_idx+1:>4}/{len(train_loader)}] \"\n",
    "                  f\"Loss: {loss.item():.4f} (avg: {avg_loss:.4f}) \"\n",
    "                  f\"Time: {avg_time:.3f}s/batch\")\n",
    "            \n",
    "            if wandb_run:\n",
    "                wandb_run.log({\n",
    "                    \"train/batch_loss\": loss.item(),\n",
    "                    \"train/avg_loss\": avg_loss,\n",
    "                    \"train/batch_time\": avg_time,\n",
    "                }, step=epoch * len(train_loader) + batch_idx)\n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    all_preds = torch.cat(predictions, dim=0).long()\n",
    "    all_ans = torch.cat(answers, dim=0).long()\n",
    "    epoch_acc = (all_preds == all_ans).sum().item() * 100.0 / len(all_ans)\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Per question type accuracy\n",
    "    qtype_acc = {}\n",
    "    qtype_names = ['Des', 'Exp', 'Pred-A', 'Pred-R', 'CF-A', 'CF-R']\n",
    "    for qt in range(6):\n",
    "        if qtype_total[qt] > 0:\n",
    "            qtype_acc[qtype_names[qt]] = qtype_correct[qt] * 100.0 / qtype_total[qt]\n",
    "    \n",
    "    return {\n",
    "        'loss': epoch_loss,\n",
    "        'acc': epoch_acc,\n",
    "        'time': epoch_time,\n",
    "        'qtype_acc': qtype_acc\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device, split_name='val'):\n",
    "    \"\"\"Evaluate with detailed per-type accuracy\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    answers = []\n",
    "    qtype_correct = defaultdict(int)\n",
    "    qtype_total = defaultdict(int)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "            vid_frame_feat = vid_frame_feat.to(device)\n",
    "            vid_obj_feat = vid_obj_feat.to(device)\n",
    "            \n",
    "            out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "            pred = out.max(-1)[1].cpu()\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            answers.append(ans_id)\n",
    "            \n",
    "            for qkey, p, a in zip(qns_keys, pred.numpy(), ans_id.numpy()):\n",
    "                qtype = int(qkey.split('_')[-1])\n",
    "                qtype_total[qtype] += 1\n",
    "                if p == a:\n",
    "                    qtype_correct[qtype] += 1\n",
    "    \n",
    "    all_preds = torch.cat(predictions, dim=0).long()\n",
    "    all_ans = torch.cat(answers, dim=0).long()\n",
    "    overall_acc = (all_preds == all_ans).sum().item() * 100.0 / len(all_ans)\n",
    "    \n",
    "    # Per question type accuracy\n",
    "    qtype_names = ['Des', 'Exp', 'Pred-A', 'Pred-R', 'CF-A', 'CF-R']\n",
    "    qtype_acc = {}\n",
    "    for qt in range(6):\n",
    "        if qtype_total[qt] > 0:\n",
    "            qtype_acc[qtype_names[qt]] = qtype_correct[qt] * 100.0 / qtype_total[qt]\n",
    "    \n",
    "    # Combined metrics (Pred = both Pred-A and Pred-R correct for same video)\n",
    "    # This is computed at video level, need results dict for that\n",
    "    \n",
    "    return {\n",
    "        'acc': overall_acc,\n",
    "        'qtype_acc': qtype_acc,\n",
    "        'n_samples': len(all_ans)\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_and_save(model, data_loader, device, save_path):\n",
    "    \"\"\"Generate predictions and save to JSON\"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "            vid_frame_feat = vid_frame_feat.to(device)\n",
    "            vid_obj_feat = vid_obj_feat.to(device)\n",
    "            \n",
    "            out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "            pred = out.max(-1)[1].cpu()\n",
    "            \n",
    "            for qid, p, a in zip(qns_keys, pred.numpy(), ans_id.numpy()):\n",
    "                results[qid] = {'prediction': int(p), 'answer': int(a)}\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    correct = sum(1 for v in results.values() if v['prediction'] == v['answer'])\n",
    "    acc = correct * 100.0 / len(results)\n",
    "    \n",
    "    return results, acc\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575add3c",
   "metadata": {},
   "source": [
    "## 9. üöÄ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP OPTIMIZER, SCHEDULER, CRITERION\n",
    "# ============================================================\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "os.makedirs('./prediction', exist_ok=True)\n",
    "\n",
    "# Get base model for parameter groups (handle DataParallel)\n",
    "base_model = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "# Optimizer with different LR for text encoder\n",
    "param_groups = [\n",
    "    {\n",
    "        \"params\": [p for n, p in base_model.named_parameters() \n",
    "                   if \"text_encoder\" not in n and p.requires_grad],\n",
    "        \"lr\": args.lr\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in base_model.named_parameters() \n",
    "                   if \"text_encoder\" in n and p.requires_grad],\n",
    "        \"lr\": args.text_encoder_lr\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, weight_decay=args.decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=args.gamma, \n",
    "                               patience=args.patience, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"‚úÖ Optimizer and scheduler created\")\n",
    "print(f\"   Main LR: {args.lr}\")\n",
    "print(f\"   Text encoder LR: {args.text_encoder_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5747d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INITIALIZE WANDB\n",
    "# ============================================================\n",
    "wandb_config = {\n",
    "    \"architecture\": \"TranSTR\",\n",
    "    \"dataset\": \"CausalVidQA\",\n",
    "    \"epochs\": args.epoch,\n",
    "    \"batch_size\": args.bs,\n",
    "    \"learning_rate\": args.lr,\n",
    "    \"text_encoder_lr\": args.text_encoder_lr,\n",
    "    \"text_encoder\": args.text_encoder_type,\n",
    "    \"d_model\": args.d_model,\n",
    "    \"topK_frame\": args.topK_frame,\n",
    "    \"topK_obj\": args.topK_obj,\n",
    "    \"n_objects\": args.objs,\n",
    "    \"num_encoder_layers\": args.num_encoder_layers,\n",
    "    \"num_decoder_layers\": args.num_decoder_layers,\n",
    "    \"multi_gpu\": args.use_multi_gpu,\n",
    "    \"n_gpus\": torch.cuda.device_count(),\n",
    "    \"train_samples\": len(train_dataset),\n",
    "    \"val_samples\": len(val_dataset),\n",
    "    \"test_samples\": len(test_dataset),\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=args.project_name,\n",
    "    name=args.run_name,\n",
    "    config=wandb_config,\n",
    "    tags=[\"causalvid\", \"multi-gpu\" if args.use_multi_gpu else \"single-gpu\"]\n",
    ")\n",
    "\n",
    "# Log dataset info\n",
    "wandb.log({\n",
    "    \"data/train_videos\": len(train_dataset.vids) if hasattr(train_dataset, 'vids') else 0,\n",
    "    \"data/val_videos\": len(val_dataset.vids) if hasattr(val_dataset, 'vids') else 0,\n",
    "    \"data/test_videos\": len(test_dataset.vids) if hasattr(test_dataset, 'vids') else 0,\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ W&B initialized: {run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb5710",
   "metadata": {},
   "source": [
    "## 10. üöÄ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c6af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP WITH EARLY STOPPING\n",
    "# ============================================================\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = f'./models/best_model-{args.run_name}.ckpt'\n",
    "history = {'train': [], 'val': [], 'test': []}\n",
    "epochs_without_improvement = 0  # Early stopping counter\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"üöÄ STARTING TRAINING: {args.run_name}\")\n",
    "print(f\"   Epochs: {args.epoch} | Batch size: {args.bs} | GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"   Early stopping: {args.early_stopping_patience} epochs without improvement\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(1, args.epoch + 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìö EPOCH [{epoch}/{args.epoch}]\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ============ TRAIN ============\n",
    "    train_metrics = train_epoch(model, optimizer, train_loader, criterion, device, epoch, run)\n",
    "    \n",
    "    # ============ EVALUATE ============\n",
    "    val_metrics = evaluate(model, val_loader, device, 'val')\n",
    "    test_metrics = evaluate(model, test_loader, device, 'test')\n",
    "    \n",
    "    # ============ UPDATE SCHEDULER ============\n",
    "    scheduler.step(val_metrics['acc'])\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # ============ SAVE BEST MODEL & EARLY STOPPING ============\n",
    "    is_best = val_metrics['acc'] > best_val_acc\n",
    "    if is_best:\n",
    "        best_val_acc = val_metrics['acc']\n",
    "        best_epoch = epoch\n",
    "        epochs_without_improvement = 0  # Reset counter\n",
    "        # Save model (handle DataParallel)\n",
    "        state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
    "        torch.save(state_dict, best_model_path)\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # ============ LOGGING ============\n",
    "    print(f\"\\n  üìä Results:\")\n",
    "    print(f\"     {'Metric':<15} {'Train':>10} {'Val':>10} {'Test':>10}\")\n",
    "    print(f\"     {'-'*15} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "    print(f\"     {'Loss':<15} {train_metrics['loss']:>10.4f} {'-':>10} {'-':>10}\")\n",
    "    print(f\"     {'Accuracy':<15} {train_metrics['acc']:>9.2f}% {val_metrics['acc']:>9.2f}% {test_metrics['acc']:>9.2f}%\")\n",
    "    \n",
    "    # Per question type accuracy\n",
    "    print(f\"\\n  üìà Per Question Type Accuracy (Val):\")\n",
    "    qtype_order = ['Des', 'Exp', 'Pred-A', 'Pred-R', 'CF-A', 'CF-R']\n",
    "    for qt in qtype_order:\n",
    "        if qt in val_metrics['qtype_acc']:\n",
    "            print(f\"     {qt:<10}: {val_metrics['qtype_acc'][qt]:>6.2f}%\")\n",
    "    \n",
    "    print(f\"\\n  ‚è±Ô∏è  Time: {train_metrics['time']:.1f}s | LR: {current_lr:.2e}\")\n",
    "    print(f\"  üìâ No improvement: {epochs_without_improvement}/{args.early_stopping_patience} epochs\")\n",
    "    if is_best:\n",
    "        print(f\"  üíæ Saved best model! (Val acc: {best_val_acc:.2f}%)\")\n",
    "    \n",
    "    # ============ WANDB LOGGING ============\n",
    "    wandb_log = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train/loss\": train_metrics['loss'],\n",
    "        \"train/acc\": train_metrics['acc'],\n",
    "        \"val/acc\": val_metrics['acc'],\n",
    "        \"test/acc\": test_metrics['acc'],\n",
    "        \"lr\": current_lr,\n",
    "        \"epoch_time\": train_metrics['time'],\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"epochs_without_improvement\": epochs_without_improvement,\n",
    "    }\n",
    "    \n",
    "    # Log per question type accuracy\n",
    "    for qt, acc in train_metrics['qtype_acc'].items():\n",
    "        wandb_log[f\"train/acc_{qt}\"] = acc\n",
    "    for qt, acc in val_metrics['qtype_acc'].items():\n",
    "        wandb_log[f\"val/acc_{qt}\"] = acc\n",
    "    for qt, acc in test_metrics['qtype_acc'].items():\n",
    "        wandb_log[f\"test/acc_{qt}\"] = acc\n",
    "    \n",
    "    wandb.log(wandb_log)\n",
    "    \n",
    "    # Save checkpoint every N epochs\n",
    "    if epoch % args.save_every == 0:\n",
    "        ckpt_path = f'./models/checkpoint-{args.run_name}-ep{epoch}.ckpt'\n",
    "        state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': state_dict,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_metrics['acc'],\n",
    "        }, ckpt_path)\n",
    "        print(f\"  üìÅ Checkpoint saved: {ckpt_path}\")\n",
    "    \n",
    "    # ============ EARLY STOPPING CHECK ============\n",
    "    if epochs_without_improvement >= args.early_stopping_patience:\n",
    "        print(f\"\\n  ‚ö†Ô∏è EARLY STOPPING: No improvement for {args.early_stopping_patience} epochs\")\n",
    "        print(f\"     Best val acc: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "        wandb.log({\"early_stopped\": True, \"stopped_at_epoch\": epoch})\n",
    "        break\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING COMPLETE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Best epoch: {best_epoch}\")\n",
    "print(f\"   Best val accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   Model saved: {best_model_path}\")\n",
    "if epochs_without_improvement >= args.early_stopping_patience:\n",
    "    print(f\"   Stopped early at epoch {epoch}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b38b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL EVALUATION WITH BEST MODEL\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä FINAL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load best model\n",
    "print(\"\\n  Loading best model...\")\n",
    "base_model = model.module if hasattr(model, 'module') else model\n",
    "base_model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Predict on test set\n",
    "result_path = f'./prediction/{args.run_name}-ep{best_epoch}-val{best_val_acc:.2f}.json'\n",
    "results, test_acc = predict_and_save(model, test_loader, device, result_path)\n",
    "\n",
    "print(f\"\\n  Test accuracy: {test_acc:.2f}%\")\n",
    "print(f\"  Results saved: {result_path}\")\n",
    "\n",
    "# Detailed evaluation by question type\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"  Detailed Results by Question Type:\")\n",
    "print(\"-\" * 70)\n",
    "eval_mc.accuracy_metric_cvid(result_path)\n",
    "\n",
    "# Log final results to wandb\n",
    "wandb.log({\n",
    "    \"final/test_acc\": test_acc,\n",
    "    \"final/best_epoch\": best_epoch,\n",
    "    \"final/best_val_acc\": best_val_acc,\n",
    "})\n",
    "\n",
    "# Save results artifact\n",
    "artifact = wandb.Artifact(f'predictions-{args.run_name}', type='predictions')\n",
    "artifact.add_file(result_path)\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b13304",
   "metadata": {},
   "source": [
    "## 12. üíæ Save & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cbf1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE TO KAGGLE OUTPUT & FINISH WANDB\n",
    "# ============================================================\n",
    "import shutil\n",
    "\n",
    "output_dir = '/kaggle/working'\n",
    "if os.path.exists(output_dir):\n",
    "    # Copy best model\n",
    "    shutil.copy(best_model_path, os.path.join(output_dir, f'best_model-{args.run_name}.ckpt'))\n",
    "    # Copy predictions\n",
    "    shutil.copy(result_path, output_dir)\n",
    "    print(f\"‚úÖ Files saved to {output_dir}\")\n",
    "else:\n",
    "    print(\"  Not running on Kaggle, files saved locally\")\n",
    "\n",
    "# Save model artifact to wandb\n",
    "model_artifact = wandb.Artifact(f'model-{args.run_name}', type='model')\n",
    "model_artifact.add_file(best_model_path)\n",
    "wandb.log_artifact(model_artifact)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "print(\"‚úÖ W&B run finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10fa1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "### Multi-GPU Usage\n",
    "- Code s·ª≠ d·ª•ng `DataParallel` ƒë·ªÉ chia batch ƒë·ªÅu cho 2 GPU\n",
    "- Batch size 16 s·∫Ω ƒë∆∞·ª£c chia th√†nh 8 samples/GPU\n",
    "- ƒê·ªÉ tƒÉng throughput, c√≥ th·ªÉ tƒÉng batch size l√™n 32 ho·∫∑c 64\n",
    "\n",
    "### W&B Metrics Logged\n",
    "- `train/loss`, `train/acc` - Training metrics\n",
    "- `val/acc`, `test/acc` - Evaluation accuracy  \n",
    "- `train/acc_*`, `val/acc_*` - Per question type accuracy\n",
    "- `lr` - Current learning rate\n",
    "- `epoch_time` - Time per epoch\n",
    "\n",
    "### Question Types\n",
    "- **Des**: Descriptive (What is happening?)\n",
    "- **Exp**: Explanatory (Why did it happen?)\n",
    "- **Pred-A/R**: Predictive Answer/Reason\n",
    "- **CF-A/R**: Counterfactual Answer/Reason"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
