{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-08T15:57:55.735237Z",
     "iopub.status.busy": "2025-12-08T15:57:55.734430Z",
     "iopub.status.idle": "2025-12-08T15:57:56.246526Z",
     "shell.execute_reply": "2025-12-08T15:57:56.245872Z",
     "shell.execute_reply.started": "2025-12-08T15:57:55.735205Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'tranSTR_Casual'...\n",
      "remote: Enumerating objects: 43, done.\u001b[K\n",
      "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
      "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
      "remote: Total 43 (delta 11), reused 41 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (43/43), 267.42 KiB | 3.38 MiB/s, done.\n",
      "Resolving deltas: 100% (11/11), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/DanielQH07/tranSTR_Casual.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ CausalVidQA Training on Kaggle (T4 x2 GPU)\n",
    "\n",
    "This notebook trains the TranSTR model on CausalVidQA dataset with:\n",
    "- ‚úÖ **Multi-GPU support** (DataParallel for 2x T4)\n",
    "- ‚úÖ **W&B logging** with per-question-type metrics\n",
    "- ‚úÖ **Early stopping** after 5 epochs without improvement\n",
    "- ‚úÖ **Full test evaluation** regardless of training sample limit\n",
    "\n",
    "## üìã Cell Execution Order:\n",
    "1. Clone repo from GitHub\n",
    "2. Navigate to repo directory\n",
    "3. **Patch attention.py** (DataParallel fix)\n",
    "4. **Patch model.py** (repeat_interleave fix)\n",
    "5. Patch DataLoader.py (dimension handling)\n",
    "6. Continue with training setup...\n",
    "\n",
    "**Important:** After cloning, patches are applied to fix DataParallel compatibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T15:57:56.248250Z",
     "iopub.status.busy": "2025-12-08T15:57:56.248049Z",
     "iopub.status.idle": "2025-12-08T15:57:56.252948Z",
     "shell.execute_reply": "2025-12-08T15:57:56.252245Z",
     "shell.execute_reply.started": "2025-12-08T15:57:56.248231Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/tranSTR_Casual\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/tranSTR_Casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PATCH ATTENTION.PY - Fix DataParallel mask handling\n",
    "# ============================================================\n",
    "\n",
    "attention_fix = '''import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        assert self.dim % self.n_heads == 0\n",
    "\n",
    "        self.q_lin = nn.Linear(in_features=self.dim, out_features=self.dim)\n",
    "        self.k_lin = nn.Linear(in_features=self.dim, out_features=self.dim)\n",
    "        self.v_lin = nn.Linear(in_features=self.dim, out_features=self.dim)\n",
    "        self.out_lin = nn.Linear(in_features=self.dim, out_features=self.dim)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask, attn_mask=None, output_attentions=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        query: torch.tensor(bs, seq_length, dim)\n",
    "        key: torch.tensor(bs, seq_length, dim)\n",
    "        value: torch.tensor(bs, seq_length, dim)\n",
    "        key_padding_mask: torch.tensor(bs, seq_length)\n",
    "        Outputs\n",
    "        -------\n",
    "        weights: torch.tensor(bs, n_heads, seq_length, seq_length)\n",
    "            Attention weights\n",
    "        context: torch.tensor(bs, seq_length, dim)\n",
    "            Contextualized layer. Optional: only if `output_attentions=True`\n",
    "        \"\"\"\n",
    "        bs, q_length, dim = query.size()\n",
    "        k_length = key.size(1)\n",
    "\n",
    "        dim_per_head = self.dim // self.n_heads\n",
    "\n",
    "        def shape(x):\n",
    "            \"\"\" separate heads \"\"\"\n",
    "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
    "\n",
    "        def unshape(x):\n",
    "            \"\"\" group heads \"\"\"\n",
    "            return (\n",
    "                x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
    "            )\n",
    "\n",
    "        q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n",
    "        k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n",
    "        v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n",
    "\n",
    "        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\n",
    "        scores_ = scores.mean(1)\n",
    "        \n",
    "        if key_padding_mask is not None:\n",
    "            # DataParallel fix: get actual batch size from mask\n",
    "            actual_bs = key_padding_mask.size(0)\n",
    "            mask_len = key_padding_mask.size(1)\n",
    "            \n",
    "            # Trim or pad mask to match k_length\n",
    "            if mask_len > k_length:\n",
    "                key_padding_mask = key_padding_mask[:, :k_length]\n",
    "            elif mask_len < k_length:\n",
    "                padding = torch.ones(actual_bs, k_length - mask_len, \n",
    "                                   dtype=key_padding_mask.dtype, \n",
    "                                   device=key_padding_mask.device)\n",
    "                key_padding_mask = torch.cat([key_padding_mask, padding], dim=1)\n",
    "            \n",
    "            # Reshape mask - use actual_bs not bs\n",
    "            mask_reshp = (actual_bs, 1, 1, k_length)\n",
    "            padding_mask = (~key_padding_mask).view(mask_reshp).expand_as(scores)\n",
    "            scores = scores.masked_fill(padding_mask, -float(\"inf\"))\n",
    "\n",
    "        weights = nn.Softmax(dim=-1)(scores)  # (bs, n_heads, q_length, k_length)\n",
    "        weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if attn_mask is not None:\n",
    "            weights = weights * attn_mask\n",
    "\n",
    "        context = torch.matmul(weights, v)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        context = unshape(context)  # (bs, q_length, dim)\n",
    "        context = self.out_lin(context)  # (bs, q_length, dim)\n",
    "\n",
    "        if output_attentions:\n",
    "            return context, weights.mean(1)\n",
    "        else:\n",
    "            return context\n",
    "'''\n",
    "\n",
    "# Write patched attention.py\n",
    "import os\n",
    "os.makedirs('networks', exist_ok=True)\n",
    "with open('networks/attention.py', 'w') as f:\n",
    "    f.write(attention_fix)\n",
    "\n",
    "print(\"‚úÖ attention.py patched for DataParallel compatibility!\")\n",
    "print(f\"   File written to: {os.path.abspath('networks/attention.py')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PATCH MODEL.PY - Fix repeat_interleave for DataParallel\n",
    "# ============================================================\n",
    "\n",
    "# Read current model.py\n",
    "with open('networks/model.py', 'r') as f:\n",
    "    model_code = f.read()\n",
    "\n",
    "# Fix the obj_decoder call to handle repeat_interleave properly\n",
    "old_pattern = '''        obj_local, obj_att = self.obj_decoder(obj_local.flatten(0,1),\n",
    "                                            q_local.repeat_interleave(self.frame_topK, dim=0), \n",
    "                                            memory_key_padding_mask=q_mask.repeat_interleave(self.frame_topK, dim=0),\n",
    "                                            output_attentions=True\n",
    "                                            )  # b*16,5,d        #.view(B, F, O, -1) # b,16,5,d'''\n",
    "\n",
    "new_pattern = '''        # Repeat q_local and q_mask for each frame (handle potential batch size mismatch)\n",
    "        q_local_repeated = q_local.repeat_interleave(self.frame_topK, dim=0)\n",
    "        q_mask_repeated = q_mask.repeat_interleave(self.frame_topK, dim=0) if q_mask is not None else None\n",
    "        \n",
    "        obj_local, obj_att = self.obj_decoder(obj_local.flatten(0,1),\n",
    "                                            q_local_repeated, \n",
    "                                            memory_key_padding_mask=q_mask_repeated,\n",
    "                                            output_attentions=True\n",
    "                                            )  # b*16,5,d        #.view(B, F, O, -1) # b,16,5,d'''\n",
    "\n",
    "if old_pattern in model_code:\n",
    "    model_code = model_code.replace(old_pattern, new_pattern)\n",
    "    with open('networks/model.py', 'w') as f:\n",
    "        f.write(model_code)\n",
    "    print(\"‚úÖ model.py patched for DataParallel compatibility!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Pattern not found - model.py may already be patched or different\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T15:57:56.253807Z",
     "iopub.status.busy": "2025-12-08T15:57:56.253637Z",
     "iopub.status.idle": "2025-12-08T15:57:56.268318Z",
     "shell.execute_reply": "2025-12-08T15:57:56.267659Z",
     "shell.execute_reply.started": "2025-12-08T15:57:56.253793Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataLoader.py patched with dimension fix!\n"
     ]
    }
   ],
   "source": [
    "# Patch DataLoader.py ƒë·ªÉ x·ª≠ l√Ω dimension mismatch\n",
    "# Ch·∫°y cell n√†y tr∆∞·ªõc khi import DataLoader\n",
    "\n",
    "patch_code = '''\n",
    "import torch\n",
    "import os\n",
    "import h5py\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle as pkl\n",
    "from torch.utils import data\n",
    "from utils.util import load_file, pause, transform_bb, pkload\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "\n",
    "class VideoQADataset(Dataset):\n",
    "    \"\"\"\n",
    "    DataLoader cho CausalVidQA v·ªõi output format t∆∞∆°ng th√≠ch NextQA\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, split, n_query=5, obj_num=1, \n",
    "                 sample_list_path=None,\n",
    "                 video_feature_path=None,\n",
    "                 text_annotation_path=None,\n",
    "                 qtype=-1,\n",
    "                 max_samples=None):\n",
    "        super(VideoQADataset, self).__init__()\n",
    "        \n",
    "        self.split = split\n",
    "        self.mc = n_query\n",
    "        self.obj_num = obj_num\n",
    "        self.qtype = qtype\n",
    "        self.video_feature_path = video_feature_path\n",
    "        self.text_annotation_path = text_annotation_path\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "        # Load video ids for this split\n",
    "        split_name = split\n",
    "        if split == 'val':\n",
    "            split_file = osp.join(sample_list_path, 'val.pkl')\n",
    "            if not osp.exists(split_file):\n",
    "                split_file = osp.join(sample_list_path, 'valid.pkl')\n",
    "        else:\n",
    "            split_file = osp.join(sample_list_path, f'{split}.pkl')\n",
    "        \n",
    "        if not osp.exists(split_file):\n",
    "            raise FileNotFoundError(f\"Split file not found: {split_file}\")\n",
    "        \n",
    "        self.vids = pkload(split_file)\n",
    "        \n",
    "        if self.vids is None:\n",
    "            raise ValueError(f\"Failed to load split file: {split_file}\")\n",
    "        \n",
    "        if max_samples is not None and max_samples > 0:\n",
    "            self.vids = self.vids[:max_samples]\n",
    "            print(f\"Limited to {len(self.vids)} videos (max_samples={max_samples})\")\n",
    "        else:\n",
    "            print(f\"Loaded {len(self.vids)} videos from {split_file}\")\n",
    "        \n",
    "        # Load video feature index mapping\n",
    "        idx2vid_file = osp.join(video_feature_path, 'idx2vid.pkl')\n",
    "        vf_info = pkload(idx2vid_file)\n",
    "        self.vf_info = dict()\n",
    "        for idx, vid in enumerate(vf_info):\n",
    "            if vid in self.vids:\n",
    "                self.vf_info[vid] = idx\n",
    "        \n",
    "        # Load appearance features\n",
    "        app_file = osp.join(video_feature_path, 'appearance_feat.h5')\n",
    "        print(f'Loading {app_file}...')\n",
    "        self.app_feats = dict()\n",
    "        with h5py.File(app_file, 'r') as fp:\n",
    "            feats = fp['resnet_features']\n",
    "            for vid, idx in self.vf_info.items():\n",
    "                self.app_feats[vid] = feats[idx][...]\n",
    "        \n",
    "        # Load motion features\n",
    "        mot_file = osp.join(video_feature_path, 'motion_feat.h5')\n",
    "        print(f'Loading {mot_file}...')\n",
    "        self.mot_feats = dict()\n",
    "        with h5py.File(mot_file, 'r') as fp:\n",
    "            feats = fp['resnet_features']\n",
    "            for vid, idx in self.vf_info.items():\n",
    "                self.mot_feats[vid] = feats[idx][...]\n",
    "        \n",
    "        self._build_sample_list()\n",
    "\n",
    "    def _build_sample_list(self):\n",
    "        self.samples = []\n",
    "        \n",
    "        if self.qtype == -1:\n",
    "            for vid in self.vids:\n",
    "                for qt in range(6):\n",
    "                    self.samples.append((vid, qt))\n",
    "        elif self.qtype == 0 or self.qtype == 1:\n",
    "            for vid in self.vids:\n",
    "                self.samples.append((vid, self.qtype))\n",
    "        elif self.qtype == 2:\n",
    "            for vid in self.vids:\n",
    "                self.samples.append((vid, 2))\n",
    "                self.samples.append((vid, 3))\n",
    "        elif self.qtype == 3:\n",
    "            for vid in self.vids:\n",
    "                self.samples.append((vid, 4))\n",
    "                self.samples.append((vid, 5))\n",
    "        else:\n",
    "            for vid in self.vids:\n",
    "                self.samples.append((vid, self.qtype))\n",
    "        \n",
    "        print(f\"Total samples: {len(self.samples)}\")\n",
    "\n",
    "    def _load_text(self, vid, qtype):\n",
    "        text_file = osp.join(self.text_annotation_path, vid, 'text.json')\n",
    "        answer_file = osp.join(self.text_annotation_path, vid, 'answer.json')\n",
    "        \n",
    "        if not osp.exists(text_file):\n",
    "            text_file = osp.join(self.text_annotation_path, 'QA', vid, 'text.json')\n",
    "            answer_file = osp.join(self.text_annotation_path, 'QA', vid, 'answer.json')\n",
    "        \n",
    "        if not osp.exists(text_file):\n",
    "            raise FileNotFoundError(f\"Text annotation not found for video: {vid}\")\n",
    "        \n",
    "        with open(text_file, 'r') as f:\n",
    "            text = json.load(f)\n",
    "        with open(answer_file, 'r') as f:\n",
    "            answer = json.load(f)\n",
    "        \n",
    "        if qtype == 0:\n",
    "            qns = text['descriptive']['question']\n",
    "            cand_ans = text['descriptive']['answer']\n",
    "            ans_id = answer['descriptive']['answer']\n",
    "        elif qtype == 1:\n",
    "            qns = text['explanatory']['question']\n",
    "            cand_ans = text['explanatory']['answer']\n",
    "            ans_id = answer['explanatory']['answer']\n",
    "        elif qtype == 2:\n",
    "            qns = text['predictive']['question']\n",
    "            cand_ans = text['predictive']['answer']\n",
    "            ans_id = answer['predictive']['answer']\n",
    "        elif qtype == 3:\n",
    "            qns = text['predictive']['question']\n",
    "            cand_ans = text['predictive']['reason']\n",
    "            ans_id = answer['predictive']['reason']\n",
    "        elif qtype == 4:\n",
    "            qns = text['counterfactual']['question']\n",
    "            cand_ans = text['counterfactual']['answer']\n",
    "            ans_id = answer['counterfactual']['answer']\n",
    "        elif qtype == 5:\n",
    "            qns = text['counterfactual']['question']\n",
    "            cand_ans = text['counterfactual']['reason']\n",
    "            ans_id = answer['counterfactual']['reason']\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid qtype: {qtype}\")\n",
    "        \n",
    "        return qns, cand_ans, ans_id\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid, qtype = self.samples[idx]\n",
    "        \n",
    "        qns_word, cand_ans, ans_id = self._load_text(vid, qtype)\n",
    "        ans_word = ['[CLS] ' + qns_word + ' [SEP] ' + str(cand_ans[i]) for i in range(self.mc)]\n",
    "        \n",
    "        # Load video features\n",
    "        app_feat = self.app_feats[vid]\n",
    "        mot_feat = self.mot_feats[vid]\n",
    "        \n",
    "        # === FIX: Handle different feature shapes ===\n",
    "        # Squeeze or reshape if needed to get (T, D)\n",
    "        if app_feat.ndim == 3:\n",
    "            app_feat = app_feat.mean(axis=1) if app_feat.shape[1] > 1 else app_feat.squeeze(1)\n",
    "        if mot_feat.ndim == 3:\n",
    "            mot_feat = mot_feat.mean(axis=1) if mot_feat.shape[1] > 1 else mot_feat.squeeze(1)\n",
    "        \n",
    "        if app_feat.ndim == 1:\n",
    "            app_feat = app_feat[np.newaxis, :]\n",
    "        if mot_feat.ndim == 1:\n",
    "            mot_feat = mot_feat[np.newaxis, :]\n",
    "        # === END FIX ===\n",
    "        \n",
    "        # Frame feature: concatenate app + mot\n",
    "        frame_feat = np.concatenate([app_feat, mot_feat], axis=-1)\n",
    "        vid_frame_feat = torch.from_numpy(frame_feat).type(torch.float32)\n",
    "        \n",
    "        # Object features\n",
    "        T = app_feat.shape[0]\n",
    "        D_obj = app_feat.shape[-1]\n",
    "        \n",
    "        obj_feat = np.tile(app_feat[:, np.newaxis, :], (1, self.obj_num, 1))\n",
    "        dummy_bbox = np.zeros((T, self.obj_num, 5), dtype=np.float32)\n",
    "        dummy_bbox[:, :, :4] = np.array([0.0, 0.0, 1.0, 1.0])\n",
    "        dummy_bbox[:, :, 4] = 1.0\n",
    "        \n",
    "        obj_feat = np.concatenate([obj_feat, dummy_bbox], axis=-1)\n",
    "        vid_obj_feat = torch.from_numpy(obj_feat).type(torch.float32)\n",
    "        \n",
    "        qns_key = vid + '_' + str(qtype)\n",
    "        \n",
    "        return vid_frame_feat, vid_obj_feat, qns_word, ans_word, ans_id, qns_key\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "'''\n",
    "\n",
    "# Write patched DataLoader.py\n",
    "with open('DataLoader.py', 'w') as f:\n",
    "    f.write(patch_code)\n",
    "\n",
    "print(\"‚úÖ DataLoader.py patched with dimension fix!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T15:57:56.270286Z",
     "iopub.status.busy": "2025-12-08T15:57:56.269643Z",
     "iopub.status.idle": "2025-12-08T15:57:56.310917Z",
     "shell.execute_reply": "2025-12-08T15:57:56.310376Z",
     "shell.execute_reply.started": "2025-12-08T15:57:56.270260Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìÇ DATA PATHS\n",
      "======================================================================\n",
      "  ‚úì Visual features: /kaggle/input/visual-feature\n",
      "  ‚úì Split files: /kaggle/input/casual-vid-data-split/split\n",
      "  ‚úì Text annotations: /kaggle/input/text-annotation\n",
      "\n",
      "======================================================================\n",
      "üìä DATASET STATISTICS\n",
      "======================================================================\n",
      "\n",
      "üìÅ Split Files:\n",
      "   train:  18776 videos ‚Üí 112656 samples\n",
      "   valid:   2695 videos ‚Üí  16170 samples\n",
      "    test:   5429 videos ‚Üí  32574 samples\n",
      "\n",
      "üé¨ Visual Features:\n",
      "  Indexed videos: 26900\n",
      "  appearance_feat.h5: (26900, 8, 16, 2048)\n",
      "  motion_feat.h5: (26900, 8, 2048)\n",
      "\n",
      "‚ùì Question Types (qtype):\n",
      "  0: Descriptive          - What is happening?\n",
      "  1: Explanatory          - Why did it happen?\n",
      "  2: Predictive-Ans       - What will happen?\n",
      "  3: Predictive-Reason    - Why will it happen?\n",
      "  4: Counterfactual-Ans   - What if X didn't happen?\n",
      "  5: Counterfactual-Reason - Why would that result?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "# ============================================================\n",
    "# DATA PATHS (Kaggle Input)\n",
    "# ============================================================\n",
    "text_feature_path = '/kaggle/input/text-feature'\n",
    "visual_feature_path = '/kaggle/input/visual-feature'\n",
    "split_path = '/kaggle/input/casual-vid-data-split/split'\n",
    "text_annotation_path = '/kaggle/input/text-annotation'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìÇ DATA PATHS\")\n",
    "print(\"=\" * 70)\n",
    "for name, path in [(\"Visual features\", visual_feature_path), \n",
    "                   (\"Split files\", split_path), \n",
    "                   (\"Text annotations\", text_annotation_path)]:\n",
    "    status = \"‚úì\" if os.path.exists(path) else \"‚úó\"\n",
    "    print(f\"  {status} {name}: {path}\")\n",
    "\n",
    "# ============================================================\n",
    "# DATA STATISTICS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä DATASET STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Split files\n",
    "print(\"\\nüìÅ Split Files:\")\n",
    "split_stats = {}\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    split_file = f'{split_path}/{split_name}.pkl'\n",
    "    if os.path.exists(split_file):\n",
    "        with open(split_file, 'rb') as f:\n",
    "            vids = pickle.load(f)\n",
    "        split_stats[split_name] = len(vids)\n",
    "        samples = len(vids) * 6  # 6 question types per video\n",
    "        print(f\"  {split_name:>6}: {len(vids):>6} videos ‚Üí {samples:>6} samples\")\n",
    "\n",
    "# 2. Visual features\n",
    "print(\"\\nüé¨ Visual Features:\")\n",
    "idx2vid_file = f'{visual_feature_path}/idx2vid.pkl'\n",
    "if os.path.exists(idx2vid_file):\n",
    "    with open(idx2vid_file, 'rb') as f:\n",
    "        idx2vid = pickle.load(f)\n",
    "    print(f\"  Indexed videos: {len(idx2vid)}\")\n",
    "\n",
    "for feat_name in ['appearance_feat.h5', 'motion_feat.h5']:\n",
    "    feat_file = f'{visual_feature_path}/{feat_name}'\n",
    "    if os.path.exists(feat_file):\n",
    "        with h5py.File(feat_file, 'r') as f:\n",
    "            shape = f['resnet_features'].shape\n",
    "        print(f\"  {feat_name}: {shape}\")\n",
    "\n",
    "# 3. Question types\n",
    "print(\"\\n‚ùì Question Types (qtype):\")\n",
    "qtype_info = [\n",
    "    (\"0\", \"Descriptive\", \"What is happening?\"),\n",
    "    (\"1\", \"Explanatory\", \"Why did it happen?\"),\n",
    "    (\"2\", \"Predictive-Ans\", \"What will happen?\"),\n",
    "    (\"3\", \"Predictive-Reason\", \"Why will it happen?\"),\n",
    "    (\"4\", \"Counterfactual-Ans\", \"What if X didn't happen?\"),\n",
    "    (\"5\", \"Counterfactual-Reason\", \"Why would that result?\"),\n",
    "]\n",
    "for qt, name, desc in qtype_info:\n",
    "    print(f\"  {qt}: {name:<20} - {desc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T15:57:56.311635Z",
     "iopub.status.busy": "2025-12-08T15:57:56.311471Z",
     "iopub.status.idle": "2025-12-08T15:58:09.944622Z",
     "shell.execute_reply": "2025-12-08T15:58:09.943872Z",
     "shell.execute_reply.started": "2025-12-08T15:57:56.311622Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhaidang262004\u001b[0m (\u001b[33mintroSE\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -q transformers einops h5py wandb\n",
    "\n",
    "# Login to W&B (uncomment v√† th√™m API key c·ªßa b·∫°n)\n",
    "my_key = \"80b5a02ccaed80f35a2e893aed6446d4467c0c45\"\n",
    "import wandb\n",
    "wandb.login(key=my_key, relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T15:58:09.946285Z",
     "iopub.status.busy": "2025-12-08T15:58:09.945582Z",
     "iopub.status.idle": "2025-12-08T15:58:14.233007Z",
     "shell.execute_reply": "2025-12-08T15:58:14.232314Z",
     "shell.execute_reply.started": "2025-12-08T15:58:09.946261Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üñ•Ô∏è GPU CONFIGURATION\n",
      "======================================================================\n",
      "  Available GPUs: 2\n",
      "    GPU 0: Tesla T4\n",
      "           Memory: 15.8 GB\n",
      "    GPU 1: Tesla T4\n",
      "           Memory: 15.8 GB\n",
      "\n",
      "  ‚úì Multi-GPU mode: DataParallel on 2 GPUs\n",
      "  ‚úì Effective batch size: 2 (total)\n",
      "  Primary device: cuda\n",
      "\n",
      "======================================================================\n",
      "‚öôÔ∏è TRAINING CONFIG\n",
      "======================================================================\n",
      "  Batch size          : 2\n",
      "  Learning rate       : 0.0001\n",
      "  Text encoder LR     : 1e-05\n",
      "  Epochs              : 20\n",
      "  Early stopping      : 5 epochs\n",
      "  d_model             : 768\n",
      "  TopK frames         : 8\n",
      "  TopK objects        : 5\n",
      "  Objects/frame       : 20\n",
      "  Text encoder        : microsoft/deberta-base\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Training configuration for CausalVidQA\"\"\"\n",
    "    \n",
    "    # Experiment\n",
    "    project_name = \"CausalVidQA-TranSTR\"\n",
    "    run_name = \"causalvid_2gpu\"\n",
    "    \n",
    "    # Data paths\n",
    "    sample_list_path = split_path\n",
    "    video_feature_path = visual_feature_path\n",
    "    text_annotation_path = text_annotation_path\n",
    "    \n",
    "    # Training\n",
    "    bs = 2                    # Batch size (s·∫Ω chia ƒë·ªÅu cho 2 GPU)\n",
    "    lr = 1e-4                  # Learning rate\n",
    "    text_encoder_lr = 1e-5     # Text encoder LR (lower)\n",
    "    epoch = 20\n",
    "    warmup_epochs = 2          # Warmup epochs\n",
    "    \n",
    "    # Dataset\n",
    "    dataset = 'causal-vid'\n",
    "    qtype = 4                 # -1 = all question types\n",
    "    max_samples = 500         # None = use all data\n",
    "    \n",
    "    # Model architecture\n",
    "    d_model = 768\n",
    "    word_dim = 768\n",
    "    nheads = 8\n",
    "    num_encoder_layers = 1\n",
    "    num_decoder_layers = 1\n",
    "    dropout = 0.1\n",
    "    encoder_dropout = 0.1\n",
    "    activation = 'relu'\n",
    "    normalize_before = False\n",
    "    \n",
    "    # Video features\n",
    "    objs = 20                  # Objects per frame\n",
    "    topK_frame = 8             # Top-K frames to select\n",
    "    topK_obj = 5               # Top-K objects to select\n",
    "    frame_feat_dim = 4096      # app(2048) + mot(2048)\n",
    "    obj_feat_dim = 2053        # feat(2048) + bbox(5)\n",
    "    n_query = 5                # 5-way multiple choice\n",
    "    \n",
    "    # Text encoder\n",
    "    text_encoder_type = \"microsoft/deberta-base\"\n",
    "    freeze_text_encoder = False\n",
    "    text_pool_mode = 0\n",
    "    hard_eval = False\n",
    "    \n",
    "    # Optimizer\n",
    "    decay = 0.001              # Weight decay\n",
    "    patience = 3               # LR scheduler patience\n",
    "    gamma = 0.5                # LR decay factor\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_patience = 5  # Stop after 5 epochs without improvement\n",
    "    \n",
    "    # Contrastive learning\n",
    "    pos_ratio = 0.7\n",
    "    neg_ratio = 0.3\n",
    "    a = 1\n",
    "    \n",
    "    # Multi-GPU\n",
    "    use_multi_gpu = True       # Enable DataParallel\n",
    "    num_workers = 2            # DataLoader workers\n",
    "    \n",
    "    # Logging\n",
    "    log_interval = 50          # Log every N batches\n",
    "    save_every = 5             # Save checkpoint every N epochs\n",
    "\n",
    "args = Config()\n",
    "\n",
    "# ============================================================\n",
    "# GPU SETUP\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üñ•Ô∏è GPU CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"  Available GPUs: {n_gpus}\")\n",
    "for i in range(n_gpus):\n",
    "    print(f\"    GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    mem = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "    print(f\"           Memory: {mem:.1f} GB\")\n",
    "\n",
    "if n_gpus >= 2 and args.use_multi_gpu:\n",
    "    print(f\"\\n  ‚úì Multi-GPU mode: DataParallel on {n_gpus} GPUs\")\n",
    "    print(f\"  ‚úì Effective batch size: {args.bs} (total)\")\n",
    "else:\n",
    "    print(f\"\\n  ‚Üí Single GPU mode\")\n",
    "    args.use_multi_gpu = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"  Primary device: {device}\")\n",
    "\n",
    "# ============================================================\n",
    "# PRINT CONFIG\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚öôÔ∏è TRAINING CONFIG\")\n",
    "print(\"=\" * 70)\n",
    "config_items = [\n",
    "    (\"Batch size\", args.bs),\n",
    "    (\"Learning rate\", args.lr),\n",
    "    (\"Text encoder LR\", args.text_encoder_lr),\n",
    "    (\"Epochs\", args.epoch),\n",
    "    (\"Early stopping\", f\"{args.early_stopping_patience} epochs\"),\n",
    "    (\"d_model\", args.d_model),\n",
    "    (\"TopK frames\", args.topK_frame),\n",
    "    (\"TopK objects\", args.topK_obj),\n",
    "    (\"Objects/frame\", args.objs),\n",
    "    (\"Text encoder\", args.text_encoder_type),\n",
    "]\n",
    "\n",
    "for name, val in config_items:\n",
    "    print(f\"  {name:<20}: {val}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T15:58:14.234293Z",
     "iopub.status.busy": "2025-12-08T15:58:14.233758Z",
     "iopub.status.idle": "2025-12-08T15:58:18.470883Z",
     "shell.execute_reply": "2025-12-08T15:58:18.470230Z",
     "shell.execute_reply.started": "2025-12-08T15:58:14.234267Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modules imported, seed set to 999\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Local imports\n",
    "from DataLoader import VideoQADataset\n",
    "from networks.model import VideoQAmodel\n",
    "import eval_mc\n",
    "\n",
    "# ============================================================\n",
    "# REPRODUCIBILITY\n",
    "# ============================================================\n",
    "def set_seed(seed=999):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(999)\n",
    "print(\"‚úÖ Modules imported, seed set to 999\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T15:58:18.472281Z",
     "iopub.status.busy": "2025-12-08T15:58:18.471688Z",
     "iopub.status.idle": "2025-12-08T15:59:41.240284Z",
     "shell.execute_reply": "2025-12-08T15:59:41.239608Z",
     "shell.execute_reply.started": "2025-12-08T15:58:18.472258Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "Limited to 500 videos (max_samples=500)\n",
      "Loading /kaggle/input/visual-feature/appearance_feat.h5...\n",
      "Loading /kaggle/input/visual-feature/motion_feat.h5...\n",
      "Total samples: 500\n",
      "Limited to 500 videos (max_samples=500)\n",
      "Loading /kaggle/input/visual-feature/appearance_feat.h5...\n",
      "Loading /kaggle/input/visual-feature/motion_feat.h5...\n",
      "Total samples: 500\n",
      "Loaded 5429 videos from /kaggle/input/casual-vid-data-split/split/test.pkl\n",
      "Loading /kaggle/input/visual-feature/appearance_feat.h5...\n",
      "Loading /kaggle/input/visual-feature/motion_feat.h5...\n",
      "Total samples: 5429\n",
      "\n",
      "======================================================================\n",
      "üìä DATALOADER SUMMARY\n",
      "======================================================================\n",
      "  Split          Videos    Samples    Batches\n",
      "  ---------- ---------- ---------- ----------\n",
      "  Train             500        500        250\n",
      "  Val               500        500        250\n",
      "  Test (FULL)       5429       5429       2715\n",
      "======================================================================\n",
      "  ‚ÑπÔ∏è  Test set always uses ALL data regardless of max_samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating datasets...\")\n",
    "\n",
    "# ============================================================\n",
    "# CREATE DATASETS\n",
    "# ============================================================\n",
    "dataset_kwargs = dict(\n",
    "    n_query=args.n_query,\n",
    "    obj_num=args.objs,\n",
    "    sample_list_path=args.sample_list_path,\n",
    "    video_feature_path=args.video_feature_path,\n",
    "    text_annotation_path=args.text_annotation_path,\n",
    "    qtype=args.qtype,\n",
    "    max_samples=args.max_samples\n",
    ")\n",
    "\n",
    "train_dataset = VideoQADataset(split='train', **dataset_kwargs)\n",
    "val_dataset = VideoQADataset(split='val', **dataset_kwargs)\n",
    "\n",
    "# Test set LU√îN d√πng to√†n b·ªô data (kh√¥ng gi·ªõi h·∫°n max_samples)\n",
    "test_kwargs = dataset_kwargs.copy()\n",
    "test_kwargs['max_samples'] = None  # Force full test set\n",
    "test_dataset = VideoQADataset(split='test', **test_kwargs)\n",
    "\n",
    "# ============================================================\n",
    "# CREATE DATALOADERS (optimized for multi-GPU)\n",
    "# ============================================================\n",
    "loader_kwargs = dict(\n",
    "    batch_size=args.bs,\n",
    "    num_workers=args.num_workers if args.use_multi_gpu else 0,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2 if args.num_workers > 0 else None,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True,drop_last=True, **loader_kwargs)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, **loader_kwargs)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, **loader_kwargs)\n",
    "\n",
    "# ============================================================\n",
    "# DATASET SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä DATALOADER SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  {'Split':<10} {'Videos':>10} {'Samples':>10} {'Batches':>10}\")\n",
    "print(f\"  {'-'*10} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "for name, dataset, loader in [\n",
    "    (\"Train\", train_dataset, train_loader),\n",
    "    (\"Val\", val_dataset, val_loader),\n",
    "    (\"Test (FULL)\", test_dataset, test_loader)\n",
    "]:\n",
    "    n_vids = len(dataset.vids) if hasattr(dataset, 'vids') else \"?\"\n",
    "    print(f\"  {name:<10} {n_vids:>10} {len(dataset):>10} {len(loader):>10}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  ‚ÑπÔ∏è  Test set always uses ALL data regardless of max_samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T15:59:41.241309Z",
     "iopub.status.busy": "2025-12-08T15:59:41.241059Z",
     "iopub.status.idle": "2025-12-08T15:59:41.810504Z",
     "shell.execute_reply": "2025-12-08T15:59:41.809575Z",
     "shell.execute_reply.started": "2025-12-08T15:59:41.241284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying data sample...\n",
      "\n",
      "  Frame features:  torch.Size([2, 8, 4096])\n",
      "  Object features: torch.Size([2, 8, 20, 2053])\n",
      "  Batch size:      2\n",
      "\n",
      "  Sample question: What will happen if [person_1] cries?...\n",
      "  Sample answer:   [CLS] What will happen if [person_1] cries? [SEP] [person_3]...\n",
      "  Ground truth:    0\n",
      "  Question key:    tEdsMfaFCQM_000134_000144_4\n",
      "\n",
      "‚úÖ Data verification complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# VERIFY DATA SAMPLE\n",
    "# ============================================================\n",
    "print(\"üîç Verifying data sample...\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    vid_frame_feat, vid_obj_feat, qns_word, ans_word, ans_id, qns_key = batch\n",
    "    \n",
    "    print(f\"\\n  Frame features:  {vid_frame_feat.shape}\")\n",
    "    print(f\"  Object features: {vid_obj_feat.shape}\")\n",
    "    print(f\"  Batch size:      {len(qns_word)}\")\n",
    "    print(f\"\\n  Sample question: {qns_word[0][:80]}...\")\n",
    "    print(f\"  Sample answer:   {ans_word[0][0][:60]}...\")\n",
    "    print(f\"  Ground truth:    {ans_id[0].item()}\")\n",
    "    print(f\"  Question key:    {qns_key[0]}\")\n",
    "    break\n",
    "\n",
    "print(\"\\n‚úÖ Data verification complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T15:59:41.813501Z",
     "iopub.status.busy": "2025-12-08T15:59:41.813209Z",
     "iopub.status.idle": "2025-12-08T16:00:06.984969Z",
     "shell.execute_reply": "2025-12-08T16:00:06.983147Z",
     "shell.execute_reply.started": "2025-12-08T15:59:41.813481Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246bd026c3194f66a1e974da3140a0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 15:59:50.250529: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765209590.467672      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765209590.540981      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19cd74957c04533821698f6ac838e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7c7ee9fb1d443aa4381aaf8097c186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ef4a4199f446c582fe6cf97ad1226e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fce971156d43ef9e027f1fdda49949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73753d425e3044fbb29e683791935c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Üí Wrapping model with DataParallel (2 GPUs)\n",
      "\n",
      "======================================================================\n",
      "üß† MODEL SUMMARY\n",
      "======================================================================\n",
      "  Total parameters:     180.36M\n",
      "  Trainable parameters: 180.36M\n",
      "  Multi-GPU:            True\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CREATE MODEL\n",
    "# ============================================================\n",
    "print(\"üèóÔ∏è Creating model...\")\n",
    "\n",
    "model_config = {\n",
    "    'd_model': args.d_model,\n",
    "    'word_dim': args.word_dim,\n",
    "    'encoder_dropout': args.encoder_dropout,\n",
    "    'dropout': args.dropout,\n",
    "    'num_encoder_layers': args.num_encoder_layers,\n",
    "    'num_decoder_layers': args.num_decoder_layers,\n",
    "    'nheads': args.nheads,\n",
    "    'normalize_before': args.normalize_before,\n",
    "    'activation': args.activation,\n",
    "    'text_encoder_type': args.text_encoder_type,\n",
    "    'freeze_text_encoder': args.freeze_text_encoder,\n",
    "    'text_pool_mode': args.text_pool_mode,\n",
    "    'n_query': args.n_query,\n",
    "    'objs': args.objs,\n",
    "    'topK_frame': args.topK_frame,\n",
    "    'topK_obj': args.topK_obj,\n",
    "    'hard_eval': args.hard_eval,\n",
    "    'frame_feat_dim': args.frame_feat_dim,\n",
    "    'obj_feat_dim': args.obj_feat_dim,\n",
    "    'device': device,\n",
    "}\n",
    "\n",
    "model = VideoQAmodel(**model_config)\n",
    "\n",
    "# ============================================================\n",
    "# MULTI-GPU SETUP (DataParallel)\n",
    "# ============================================================\n",
    "if args.use_multi_gpu and torch.cuda.device_count() > 1:\n",
    "    print(f\"  ‚Üí Wrapping model with DataParallel ({torch.cuda.device_count()} GPUs)\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# ============================================================\n",
    "# MODEL SUMMARY\n",
    "# ============================================================\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üß† MODEL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Total parameters:     {total_params / 1e6:.2f}M\")\n",
    "print(f\"  Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
    "print(f\"  Multi-GPU:            {args.use_multi_gpu and torch.cuda.device_count() > 1}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T16:00:06.988371Z",
     "iopub.status.busy": "2025-12-08T16:00:06.987590Z",
     "iopub.status.idle": "2025-12-08T16:00:07.055277Z",
     "shell.execute_reply": "2025-12-08T16:00:07.046947Z",
     "shell.execute_reply.started": "2025-12-08T16:00:06.988337Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader, criterion, device, epoch, wandb_run=None):\n",
    "    \"\"\"Train for one epoch with detailed logging\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    answers = []\n",
    "    batch_times = []\n",
    "    \n",
    "    # Per question type tracking\n",
    "    qtype_correct = defaultdict(int)\n",
    "    qtype_total = defaultdict(int)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, inputs in enumerate(train_loader):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "        vid_frame_feat = vid_frame_feat.to(device)\n",
    "        vid_obj_feat = vid_obj_feat.to(device)\n",
    "        ans_targets = ans_id.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "        loss = criterion(out, ans_targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        pred = out.max(-1)[1].cpu()\n",
    "        predictions.append(pred)\n",
    "        answers.append(ans_id)\n",
    "        \n",
    "        # Track per question type accuracy\n",
    "        for qkey, p, a in zip(qns_keys, pred.numpy(), ans_id.numpy()):\n",
    "            qtype = int(qkey.split('_')[-1])\n",
    "            qtype_total[qtype] += 1\n",
    "            if p == a:\n",
    "                qtype_correct[qtype] += 1\n",
    "        \n",
    "        batch_times.append(time.time() - batch_start)\n",
    "        \n",
    "        # Logging\n",
    "        if (batch_idx + 1) % args.log_interval == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            avg_time = np.mean(batch_times[-args.log_interval:])\n",
    "            print(f\"    Batch [{batch_idx+1:>4}/{len(train_loader)}] \"\n",
    "                  f\"Loss: {loss.item():.4f} (avg: {avg_loss:.4f}) \"\n",
    "                  f\"Time: {avg_time:.3f}s/batch\")\n",
    "            \n",
    "            if wandb_run:\n",
    "                wandb_run.log({\n",
    "                    \"train/batch_loss\": loss.item(),\n",
    "                    \"train/avg_loss\": avg_loss,\n",
    "                    \"train/batch_time\": avg_time,\n",
    "                }, step=epoch * len(train_loader) + batch_idx)\n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    all_preds = torch.cat(predictions, dim=0).long()\n",
    "    all_ans = torch.cat(answers, dim=0).long()\n",
    "    epoch_acc = (all_preds == all_ans).sum().item() * 100.0 / len(all_ans)\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Per question type accuracy\n",
    "    qtype_acc = {}\n",
    "    qtype_names = ['Des', 'Exp', 'Pred-A', 'Pred-R', 'CF-A', 'CF-R']\n",
    "    for qt in range(6):\n",
    "        if qtype_total[qt] > 0:\n",
    "            qtype_acc[qtype_names[qt]] = qtype_correct[qt] * 100.0 / qtype_total[qt]\n",
    "    \n",
    "    return {\n",
    "        'loss': epoch_loss,\n",
    "        'acc': epoch_acc,\n",
    "        'time': epoch_time,\n",
    "        'qtype_acc': qtype_acc\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device, split_name='val'):\n",
    "    \"\"\"Evaluate with detailed per-type accuracy\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    answers = []\n",
    "    qtype_correct = defaultdict(int)\n",
    "    qtype_total = defaultdict(int)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "            vid_frame_feat = vid_frame_feat.to(device)\n",
    "            vid_obj_feat = vid_obj_feat.to(device)\n",
    "            \n",
    "            out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "            pred = out.max(-1)[1].cpu()\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            answers.append(ans_id)\n",
    "            \n",
    "            for qkey, p, a in zip(qns_keys, pred.numpy(), ans_id.numpy()):\n",
    "                qtype = int(qkey.split('_')[-1])\n",
    "                qtype_total[qtype] += 1\n",
    "                if p == a:\n",
    "                    qtype_correct[qtype] += 1\n",
    "    \n",
    "    all_preds = torch.cat(predictions, dim=0).long()\n",
    "    all_ans = torch.cat(answers, dim=0).long()\n",
    "    overall_acc = (all_preds == all_ans).sum().item() * 100.0 / len(all_ans)\n",
    "    \n",
    "    # Per question type accuracy\n",
    "    qtype_names = ['Des', 'Exp', 'Pred-A', 'Pred-R', 'CF-A', 'CF-R']\n",
    "    qtype_acc = {}\n",
    "    for qt in range(6):\n",
    "        if qtype_total[qt] > 0:\n",
    "            qtype_acc[qtype_names[qt]] = qtype_correct[qt] * 100.0 / qtype_total[qt]\n",
    "    \n",
    "    # Combined metrics (Pred = both Pred-A and Pred-R correct for same video)\n",
    "    # This is computed at video level, need results dict for that\n",
    "    \n",
    "    return {\n",
    "        'acc': overall_acc,\n",
    "        'qtype_acc': qtype_acc,\n",
    "        'n_samples': len(all_ans)\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_and_save(model, data_loader, device, save_path):\n",
    "    \"\"\"Generate predictions and save to JSON\"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "            vid_frame_feat = vid_frame_feat.to(device)\n",
    "            vid_obj_feat = vid_obj_feat.to(device)\n",
    "            \n",
    "            out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "            pred = out.max(-1)[1].cpu()\n",
    "            \n",
    "            for qid, p, a in zip(qns_keys, pred.numpy(), ans_id.numpy()):\n",
    "                results[qid] = {'prediction': int(p), 'answer': int(a)}\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    correct = sum(1 for v in results.values() if v['prediction'] == v['answer'])\n",
    "    acc = correct * 100.0 / len(results)\n",
    "    \n",
    "    return results, acc\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T16:00:07.056499Z",
     "iopub.status.busy": "2025-12-08T16:00:07.056180Z",
     "iopub.status.idle": "2025-12-08T16:00:07.270409Z",
     "shell.execute_reply": "2025-12-08T16:00:07.268875Z",
     "shell.execute_reply.started": "2025-12-08T16:00:07.056470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optimizer and scheduler created\n",
      "   Main LR: 0.0001\n",
      "   Text encoder LR: 1e-05\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SETUP OPTIMIZER, SCHEDULER, CRITERION\n",
    "# ============================================================\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "os.makedirs('./prediction', exist_ok=True)\n",
    "\n",
    "# Get base model for parameter groups (handle DataParallel)\n",
    "base_model = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "# Optimizer with different LR for text encoder\n",
    "param_groups = [\n",
    "    {\n",
    "        \"params\": [p for n, p in base_model.named_parameters() \n",
    "                   if \"text_encoder\" not in n and p.requires_grad],\n",
    "        \"lr\": args.lr\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in base_model.named_parameters() \n",
    "                   if \"text_encoder\" in n and p.requires_grad],\n",
    "        \"lr\": args.text_encoder_lr\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, weight_decay=args.decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=args.gamma, \n",
    "                               patience=args.patience, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"‚úÖ Optimizer and scheduler created\")\n",
    "print(f\"   Main LR: {args.lr}\")\n",
    "print(f\"   Text encoder LR: {args.text_encoder_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T16:00:07.274769Z",
     "iopub.status.busy": "2025-12-08T16:00:07.274233Z",
     "iopub.status.idle": "2025-12-08T16:00:14.738289Z",
     "shell.execute_reply": "2025-12-08T16:00:14.737706Z",
     "shell.execute_reply.started": "2025-12-08T16:00:07.274723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/tranSTR_Casual/wandb/run-20251208_160007-okts7dm0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/introSE/CausalVidQA-TranSTR/runs/okts7dm0' target=\"_blank\">causalvid_2gpu</a></strong> to <a href='https://wandb.ai/introSE/CausalVidQA-TranSTR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/introSE/CausalVidQA-TranSTR' target=\"_blank\">https://wandb.ai/introSE/CausalVidQA-TranSTR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/introSE/CausalVidQA-TranSTR/runs/okts7dm0' target=\"_blank\">https://wandb.ai/introSE/CausalVidQA-TranSTR/runs/okts7dm0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ W&B initialized: https://wandb.ai/introSE/CausalVidQA-TranSTR/runs/okts7dm0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# INITIALIZE WANDB\n",
    "# ============================================================\n",
    "wandb_config = {\n",
    "    \"architecture\": \"TranSTR\",\n",
    "    \"dataset\": \"CausalVidQA\",\n",
    "    \"epochs\": args.epoch,\n",
    "    \"batch_size\": args.bs,\n",
    "    \"learning_rate\": args.lr,\n",
    "    \"text_encoder_lr\": args.text_encoder_lr,\n",
    "    \"text_encoder\": args.text_encoder_type,\n",
    "    \"d_model\": args.d_model,\n",
    "    \"topK_frame\": args.topK_frame,\n",
    "    \"topK_obj\": args.topK_obj,\n",
    "    \"n_objects\": args.objs,\n",
    "    \"num_encoder_layers\": args.num_encoder_layers,\n",
    "    \"num_decoder_layers\": args.num_decoder_layers,\n",
    "    \"multi_gpu\": args.use_multi_gpu,\n",
    "    \"n_gpus\": torch.cuda.device_count(),\n",
    "    \"train_samples\": len(train_dataset),\n",
    "    \"val_samples\": len(val_dataset),\n",
    "    \"test_samples\": len(test_dataset),\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=args.project_name,\n",
    "    name=args.run_name,\n",
    "    config=wandb_config,\n",
    "    tags=[\"causalvid\", \"multi-gpu\" if args.use_multi_gpu else \"single-gpu\"]\n",
    ")\n",
    "\n",
    "# Log dataset info\n",
    "wandb.log({\n",
    "    \"data/train_videos\": len(train_dataset.vids) if hasattr(train_dataset, 'vids') else 0,\n",
    "    \"data/val_videos\": len(val_dataset.vids) if hasattr(val_dataset, 'vids') else 0,\n",
    "    \"data/test_videos\": len(test_dataset.vids) if hasattr(test_dataset, 'vids') else 0,\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ W&B initialized: {run.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T16:00:14.739296Z",
     "iopub.status.busy": "2025-12-08T16:00:14.739055Z",
     "iopub.status.idle": "2025-12-08T16:00:17.674879Z",
     "shell.execute_reply": "2025-12-08T16:00:17.673634Z",
     "shell.execute_reply.started": "2025-12-08T16:00:14.739271Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ STARTING TRAINING: causalvid_2gpu\n",
      "   Epochs: 20 | Batch size: 2 | GPUs: 2\n",
      "   Early stopping: 5 epochs without improvement\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìö EPOCH [1/20]\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/model.py\", line 102, in forward\n    frame_local, frame_att = self.frame_decoder(frame_feat,\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/multimodal_transformer.py\", line 59, in forward\n    output, c_att = layer(output, memory, tgt_mask=tgt_mask,\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/multimodal_transformer.py\", line 158, in forward\n    tgt2, c_att = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/attention.py\", line 66, in forward\n    key_padding_mask = key_padding_mask.reshape(bs, k_length)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: shape '[1, 18]' is invalid for input of size 36\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38/1093866606.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# ============ TRAIN ============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# ============ EVALUATE ============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_38/3341790955.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, train_loader, criterion, device, epoch, wandb_run)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_frame_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_obj_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqns_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/model.py\", line 102, in forward\n    frame_local, frame_att = self.frame_decoder(frame_feat,\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/multimodal_transformer.py\", line 59, in forward\n    output, c_att = layer(output, memory, tgt_mask=tgt_mask,\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/multimodal_transformer.py\", line 158, in forward\n    tgt2, c_att = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/attention.py\", line 66, in forward\n    key_padding_mask = key_padding_mask.reshape(bs, k_length)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: shape '[1, 18]' is invalid for input of size 36\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP WITH EARLY STOPPING\n",
    "# ============================================================\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = f'./models/best_model-{args.run_name}.ckpt'\n",
    "history = {'train': [], 'val': [], 'test': []}\n",
    "epochs_without_improvement = 0  # Early stopping counter\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"üöÄ STARTING TRAINING: {args.run_name}\")\n",
    "print(f\"   Epochs: {args.epoch} | Batch size: {args.bs} | GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"   Early stopping: {args.early_stopping_patience} epochs without improvement\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(1, args.epoch + 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìö EPOCH [{epoch}/{args.epoch}]\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ============ TRAIN ============\n",
    "    train_metrics = train_epoch(model, optimizer, train_loader, criterion, device, epoch, run)\n",
    "    \n",
    "    # ============ EVALUATE ============\n",
    "    val_metrics = evaluate(model, val_loader, device, 'val')\n",
    "    test_metrics = evaluate(model, test_loader, device, 'test')\n",
    "    \n",
    "    # ============ UPDATE SCHEDULER ============\n",
    "    scheduler.step(val_metrics['acc'])\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # ============ SAVE BEST MODEL & EARLY STOPPING ============\n",
    "    is_best = val_metrics['acc'] > best_val_acc\n",
    "    if is_best:\n",
    "        best_val_acc = val_metrics['acc']\n",
    "        best_epoch = epoch\n",
    "        epochs_without_improvement = 0  # Reset counter\n",
    "        # Save model (handle DataParallel)\n",
    "        state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
    "        torch.save(state_dict, best_model_path)\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # ============ LOGGING ============\n",
    "    print(f\"\\n  üìä Results:\")\n",
    "    print(f\"     {'Metric':<15} {'Train':>10} {'Val':>10} {'Test':>10}\")\n",
    "    print(f\"     {'-'*15} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "    print(f\"     {'Loss':<15} {train_metrics['loss']:>10.4f} {'-':>10} {'-':>10}\")\n",
    "    print(f\"     {'Accuracy':<15} {train_metrics['acc']:>9.2f}% {val_metrics['acc']:>9.2f}% {test_metrics['acc']:>9.2f}%\")\n",
    "    \n",
    "    # Per question type accuracy\n",
    "    print(f\"\\n  üìà Per Question Type Accuracy (Val):\")\n",
    "    qtype_order = ['Des', 'Exp', 'Pred-A', 'Pred-R', 'CF-A', 'CF-R']\n",
    "    for qt in qtype_order:\n",
    "        if qt in val_metrics['qtype_acc']:\n",
    "            print(f\"     {qt:<10}: {val_metrics['qtype_acc'][qt]:>6.2f}%\")\n",
    "    \n",
    "    print(f\"\\n  ‚è±Ô∏è  Time: {train_metrics['time']:.1f}s | LR: {current_lr:.2e}\")\n",
    "    print(f\"  üìâ No improvement: {epochs_without_improvement}/{args.early_stopping_patience} epochs\")\n",
    "    if is_best:\n",
    "        print(f\"  üíæ Saved best model! (Val acc: {best_val_acc:.2f}%)\")\n",
    "    \n",
    "    # ============ WANDB LOGGING ============\n",
    "    wandb_log = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train/loss\": train_metrics['loss'],\n",
    "        \"train/acc\": train_metrics['acc'],\n",
    "        \"val/acc\": val_metrics['acc'],\n",
    "        \"test/acc\": test_metrics['acc'],\n",
    "        \"lr\": current_lr,\n",
    "        \"epoch_time\": train_metrics['time'],\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"epochs_without_improvement\": epochs_without_improvement,\n",
    "    }\n",
    "    \n",
    "    # Log per question type accuracy\n",
    "    for qt, acc in train_metrics['qtype_acc'].items():\n",
    "        wandb_log[f\"train/acc_{qt}\"] = acc\n",
    "    for qt, acc in val_metrics['qtype_acc'].items():\n",
    "        wandb_log[f\"val/acc_{qt}\"] = acc\n",
    "    for qt, acc in test_metrics['qtype_acc'].items():\n",
    "        wandb_log[f\"test/acc_{qt}\"] = acc\n",
    "    \n",
    "    wandb.log(wandb_log)\n",
    "    \n",
    "    # Save checkpoint every N epochs\n",
    "    if epoch % args.save_every == 0:\n",
    "        ckpt_path = f'./models/checkpoint-{args.run_name}-ep{epoch}.ckpt'\n",
    "        state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': state_dict,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_metrics['acc'],\n",
    "        }, ckpt_path)\n",
    "        print(f\"  üìÅ Checkpoint saved: {ckpt_path}\")\n",
    "    \n",
    "    # ============ EARLY STOPPING CHECK ============\n",
    "    if epochs_without_improvement >= args.early_stopping_patience:\n",
    "        print(f\"\\n  ‚ö†Ô∏è EARLY STOPPING: No improvement for {args.early_stopping_patience} epochs\")\n",
    "        print(f\"     Best val acc: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "        wandb.log({\"early_stopped\": True, \"stopped_at_epoch\": epoch})\n",
    "        break\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING COMPLETE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Best epoch: {best_epoch}\")\n",
    "print(f\"   Best val accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   Model saved: {best_model_path}\")\n",
    "if epochs_without_improvement >= args.early_stopping_patience:\n",
    "    print(f\"   Stopped early at epoch {epoch}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-08T16:00:17.675585Z",
     "iopub.status.idle": "2025-12-08T16:00:17.676005Z",
     "shell.execute_reply": "2025-12-08T16:00:17.675820Z",
     "shell.execute_reply.started": "2025-12-08T16:00:17.675804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL EVALUATION WITH BEST MODEL\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä FINAL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load best model\n",
    "print(\"\\n  Loading best model...\")\n",
    "base_model = model.module if hasattr(model, 'module') else model\n",
    "base_model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Predict on test set\n",
    "result_path = f'./prediction/{args.run_name}-ep{best_epoch}-val{best_val_acc:.2f}.json'\n",
    "results, test_acc = predict_and_save(model, test_loader, device, result_path)\n",
    "\n",
    "print(f\"\\n  Test accuracy: {test_acc:.2f}%\")\n",
    "print(f\"  Results saved: {result_path}\")\n",
    "\n",
    "# Detailed evaluation by question type\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"  Detailed Results by Question Type:\")\n",
    "print(\"-\" * 70)\n",
    "eval_mc.accuracy_metric_cvid(result_path)\n",
    "\n",
    "# Log final results to wandb\n",
    "wandb.log({\n",
    "    \"final/test_acc\": test_acc,\n",
    "    \"final/best_epoch\": best_epoch,\n",
    "    \"final/best_val_acc\": best_val_acc,\n",
    "})\n",
    "\n",
    "# Save results artifact\n",
    "artifact = wandb.Artifact(f'predictions-{args.run_name}', type='predictions')\n",
    "artifact.add_file(result_path)\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-08T16:00:17.677111Z",
     "iopub.status.idle": "2025-12-08T16:00:17.677343Z",
     "shell.execute_reply": "2025-12-08T16:00:17.677248Z",
     "shell.execute_reply.started": "2025-12-08T16:00:17.677237Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE TO KAGGLE OUTPUT & FINISH WANDB\n",
    "# ============================================================\n",
    "import shutil\n",
    "\n",
    "output_dir = '/kaggle/working'\n",
    "if os.path.exists(output_dir):\n",
    "    # Copy best model\n",
    "    shutil.copy(best_model_path, os.path.join(output_dir, f'best_model-{args.run_name}.ckpt'))\n",
    "    # Copy predictions\n",
    "    shutil.copy(result_path, output_dir)\n",
    "    print(f\"‚úÖ Files saved to {output_dir}\")\n",
    "else:\n",
    "    print(\"  Not running on Kaggle, files saved locally\")\n",
    "\n",
    "# Save model artifact to wandb\n",
    "model_artifact = wandb.Artifact(f'model-{args.run_name}', type='model')\n",
    "model_artifact.add_file(best_model_path)\n",
    "wandb.log_artifact(model_artifact)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "print(\"‚úÖ W&B run finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-08T16:00:17.678209Z",
     "iopub.status.idle": "2025-12-08T16:00:17.678461Z",
     "shell.execute_reply": "2025-12-08T16:00:17.678346Z",
     "shell.execute_reply.started": "2025-12-08T16:00:17.678333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Predict on test set\n",
    "result_path = f'./prediction/{args.v}-{best_epoch}-{best_val_acc:.2f}.json'\n",
    "results, test_acc = predict_and_save(model, test_loader, device, result_path)\n",
    "\n",
    "print(f\"\\nüìä Final Test Results:\")\n",
    "print(f\"   Overall Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"   Results saved to: {result_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-08T16:00:17.679739Z",
     "iopub.status.idle": "2025-12-08T16:00:17.680075Z",
     "shell.execute_reply": "2025-12-08T16:00:17.679925Z",
     "shell.execute_reply.started": "2025-12-08T16:00:17.679910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Quick test v·ªõi 10 videos\n",
    "# args.max_samples = 10\n",
    "# args.bs = 4\n",
    "# args.epoch = 2\n",
    "# \n",
    "# # Re-create datasets v·ªõi max_samples\n",
    "# train_dataset = VideoQADataset(\n",
    "#     split='train', n_query=args.n_query, obj_num=args.objs,\n",
    "#     sample_list_path=args.sample_list_path,\n",
    "#     video_feature_path=args.video_feature_path,\n",
    "#     text_annotation_path=args.text_annotation_path,\n",
    "#     qtype=args.qtype, max_samples=args.max_samples\n",
    "# )\n",
    "# print(f\"Quick test dataset: {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating pretrained model B2A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8208331,
     "sourceId": 12969233,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8210299,
     "sourceId": 12972016,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8210716,
     "sourceId": 12972597,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8211749,
     "sourceId": 12977127,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8383289,
     "isSourceIdPinned": true,
     "sourceId": 13226170,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8208446,
     "sourceId": 13229803,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8208348,
     "sourceId": 13919566,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8888719,
     "sourceId": 13946226,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
