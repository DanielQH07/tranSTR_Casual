{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a98c20",
   "metadata": {},
   "source": [
    "## 1. ðŸ“¦ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers einops h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17953d4d",
   "metadata": {},
   "source": [
    "## 2. ðŸ“‚ Data Paths (Kaggle Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths tá»« Kaggle Input\n",
    "text_feature_path = '/kaggle/input/text-feature'\n",
    "visual_feature_path = '/kaggle/input/visual-feature'\n",
    "split_path = '/kaggle/input/dataset-split-1'\n",
    "text_annotation_path = '/kaggle/input/text-annotation'\n",
    "\n",
    "print(\"âœ… Data paths:\")\n",
    "print(f\"  Text features: {text_feature_path}\")\n",
    "print(f\"  Visual features: {visual_feature_path}\")\n",
    "print(f\"  Split files: {split_path}\")\n",
    "print(f\"  Text annotations: {text_annotation_path}\")\n",
    "\n",
    "# Verify paths exist\n",
    "import os\n",
    "for path in [text_feature_path, visual_feature_path, split_path, text_annotation_path]:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  âœ“ {path} exists\")\n",
    "    else:\n",
    "        print(f\"  âœ— {path} NOT FOUND!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29601a5f",
   "metadata": {},
   "source": [
    "## 3. âš™ï¸ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "class Config:\n",
    "    # Version name\n",
    "    v = \"causalvid_kaggle\"\n",
    "    \n",
    "    # Data paths (from Kaggle input)\n",
    "    sample_list_path = split_path\n",
    "    video_feature_path = visual_feature_path\n",
    "    text_annotation_path = text_annotation_path\n",
    "    \n",
    "    # Training params\n",
    "    bs = 8                     # Batch size (reduce if OOM)\n",
    "    lr = 1e-5                  # Learning rate\n",
    "    epoch = 5                  # Number of epochs\n",
    "    gpu = 0                    # GPU ID\n",
    "    \n",
    "    # Dataset params\n",
    "    dataset = 'causal-vid'\n",
    "    qtype = -1                 # -1 for all, 0-5 for specific\n",
    "    max_samples = 20           # ðŸ”¥ Giá»›i háº¡n sá»‘ video (20 videos = 120 samples vá»›i qtype=-1)\n",
    "    objs = 20                  # Number of objects\n",
    "    n_query = 5                # Number of answer choices\n",
    "    \n",
    "    # Model params\n",
    "    d_model = 768\n",
    "    word_dim = 768\n",
    "    topK_frame = 8             # Top-K frames\n",
    "    topK_obj = 5               # Top-K objects\n",
    "    hard_eval = False\n",
    "    frame_feat_dim = 4096      # app + mot concatenated\n",
    "    obj_feat_dim = 2053        # 2048 + 5 bbox\n",
    "    \n",
    "    # Transformer params\n",
    "    num_encoder_layers = 1\n",
    "    num_decoder_layers = 1\n",
    "    nheads = 8\n",
    "    normalize_before = False\n",
    "    activation = 'relu'\n",
    "    \n",
    "    # Dropout\n",
    "    dropout = 0.1\n",
    "    encoder_dropout = 0.1\n",
    "    \n",
    "    # Text encoder\n",
    "    text_encoder_type = \"microsoft/deberta-base\"\n",
    "    text_encoder_lr = 5e-6\n",
    "    freeze_text_encoder = False\n",
    "    text_pool_mode = 0\n",
    "    \n",
    "    # Scheduler\n",
    "    patience = 1\n",
    "    gamma = 0.25\n",
    "    decay = 0.001\n",
    "    \n",
    "    # Contrastive learning\n",
    "    pos_ratio = 0.7\n",
    "    neg_ratio = 0.3\n",
    "    a = 1\n",
    "    \n",
    "    # Early stopping\n",
    "    es = False\n",
    "\n",
    "args = Config()\n",
    "print(f\"âœ… Config loaded: {args.v}\")\n",
    "print(f\"   Batch size: {args.bs}\")\n",
    "print(f\"   Epochs: {args.epoch}\")\n",
    "print(f\"   Max samples: {args.max_samples} videos ({args.max_samples * 6} samples vá»›i qtype=-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081795f3",
   "metadata": {},
   "source": [
    "## 4. ðŸ“š Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6efa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Import local modules\n",
    "from DataLoader import VideoQADataset\n",
    "from networks.model import VideoQAmodel\n",
    "import eval_mc\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed=999):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(999)\n",
    "print(\"âœ… Modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e945a",
   "metadata": {},
   "source": [
    "## 5. ðŸ“Š Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60fd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating datasets...\")\n",
    "\n",
    "# Train dataset\n",
    "train_dataset = VideoQADataset(\n",
    "    split='train',\n",
    "    n_query=args.n_query,\n",
    "    obj_num=args.objs,\n",
    "    sample_list_path=args.sample_list_path,\n",
    "    video_feature_path=args.video_feature_path,\n",
    "    text_annotation_path=args.text_annotation_path,\n",
    "    qtype=args.qtype,\n",
    "    max_samples=args.max_samples\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "val_dataset = VideoQADataset(\n",
    "    split='val',\n",
    "    n_query=args.n_query,\n",
    "    obj_num=args.objs,\n",
    "    sample_list_path=args.sample_list_path,\n",
    "    video_feature_path=args.video_feature_path,\n",
    "    text_annotation_path=args.text_annotation_path,\n",
    "    qtype=args.qtype,\n",
    "    max_samples=args.max_samples\n",
    ")\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = VideoQADataset(\n",
    "    split='test',\n",
    "    n_query=args.n_query,\n",
    "    obj_num=args.objs,\n",
    "    sample_list_path=args.sample_list_path,\n",
    "    video_feature_path=args.video_feature_path,\n",
    "    text_annotation_path=args.text_annotation_path,\n",
    "    qtype=args.qtype,\n",
    "    max_samples=args.max_samples\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=args.bs,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=args.bs,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=args.bs,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… DataLoaders created:\")\n",
    "print(f\"   Train: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
    "print(f\"   Val: {len(val_dataset)} samples, {len(val_loader)} batches\")\n",
    "print(f\"   Test: {len(test_dataset)} samples, {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229298f3",
   "metadata": {},
   "source": [
    "## 6. ðŸ” Check Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7186de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one batch\n",
    "for batch in train_loader:\n",
    "    vid_frame_feat, vid_obj_feat, qns_word, ans_word, ans_id, qns_key = batch\n",
    "    print(\"Sample batch:\")\n",
    "    print(f\"  Frame feat shape: {vid_frame_feat.shape}\")\n",
    "    print(f\"  Object feat shape: {vid_obj_feat.shape}\")\n",
    "    print(f\"  Question: {qns_word[0]}\")\n",
    "    print(f\"  Answer choices: {ans_word[0][0][:100]}...\")\n",
    "    print(f\"  Answer ID: {ans_id[0]}\")\n",
    "    print(f\"  Question key: {qns_key[0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e180162",
   "metadata": {},
   "source": [
    "## 7. ðŸ—ï¸ Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bfaceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert args to dict for model\n",
    "args.device = device\n",
    "config = vars(args)\n",
    "\n",
    "# Create model\n",
    "model = VideoQAmodel(**config)\n",
    "model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nâœ… Model created:\")\n",
    "print(f\"   Total parameters: {total_params/1e6:.2f}M\")\n",
    "print(f\"   Trainable parameters: {trainable_params/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb184acc",
   "metadata": {},
   "source": [
    "## 8. ðŸŽ¯ Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e68a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradScaler for mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    prediction_list = []\n",
    "    answer_list = []\n",
    "    \n",
    "    for batch_idx, inputs in enumerate(train_loader):\n",
    "        vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, _ = inputs\n",
    "        vid_frame_feat = vid_frame_feat.to(device)\n",
    "        vid_obj_feat = vid_obj_feat.to(device)\n",
    "        ans_targets = ans_id.to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "            loss = criterion(out, ans_targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        prediction = out.max(-1)[1]\n",
    "        prediction_list.append(prediction)\n",
    "        answer_list.append(ans_id)\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"    Batch [{batch_idx+1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    predict_answers = torch.cat(prediction_list, dim=0).long().cpu()\n",
    "    ref_answers = torch.cat(answer_list, dim=0).long()\n",
    "    acc = (predict_answers == ref_answers).sum().item() * 100.0 / len(ref_answers)\n",
    "    \n",
    "    return total_loss / len(train_loader), acc\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"Evaluate on validation/test set\"\"\"\n",
    "    model.eval()\n",
    "    prediction_list = []\n",
    "    answer_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, _ = inputs\n",
    "            vid_frame_feat = vid_frame_feat.to(device)\n",
    "            vid_obj_feat = vid_obj_feat.to(device)\n",
    "            \n",
    "            out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "            prediction = out.max(-1)[1]\n",
    "            prediction_list.append(prediction)\n",
    "            answer_list.append(ans_id)\n",
    "    \n",
    "    predict_answers = torch.cat(prediction_list, dim=0).long().cpu()\n",
    "    ref_answers = torch.cat(answer_list, dim=0).long()\n",
    "    acc = (predict_answers == ref_answers).sum().item() * 100.0 / len(ref_answers)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "def predict_and_save(model, data_loader, device, save_path):\n",
    "    \"\"\"Predict and save results\"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    prediction_list = []\n",
    "    answer_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "            vid_frame_feat = vid_frame_feat.to(device)\n",
    "            vid_obj_feat = vid_obj_feat.to(device)\n",
    "            \n",
    "            out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "            prediction = out.max(-1)[1]\n",
    "            prediction_list.append(prediction)\n",
    "            answer_list.append(ans_id)\n",
    "            \n",
    "            for qid, pred, ans in zip(qns_keys, prediction.cpu().numpy(), ans_id.numpy()):\n",
    "                results[qid] = {'prediction': int(pred), 'answer': int(ans)}\n",
    "    \n",
    "    predict_answers = torch.cat(prediction_list, dim=0).long().cpu()\n",
    "    ref_answers = torch.cat(answer_list, dim=0).long()\n",
    "    acc = (predict_answers == ref_answers).sum().item() * 100.0 / len(ref_answers)\n",
    "    \n",
    "    # Save results\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return results, acc\n",
    "\n",
    "print(\"âœ… Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575add3c",
   "metadata": {},
   "source": [
    "## 9. ðŸš€ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "os.makedirs('./prediction', exist_ok=True)\n",
    "\n",
    "# Optimizer with different LR for text encoder\n",
    "param_dicts = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if \"text_encoder\" not in n and p.requires_grad]},\n",
    "    {\"params\": [p for n, p in model.named_parameters() if \"text_encoder\" in n and p.requires_grad], \n",
    "     \"lr\": args.text_encoder_lr}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params=param_dicts, lr=args.lr, weight_decay=args.decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', factor=args.gamma, patience=args.patience, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "print(\"âœ… Optimizer and scheduler created\")\n",
    "print(f\"   Main LR: {args.lr}\")\n",
    "print(f\"   Text encoder LR: {args.text_encoder_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5747d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = f'./models/best_model-{args.v}.ckpt'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸš€ Starting training: {args.v}\")\n",
    "print(f\"   Epochs: {args.epoch}\")\n",
    "print(f\"   Batch size: {args.bs}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(1, args.epoch + 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nðŸ“š Epoch [{epoch}/{args.epoch}]\")\n",
    "    train_loss, train_acc = train_epoch(model, optimizer, train_loader, criterion, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_acc = evaluate(model, val_loader, device)\n",
    "    test_acc = evaluate(model, test_loader, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"   ðŸ’¾ Saved best model!\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"   â±ï¸ Time: {elapsed:.1f}s\")\n",
    "    print(f\"   ðŸ“‰ Loss: {train_loss:.4f}\")\n",
    "    print(f\"   ðŸŽ¯ Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"   ðŸŽ¯ Val Acc: {val_acc:.2f}% {'â­ BEST' if val_acc >= best_val_acc else ''}\")\n",
    "    print(f\"   ðŸŽ¯ Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"âœ… Training completed!\")\n",
    "print(f\"   Best epoch: {best_epoch}\")\n",
    "print(f\"   Best val acc: {best_val_acc:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb5710",
   "metadata": {},
   "source": [
    "## 10. ðŸ“Š Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c6af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Predict on test set\n",
    "result_path = f'./prediction/{args.v}-{best_epoch}-{best_val_acc:.2f}.json'\n",
    "results, test_acc = predict_and_save(model, test_loader, device, result_path)\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Test Results:\")\n",
    "print(f\"   Overall Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"   Results saved to: {result_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b38b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation by question type\n",
    "print(\"\\nðŸ“Š Accuracy by Question Type:\")\n",
    "print(\"=\"*60)\n",
    "eval_mc.accuracy_metric_cvid(result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b13304",
   "metadata": {},
   "source": [
    "## 11. ðŸ’¾ Save Model to Kaggle Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cbf1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model to /kaggle/working for download\n",
    "import shutil\n",
    "\n",
    "output_dir = '/kaggle/working'\n",
    "if os.path.exists(output_dir):\n",
    "    # Copy best model\n",
    "    shutil.copy(best_model_path, os.path.join(output_dir, f'best_model-{args.v}.ckpt'))\n",
    "    # Copy predictions\n",
    "    shutil.copy(result_path, output_dir)\n",
    "    print(f\"âœ… Files saved to {output_dir}\")\n",
    "else:\n",
    "    print(\"Not running on Kaggle, files saved locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10fa1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”§ Quick Test (Optional)\n",
    "\n",
    "Uncomment vÃ  cháº¡y cell dÆ°á»›i Ä‘á»ƒ test nhanh vá»›i Ã­t data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7679eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick test vá»›i 10 videos\n",
    "# args.max_samples = 10\n",
    "# args.bs = 4\n",
    "# args.epoch = 2\n",
    "# \n",
    "# # Re-create datasets vá»›i max_samples\n",
    "# train_dataset = VideoQADataset(\n",
    "#     split='train', n_query=args.n_query, obj_num=args.objs,\n",
    "#     sample_list_path=args.sample_list_path,\n",
    "#     video_feature_path=args.video_feature_path,\n",
    "#     text_annotation_path=args.text_annotation_path,\n",
    "#     qtype=args.qtype, max_samples=args.max_samples\n",
    "# )\n",
    "# print(f\"Quick test dataset: {len(train_dataset)} samples\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
