{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-08T17:36:18.255092Z",
     "iopub.status.busy": "2025-12-08T17:36:18.254386Z",
     "iopub.status.idle": "2025-12-08T17:36:18.790863Z",
     "shell.execute_reply": "2025-12-08T17:36:18.789958Z",
     "shell.execute_reply.started": "2025-12-08T17:36:18.255066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'tranSTR_Casual' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# üì• STEP 1: Clone Repository\n",
    "\n",
    "Clone the TranSTR repo from GitHub (contains base code with bugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/DanielQH07/tranSTR_Casual.git\n",
    "print(\"‚úÖ Repository cloned (WARNING: Contains OLD buggy code)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:36:18.792884Z",
     "iopub.status.busy": "2025-12-08T17:36:18.792659Z",
     "iopub.status.idle": "2025-12-08T17:36:18.799869Z",
     "shell.execute_reply": "2025-12-08T17:36:18.799326Z",
     "shell.execute_reply.started": "2025-12-08T17:36:18.792863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/tranSTR_Casual\n"
     ]
    }
   ],
   "source": [
    "# üìÇ STEP 2: Navigate to Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/tranSTR_Casual\n",
    "!pwd  # Verify current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:36:18.801274Z",
     "iopub.status.busy": "2025-12-08T17:36:18.800958Z",
     "iopub.status.idle": "2025-12-08T17:36:18.818177Z",
     "shell.execute_reply": "2025-12-08T17:36:18.817594Z",
     "shell.execute_reply.started": "2025-12-08T17:36:18.801249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ attention.py patched for DataParallel compatibility!\n",
      "   File written to: /kaggle/working/tranSTR_Casual/networks/attention.py\n",
      "\n",
      "üìå KEY FIX:\n",
      "   - Squeeze 3D masks ‚Üí 2D\n",
      "   - Align batch size to local shard\n",
      "   - CREATE new tensor with shape (bs, k_length)\n",
      "   - Copy existing data, cast to bool BEFORE view\n",
      "   - This prevents 'expanded size mismatch' errors\n"
     ]
    }
   ],
   "source": [
    "# üîß STEP 3: PATCH attention.py (CRITICAL - DataParallel Fix)\n",
    "\n",
    "**‚ö†Ô∏è YOU MUST RUN THIS CELL BEFORE TRAINING!**\n",
    "\n",
    "This overwrites the buggy `networks/attention.py` from GitHub with a fixed version that handles DataParallel mask shape mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:36:18.820063Z",
     "iopub.status.busy": "2025-12-08T17:36:18.819865Z",
     "iopub.status.idle": "2025-12-08T17:36:18.830018Z",
     "shell.execute_reply": "2025-12-08T17:36:18.829334Z",
     "shell.execute_reply.started": "2025-12-08T17:36:18.820048Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting networks/model.py\n"
     ]
    }
   ],
   "source": [
    "# üîß STEP 4: PATCH model.py (CRITICAL - repeat_interleave Fix)\n",
    "\n",
    "**‚ö†Ô∏è YOU MUST RUN THIS CELL BEFORE TRAINING!**\n",
    "\n",
    "This overwrites the buggy `networks/model.py` with a fixed version that properly repeats q_local/q_mask for DataParallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:36:18.831021Z",
     "iopub.status.busy": "2025-12-08T17:36:18.830798Z",
     "iopub.status.idle": "2025-12-08T17:36:18.845833Z",
     "shell.execute_reply": "2025-12-08T17:36:18.845257Z",
     "shell.execute_reply.started": "2025-12-08T17:36:18.831000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataLoader.py patched with dimension fix!\n"
     ]
    }
   ],
   "source": [
    "# üîß STEP 5: PATCH DataLoader.py (Feature Dimension Fix)\n",
    "\n",
    "**‚ö†Ô∏è YOU MUST RUN THIS CELL BEFORE TRAINING!**\n",
    "\n",
    "This overwrites `DataLoader.py` to handle variable feature dimensions (squeeze/mean for 3D features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ STEP 6: Verify Patches Applied\n",
    "\n",
    "Check if all 3 critical patches were successfully applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç VERIFYING PATCHES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "checks = []\n",
    "\n",
    "# Check 1: attention.py patch (look for \"new_mask\" keyword)\n",
    "attention_file = 'networks/attention.py'\n",
    "if os.path.exists(attention_file):\n",
    "    with open(attention_file, 'r') as f:\n",
    "        content = f.read()\n",
    "    has_fix = 'new_mask = mask.new_zeros(bs, k_length)' in content\n",
    "    status = \"‚úÖ PATCHED\" if has_fix else \"‚ùå NOT PATCHED\"\n",
    "    checks.append(has_fix)\n",
    "    print(f\"  {status}: {attention_file}\")\n",
    "    if not has_fix:\n",
    "        print(f\"      ‚ö†Ô∏è  File exists but doesn't contain the fix!\")\n",
    "        print(f\"      ‚Üí Go back and RUN CELL 3 (Patch attention.py)\")\n",
    "else:\n",
    "    print(f\"  ‚ùå MISSING: {attention_file}\")\n",
    "    checks.append(False)\n",
    "\n",
    "# Check 2: model.py patch (look for \"repeat_interleave\" keyword)\n",
    "model_file = 'networks/model.py'\n",
    "if os.path.exists(model_file):\n",
    "    with open(model_file, 'r') as f:\n",
    "        content = f.read()\n",
    "    has_fix = 'q_local_repeated = q_local.repeat_interleave' in content\n",
    "    status = \"‚úÖ PATCHED\" if has_fix else \"‚ùå NOT PATCHED\"\n",
    "    checks.append(has_fix)\n",
    "    print(f\"  {status}: {model_file}\")\n",
    "    if not has_fix:\n",
    "        print(f\"      ‚ö†Ô∏è  File exists but doesn't contain the fix!\")\n",
    "        print(f\"      ‚Üí Go back and RUN CELL 4 (Patch model.py)\")\n",
    "else:\n",
    "    print(f\"  ‚ùå MISSING: {model_file}\")\n",
    "    checks.append(False)\n",
    "\n",
    "# Check 3: DataLoader.py patch (look for dimension fix comment)\n",
    "dataloader_file = 'DataLoader.py'\n",
    "if os.path.exists(dataloader_file):\n",
    "    with open(dataloader_file, 'r') as f:\n",
    "        content = f.read()\n",
    "    has_fix = '=== FIX: Handle different feature shapes ===' in content\n",
    "    status = \"‚úÖ PATCHED\" if has_fix else \"‚ùå NOT PATCHED\"\n",
    "    checks.append(has_fix)\n",
    "    print(f\"  {status}: {dataloader_file}\")\n",
    "    if not has_fix:\n",
    "        print(f\"      ‚ö†Ô∏è  File exists but doesn't contain the fix!\")\n",
    "        print(f\"      ‚Üí Go back and RUN CELL 5 (Patch DataLoader.py)\")\n",
    "else:\n",
    "    print(f\"  ‚ùå MISSING: {dataloader_file}\")\n",
    "    checks.append(False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if all(checks):\n",
    "    print(\"‚úÖ ALL PATCHES VERIFIED! You can proceed with training.\")\n",
    "else:\n",
    "    print(\"‚ùå SOME PATCHES ARE MISSING!\")\n",
    "    print(\"   ‚Üí Go back and run Cells 3, 4, 5 in order\")\n",
    "    print(\"   ‚Üí Then re-run this verification cell\")\n",
    "    \n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:36:18.846981Z",
     "iopub.status.busy": "2025-12-08T17:36:18.846575Z",
     "iopub.status.idle": "2025-12-08T17:36:19.790127Z",
     "shell.execute_reply": "2025-12-08T17:36:19.789385Z",
     "shell.execute_reply.started": "2025-12-08T17:36:18.846959Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìÇ DATA PATHS\n",
      "======================================================================\n",
      "  ‚úì Visual features: /kaggle/input/visual-feature\n",
      "  ‚úì Split files: /kaggle/input/casual-vid-data-split/split\n",
      "  ‚úì Text annotations: /kaggle/input/text-annotation\n",
      "\n",
      "======================================================================\n",
      "üìä DATASET STATISTICS\n",
      "======================================================================\n",
      "\n",
      "üìÅ Split Files:\n",
      "   train:  18776 videos ‚Üí 112656 samples\n",
      "   valid:   2695 videos ‚Üí  16170 samples\n",
      "    test:   5429 videos ‚Üí  32574 samples\n",
      "\n",
      "üé¨ Visual Features:\n",
      "  Indexed videos: 26900\n",
      "  appearance_feat.h5: (26900, 8, 16, 2048)\n",
      "  motion_feat.h5: (26900, 8, 2048)\n",
      "\n",
      "‚ùì Question Types (qtype):\n",
      "  0: Descriptive          - What is happening?\n",
      "  1: Explanatory          - Why did it happen?\n",
      "  2: Predictive-Ans       - What will happen?\n",
      "  3: Predictive-Reason    - Why will it happen?\n",
      "  4: Counterfactual-Ans   - What if X didn't happen?\n",
      "  5: Counterfactual-Reason - Why would that result?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "# ============================================================\n",
    "# DATA PATHS (Kaggle Input)\n",
    "# ============================================================\n",
    "text_feature_path = '/kaggle/input/text-feature'\n",
    "visual_feature_path = '/kaggle/input/visual-feature'\n",
    "split_path = '/kaggle/input/casual-vid-data-split/split'\n",
    "text_annotation_path = '/kaggle/input/text-annotation'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìÇ DATA PATHS\")\n",
    "print(\"=\" * 70)\n",
    "for name, path in [(\"Visual features\", visual_feature_path), \n",
    "                   (\"Split files\", split_path), \n",
    "                   (\"Text annotations\", text_annotation_path)]:\n",
    "    status = \"‚úì\" if os.path.exists(path) else \"‚úó\"\n",
    "    print(f\"  {status} {name}: {path}\")\n",
    "\n",
    "# ============================================================\n",
    "# DATA STATISTICS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä DATASET STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Split files\n",
    "print(\"\\nüìÅ Split Files:\")\n",
    "split_stats = {}\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    split_file = f'{split_path}/{split_name}.pkl'\n",
    "    if os.path.exists(split_file):\n",
    "        with open(split_file, 'rb') as f:\n",
    "            vids = pickle.load(f)\n",
    "        split_stats[split_name] = len(vids)\n",
    "        samples = len(vids) * 6  # 6 question types per video\n",
    "        print(f\"  {split_name:>6}: {len(vids):>6} videos ‚Üí {samples:>6} samples\")\n",
    "\n",
    "# 2. Visual features\n",
    "print(\"\\nüé¨ Visual Features:\")\n",
    "idx2vid_file = f'{visual_feature_path}/idx2vid.pkl'\n",
    "if os.path.exists(idx2vid_file):\n",
    "    with open(idx2vid_file, 'rb') as f:\n",
    "        idx2vid = pickle.load(f)\n",
    "    print(f\"  Indexed videos: {len(idx2vid)}\")\n",
    "\n",
    "for feat_name in ['appearance_feat.h5', 'motion_feat.h5']:\n",
    "    feat_file = f'{visual_feature_path}/{feat_name}'\n",
    "    if os.path.exists(feat_file):\n",
    "        with h5py.File(feat_file, 'r') as f:\n",
    "            shape = f['resnet_features'].shape\n",
    "        print(f\"  {feat_name}: {shape}\")\n",
    "\n",
    "# 3. Question types\n",
    "print(\"\\n‚ùì Question Types (qtype):\")\n",
    "qtype_info = [\n",
    "    (\"0\", \"Descriptive\", \"What is happening?\"),\n",
    "    (\"1\", \"Explanatory\", \"Why did it happen?\"),\n",
    "    (\"2\", \"Predictive-Ans\", \"What will happen?\"),\n",
    "    (\"3\", \"Predictive-Reason\", \"Why will it happen?\"),\n",
    "    (\"4\", \"Counterfactual-Ans\", \"What if X didn't happen?\"),\n",
    "    (\"5\", \"Counterfactual-Reason\", \"Why would that result?\"),\n",
    "]\n",
    "for qt, name, desc in qtype_info:\n",
    "    print(f\"  {qt}: {name:<20} - {desc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:36:19.791122Z",
     "iopub.status.busy": "2025-12-08T17:36:19.790912Z",
     "iopub.status.idle": "2025-12-08T17:36:23.392750Z",
     "shell.execute_reply": "2025-12-08T17:36:23.391839Z",
     "shell.execute_reply.started": "2025-12-08T17:36:19.791104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -q transformers einops h5py wandb\n",
    "\n",
    "# Login to W&B (uncomment v√† th√™m API key c·ªßa b·∫°n)\n",
    "my_key = \"80b5a02ccaed80f35a2e893aed6446d4467c0c45\"\n",
    "import wandb\n",
    "wandb.login(key=my_key, relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:36:23.394481Z",
     "iopub.status.busy": "2025-12-08T17:36:23.394152Z",
     "iopub.status.idle": "2025-12-08T17:36:23.420458Z",
     "shell.execute_reply": "2025-12-08T17:36:23.419899Z",
     "shell.execute_reply.started": "2025-12-08T17:36:23.394443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üñ•Ô∏è GPU CONFIGURATION\n",
      "======================================================================\n",
      "  Available GPUs: 2\n",
      "    GPU 0: Tesla T4\n",
      "           Memory: 15.8 GB\n",
      "    GPU 1: Tesla T4\n",
      "           Memory: 15.8 GB\n",
      "\n",
      "  ‚úì Multi-GPU mode: DataParallel on 2 GPUs\n",
      "  ‚úì Effective batch size: 2 (total)\n",
      "  Primary device: cuda\n",
      "\n",
      "======================================================================\n",
      "‚öôÔ∏è TRAINING CONFIG\n",
      "======================================================================\n",
      "  Batch size          : 2\n",
      "  Learning rate       : 0.0001\n",
      "  Text encoder LR     : 1e-05\n",
      "  Epochs              : 20\n",
      "  Early stopping      : 5 epochs\n",
      "  d_model             : 768\n",
      "  TopK frames         : 8\n",
      "  TopK objects        : 5\n",
      "  Objects/frame       : 20\n",
      "  Text encoder        : microsoft/deberta-base\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Training configuration for CausalVidQA\"\"\"\n",
    "    \n",
    "    # Experiment\n",
    "    project_name = \"CausalVidQA-TranSTR\"\n",
    "    run_name = \"causalvid_2gpu\"\n",
    "    \n",
    "    # Data paths\n",
    "    sample_list_path = split_path\n",
    "    video_feature_path = visual_feature_path\n",
    "    text_annotation_path = text_annotation_path\n",
    "    \n",
    "    # Training\n",
    "    bs = 2                    # Batch size (s·∫Ω chia ƒë·ªÅu cho 2 GPU)\n",
    "    lr = 1e-4                  # Learning rate\n",
    "    text_encoder_lr = 1e-5     # Text encoder LR (lower)\n",
    "    epoch = 20\n",
    "    warmup_epochs = 2          # Warmup epochs\n",
    "    \n",
    "    # Dataset\n",
    "    dataset = 'causal-vid'\n",
    "    qtype = 4                 # -1 = all question types\n",
    "    max_samples = 500         # None = use all data\n",
    "    \n",
    "    # Model architecture\n",
    "    d_model = 768\n",
    "    word_dim = 768\n",
    "    nheads = 8\n",
    "    num_encoder_layers = 1\n",
    "    num_decoder_layers = 1\n",
    "    dropout = 0.1\n",
    "    encoder_dropout = 0.1\n",
    "    activation = 'relu'\n",
    "    normalize_before = False\n",
    "    \n",
    "    # Video features\n",
    "    objs = 20                  # Objects per frame\n",
    "    topK_frame = 8             # Top-K frames to select\n",
    "    topK_obj = 5               # Top-K objects to select\n",
    "    frame_feat_dim = 4096      # app(2048) + mot(2048)\n",
    "    obj_feat_dim = 2053        # feat(2048) + bbox(5)\n",
    "    n_query = 5                # 5-way multiple choice\n",
    "    \n",
    "    # Text encoder\n",
    "    text_encoder_type = \"microsoft/deberta-base\"\n",
    "    freeze_text_encoder = False\n",
    "    text_pool_mode = 0\n",
    "    hard_eval = False\n",
    "    \n",
    "    # Optimizer\n",
    "    decay = 0.001              # Weight decay\n",
    "    patience = 3               # LR scheduler patience\n",
    "    gamma = 0.5                # LR decay factor\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_patience = 5  # Stop after 5 epochs without improvement\n",
    "    \n",
    "    # Contrastive learning\n",
    "    pos_ratio = 0.7\n",
    "    neg_ratio = 0.3\n",
    "    a = 1\n",
    "    \n",
    "    # Multi-GPU\n",
    "    use_multi_gpu = True       # Enable DataParallel\n",
    "    num_workers = 2            # DataLoader workers\n",
    "    \n",
    "    # Logging\n",
    "    log_interval = 50          # Log every N batches\n",
    "    save_every = 5             # Save checkpoint every N epochs\n",
    "\n",
    "args = Config()\n",
    "\n",
    "# ============================================================\n",
    "# GPU SETUP\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üñ•Ô∏è GPU CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"  Available GPUs: {n_gpus}\")\n",
    "for i in range(n_gpus):\n",
    "    print(f\"    GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    mem = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "    print(f\"           Memory: {mem:.1f} GB\")\n",
    "\n",
    "if n_gpus >= 2 and args.use_multi_gpu:\n",
    "    print(f\"\\n  ‚úì Multi-GPU mode: DataParallel on {n_gpus} GPUs\")\n",
    "    print(f\"  ‚úì Effective batch size: {args.bs} (total)\")\n",
    "else:\n",
    "    print(f\"\\n  ‚Üí Single GPU mode\")\n",
    "    args.use_multi_gpu = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"  Primary device: {device}\")\n",
    "\n",
    "# ============================================================\n",
    "# PRINT CONFIG\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚öôÔ∏è TRAINING CONFIG\")\n",
    "print(\"=\" * 70)\n",
    "config_items = [\n",
    "    (\"Batch size\", args.bs),\n",
    "    (\"Learning rate\", args.lr),\n",
    "    (\"Text encoder LR\", args.text_encoder_lr),\n",
    "    (\"Epochs\", args.epoch),\n",
    "    (\"Early stopping\", f\"{args.early_stopping_patience} epochs\"),\n",
    "    (\"d_model\", args.d_model),\n",
    "    (\"TopK frames\", args.topK_frame),\n",
    "    (\"TopK objects\", args.topK_obj),\n",
    "    (\"Objects/frame\", args.objs),\n",
    "    (\"Text encoder\", args.text_encoder_type),\n",
    "]\n",
    "\n",
    "for name, val in config_items:\n",
    "    print(f\"  {name:<20}: {val}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:36:23.421496Z",
     "iopub.status.busy": "2025-12-08T17:36:23.421247Z",
     "iopub.status.idle": "2025-12-08T17:36:23.430037Z",
     "shell.execute_reply": "2025-12-08T17:36:23.429300Z",
     "shell.execute_reply.started": "2025-12-08T17:36:23.421470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modules imported, seed set to 999\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Local imports\n",
    "from DataLoader import VideoQADataset\n",
    "from networks.model import VideoQAmodel\n",
    "import eval_mc\n",
    "\n",
    "# ============================================================\n",
    "# REPRODUCIBILITY\n",
    "# ============================================================\n",
    "def set_seed(seed=999):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(999)\n",
    "print(\"‚úÖ Modules imported, seed set to 999\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:36:23.433275Z",
     "iopub.status.busy": "2025-12-08T17:36:23.433049Z",
     "iopub.status.idle": "2025-12-08T17:37:44.962862Z",
     "shell.execute_reply": "2025-12-08T17:37:44.962057Z",
     "shell.execute_reply.started": "2025-12-08T17:36:23.433259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "Limited to 500 videos (max_samples=500)\n",
      "Loading /kaggle/input/visual-feature/appearance_feat.h5...\n",
      "Loading /kaggle/input/visual-feature/motion_feat.h5...\n",
      "Total samples: 500\n",
      "Limited to 500 videos (max_samples=500)\n",
      "Loading /kaggle/input/visual-feature/appearance_feat.h5...\n",
      "Loading /kaggle/input/visual-feature/motion_feat.h5...\n",
      "Total samples: 500\n",
      "Loaded 5429 videos from /kaggle/input/casual-vid-data-split/split/test.pkl\n",
      "Loading /kaggle/input/visual-feature/appearance_feat.h5...\n",
      "Loading /kaggle/input/visual-feature/motion_feat.h5...\n",
      "Total samples: 5429\n",
      "\n",
      "======================================================================\n",
      "üìä DATALOADER SUMMARY\n",
      "======================================================================\n",
      "  Split          Videos    Samples    Batches\n",
      "  ---------- ---------- ---------- ----------\n",
      "  Train             500        500        250\n",
      "  Val               500        500        250\n",
      "  Test (FULL)       5429       5429       2715\n",
      "======================================================================\n",
      "  ‚ÑπÔ∏è  Test set always uses ALL data regardless of max_samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating datasets...\")\n",
    "\n",
    "# ============================================================\n",
    "# CREATE DATASETS\n",
    "# ============================================================\n",
    "dataset_kwargs = dict(\n",
    "    n_query=args.n_query,\n",
    "    obj_num=args.objs,\n",
    "    sample_list_path=args.sample_list_path,\n",
    "    video_feature_path=args.video_feature_path,\n",
    "    text_annotation_path=args.text_annotation_path,\n",
    "    qtype=args.qtype,\n",
    "    max_samples=args.max_samples\n",
    ")\n",
    "\n",
    "train_dataset = VideoQADataset(split='train', **dataset_kwargs)\n",
    "val_dataset = VideoQADataset(split='val', **dataset_kwargs)\n",
    "\n",
    "# Test set LU√îN d√πng to√†n b·ªô data (kh√¥ng gi·ªõi h·∫°n max_samples)\n",
    "test_kwargs = dataset_kwargs.copy()\n",
    "test_kwargs['max_samples'] = None  # Force full test set\n",
    "test_dataset = VideoQADataset(split='test', **test_kwargs)\n",
    "\n",
    "# ============================================================\n",
    "# CREATE DATALOADERS (optimized for multi-GPU)\n",
    "# ============================================================\n",
    "loader_kwargs = dict(\n",
    "    batch_size=args.bs,\n",
    "    num_workers=args.num_workers if args.use_multi_gpu else 0,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2 if args.num_workers > 0 else None,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True,drop_last=True, **loader_kwargs)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, **loader_kwargs)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, **loader_kwargs)\n",
    "\n",
    "# ============================================================\n",
    "# DATASET SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä DATALOADER SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  {'Split':<10} {'Videos':>10} {'Samples':>10} {'Batches':>10}\")\n",
    "print(f\"  {'-'*10} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "for name, dataset, loader in [\n",
    "    (\"Train\", train_dataset, train_loader),\n",
    "    (\"Val\", val_dataset, val_loader),\n",
    "    (\"Test (FULL)\", test_dataset, test_loader)\n",
    "]:\n",
    "    n_vids = len(dataset.vids) if hasattr(dataset, 'vids') else \"?\"\n",
    "    print(f\"  {name:<10} {n_vids:>10} {len(dataset):>10} {len(loader):>10}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  ‚ÑπÔ∏è  Test set always uses ALL data regardless of max_samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:37:44.964334Z",
     "iopub.status.busy": "2025-12-08T17:37:44.963699Z",
     "iopub.status.idle": "2025-12-08T17:37:45.760321Z",
     "shell.execute_reply": "2025-12-08T17:37:45.759151Z",
     "shell.execute_reply.started": "2025-12-08T17:37:44.964304Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying data sample...\n",
      "\n",
      "  Frame features:  torch.Size([2, 8, 4096])\n",
      "  Object features: torch.Size([2, 8, 20, 2053])\n",
      "  Batch size:      2\n",
      "\n",
      "  Sample question: What will happen if [person_1] cries?...\n",
      "  Sample answer:   [CLS] What will happen if [person_1] cries? [SEP] [person_3]...\n",
      "  Ground truth:    0\n",
      "  Question key:    tEdsMfaFCQM_000134_000144_4\n",
      "\n",
      "‚úÖ Data verification complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# VERIFY DATA SAMPLE\n",
    "# ============================================================\n",
    "print(\"üîç Verifying data sample...\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    vid_frame_feat, vid_obj_feat, qns_word, ans_word, ans_id, qns_key = batch\n",
    "    \n",
    "    print(f\"\\n  Frame features:  {vid_frame_feat.shape}\")\n",
    "    print(f\"  Object features: {vid_obj_feat.shape}\")\n",
    "    print(f\"  Batch size:      {len(qns_word)}\")\n",
    "    print(f\"\\n  Sample question: {qns_word[0][:80]}...\")\n",
    "    print(f\"  Sample answer:   {ans_word[0][0][:60]}...\")\n",
    "    print(f\"  Ground truth:    {ans_id[0].item()}\")\n",
    "    print(f\"  Question key:    {qns_key[0]}\")\n",
    "    break\n",
    "\n",
    "print(\"\\n‚úÖ Data verification complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:37:45.761792Z",
     "iopub.status.busy": "2025-12-08T17:37:45.761464Z",
     "iopub.status.idle": "2025-12-08T17:37:46.982904Z",
     "shell.execute_reply": "2025-12-08T17:37:46.982309Z",
     "shell.execute_reply.started": "2025-12-08T17:37:45.761752Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating model...\n",
      "  ‚Üí Wrapping model with DataParallel (2 GPUs)\n",
      "\n",
      "======================================================================\n",
      "üß† MODEL SUMMARY\n",
      "======================================================================\n",
      "  Total parameters:     180.36M\n",
      "  Trainable parameters: 180.36M\n",
      "  Multi-GPU:            True\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CREATE MODEL\n",
    "# ============================================================\n",
    "print(\"üèóÔ∏è Creating model...\")\n",
    "\n",
    "model_config = {\n",
    "    'd_model': args.d_model,\n",
    "    'word_dim': args.word_dim,\n",
    "    'encoder_dropout': args.encoder_dropout,\n",
    "    'dropout': args.dropout,\n",
    "    'num_encoder_layers': args.num_encoder_layers,\n",
    "    'num_decoder_layers': args.num_decoder_layers,\n",
    "    'nheads': args.nheads,\n",
    "    'normalize_before': args.normalize_before,\n",
    "    'activation': args.activation,\n",
    "    'text_encoder_type': args.text_encoder_type,\n",
    "    'freeze_text_encoder': args.freeze_text_encoder,\n",
    "    'text_pool_mode': args.text_pool_mode,\n",
    "    'n_query': args.n_query,\n",
    "    'objs': args.objs,\n",
    "    'topK_frame': args.topK_frame,\n",
    "    'topK_obj': args.topK_obj,\n",
    "    'hard_eval': args.hard_eval,\n",
    "    'frame_feat_dim': args.frame_feat_dim,\n",
    "    'obj_feat_dim': args.obj_feat_dim,\n",
    "    'device': device,\n",
    "}\n",
    "\n",
    "model = VideoQAmodel(**model_config)\n",
    "\n",
    "# ============================================================\n",
    "# MULTI-GPU SETUP (DataParallel)\n",
    "# ============================================================\n",
    "if args.use_multi_gpu and torch.cuda.device_count() > 1:\n",
    "    print(f\"  ‚Üí Wrapping model with DataParallel ({torch.cuda.device_count()} GPUs)\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# ============================================================\n",
    "# MODEL SUMMARY\n",
    "# ============================================================\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üß† MODEL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Total parameters:     {total_params / 1e6:.2f}M\")\n",
    "print(f\"  Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
    "print(f\"  Multi-GPU:            {args.use_multi_gpu and torch.cuda.device_count() > 1}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:37:46.983943Z",
     "iopub.status.busy": "2025-12-08T17:37:46.983719Z",
     "iopub.status.idle": "2025-12-08T17:37:47.003269Z",
     "shell.execute_reply": "2025-12-08T17:37:47.002513Z",
     "shell.execute_reply.started": "2025-12-08T17:37:46.983926Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader, criterion, device, epoch, wandb_run=None):\n",
    "    \"\"\"Train for one epoch with detailed logging\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    answers = []\n",
    "    batch_times = []\n",
    "    \n",
    "    # Per question type tracking\n",
    "    qtype_correct = defaultdict(int)\n",
    "    qtype_total = defaultdict(int)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, inputs in enumerate(train_loader):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "        vid_frame_feat = vid_frame_feat.to(device)\n",
    "        vid_obj_feat = vid_obj_feat.to(device)\n",
    "        ans_targets = ans_id.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "        loss = criterion(out, ans_targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        pred = out.max(-1)[1].cpu()\n",
    "        predictions.append(pred)\n",
    "        answers.append(ans_id)\n",
    "        \n",
    "        # Track per question type accuracy\n",
    "        for qkey, p, a in zip(qns_keys, pred.numpy(), ans_id.numpy()):\n",
    "            qtype = int(qkey.split('_')[-1])\n",
    "            qtype_total[qtype] += 1\n",
    "            if p == a:\n",
    "                qtype_correct[qtype] += 1\n",
    "        \n",
    "        batch_times.append(time.time() - batch_start)\n",
    "        \n",
    "        # Logging\n",
    "        if (batch_idx + 1) % args.log_interval == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            avg_time = np.mean(batch_times[-args.log_interval:])\n",
    "            print(f\"    Batch [{batch_idx+1:>4}/{len(train_loader)}] \"\n",
    "                  f\"Loss: {loss.item():.4f} (avg: {avg_loss:.4f}) \"\n",
    "                  f\"Time: {avg_time:.3f}s/batch\")\n",
    "            \n",
    "            if wandb_run:\n",
    "                wandb_run.log({\n",
    "                    \"train/batch_loss\": loss.item(),\n",
    "                    \"train/avg_loss\": avg_loss,\n",
    "                    \"train/batch_time\": avg_time,\n",
    "                }, step=epoch * len(train_loader) + batch_idx)\n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    all_preds = torch.cat(predictions, dim=0).long()\n",
    "    all_ans = torch.cat(answers, dim=0).long()\n",
    "    epoch_acc = (all_preds == all_ans).sum().item() * 100.0 / len(all_ans)\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Per question type accuracy\n",
    "    qtype_acc = {}\n",
    "    qtype_names = ['Des', 'Exp', 'Pred-A', 'Pred-R', 'CF-A', 'CF-R']\n",
    "    for qt in range(6):\n",
    "        if qtype_total[qt] > 0:\n",
    "            qtype_acc[qtype_names[qt]] = qtype_correct[qt] * 100.0 / qtype_total[qt]\n",
    "    \n",
    "    return {\n",
    "        'loss': epoch_loss,\n",
    "        'acc': epoch_acc,\n",
    "        'time': epoch_time,\n",
    "        'qtype_acc': qtype_acc\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device, split_name='val'):\n",
    "    \"\"\"Evaluate with detailed per-type accuracy\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    answers = []\n",
    "    qtype_correct = defaultdict(int)\n",
    "    qtype_total = defaultdict(int)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "            vid_frame_feat = vid_frame_feat.to(device)\n",
    "            vid_obj_feat = vid_obj_feat.to(device)\n",
    "            \n",
    "            out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "            pred = out.max(-1)[1].cpu()\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            answers.append(ans_id)\n",
    "            \n",
    "            for qkey, p, a in zip(qns_keys, pred.numpy(), ans_id.numpy()):\n",
    "                qtype = int(qkey.split('_')[-1])\n",
    "                qtype_total[qtype] += 1\n",
    "                if p == a:\n",
    "                    qtype_correct[qtype] += 1\n",
    "    \n",
    "    all_preds = torch.cat(predictions, dim=0).long()\n",
    "    all_ans = torch.cat(answers, dim=0).long()\n",
    "    overall_acc = (all_preds == all_ans).sum().item() * 100.0 / len(all_ans)\n",
    "    \n",
    "    # Per question type accuracy\n",
    "    qtype_names = ['Des', 'Exp', 'Pred-A', 'Pred-R', 'CF-A', 'CF-R']\n",
    "    qtype_acc = {}\n",
    "    for qt in range(6):\n",
    "        if qtype_total[qt] > 0:\n",
    "            qtype_acc[qtype_names[qt]] = qtype_correct[qt] * 100.0 / qtype_total[qt]\n",
    "    \n",
    "    # Combined metrics (Pred = both Pred-A and Pred-R correct for same video)\n",
    "    # This is computed at video level, need results dict for that\n",
    "    \n",
    "    return {\n",
    "        'acc': overall_acc,\n",
    "        'qtype_acc': qtype_acc,\n",
    "        'n_samples': len(all_ans)\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_and_save(model, data_loader, device, save_path):\n",
    "    \"\"\"Generate predictions and save to JSON\"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "            vid_frame_feat = vid_frame_feat.to(device)\n",
    "            vid_obj_feat = vid_obj_feat.to(device)\n",
    "            \n",
    "            out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "            pred = out.max(-1)[1].cpu()\n",
    "            \n",
    "            for qid, p, a in zip(qns_keys, pred.numpy(), ans_id.numpy()):\n",
    "                results[qid] = {'prediction': int(p), 'answer': int(a)}\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    correct = sum(1 for v in results.values() if v['prediction'] == v['answer'])\n",
    "    acc = correct * 100.0 / len(results)\n",
    "    \n",
    "    return results, acc\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:37:47.004396Z",
     "iopub.status.busy": "2025-12-08T17:37:47.004135Z",
     "iopub.status.idle": "2025-12-08T17:37:47.022634Z",
     "shell.execute_reply": "2025-12-08T17:37:47.021923Z",
     "shell.execute_reply.started": "2025-12-08T17:37:47.004374Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optimizer and scheduler created\n",
      "   Main LR: 0.0001\n",
      "   Text encoder LR: 1e-05\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SETUP OPTIMIZER, SCHEDULER, CRITERION\n",
    "# ============================================================\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "os.makedirs('./prediction', exist_ok=True)\n",
    "\n",
    "# Get base model for parameter groups (handle DataParallel)\n",
    "base_model = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "# Optimizer with different LR for text encoder\n",
    "param_groups = [\n",
    "    {\n",
    "        \"params\": [p for n, p in base_model.named_parameters() \n",
    "                   if \"text_encoder\" not in n and p.requires_grad],\n",
    "        \"lr\": args.lr\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in base_model.named_parameters() \n",
    "                   if \"text_encoder\" in n and p.requires_grad],\n",
    "        \"lr\": args.text_encoder_lr\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, weight_decay=args.decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=args.gamma, \n",
    "                               patience=args.patience, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"‚úÖ Optimizer and scheduler created\")\n",
    "print(f\"   Main LR: {args.lr}\")\n",
    "print(f\"   Text encoder LR: {args.text_encoder_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:37:47.024338Z",
     "iopub.status.busy": "2025-12-08T17:37:47.023398Z",
     "iopub.status.idle": "2025-12-08T17:37:54.346935Z",
     "shell.execute_reply": "2025-12-08T17:37:54.346178Z",
     "shell.execute_reply.started": "2025-12-08T17:37:47.024321Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>data/test_videos</td><td>‚ñÅ</td></tr><tr><td>data/train_videos</td><td>‚ñÅ</td></tr><tr><td>data/val_videos</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>data/test_videos</td><td>5429</td></tr><tr><td>data/train_videos</td><td>500</td></tr><tr><td>data/val_videos</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">causalvid_2gpu</strong> at: <a href='https://wandb.ai/introSE/CausalVidQA-TranSTR/runs/me69ovt5' target=\"_blank\">https://wandb.ai/introSE/CausalVidQA-TranSTR/runs/me69ovt5</a><br> View project at: <a href='https://wandb.ai/introSE/CausalVidQA-TranSTR' target=\"_blank\">https://wandb.ai/introSE/CausalVidQA-TranSTR</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251208_173203-me69ovt5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/tranSTR_Casual/wandb/run-20251208_173747-jte18czj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/introSE/CausalVidQA-TranSTR/runs/jte18czj' target=\"_blank\">causalvid_2gpu</a></strong> to <a href='https://wandb.ai/introSE/CausalVidQA-TranSTR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/introSE/CausalVidQA-TranSTR' target=\"_blank\">https://wandb.ai/introSE/CausalVidQA-TranSTR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/introSE/CausalVidQA-TranSTR/runs/jte18czj' target=\"_blank\">https://wandb.ai/introSE/CausalVidQA-TranSTR/runs/jte18czj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ W&B initialized: https://wandb.ai/introSE/CausalVidQA-TranSTR/runs/jte18czj\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# INITIALIZE WANDB\n",
    "# ============================================================\n",
    "wandb_config = {\n",
    "    \"architecture\": \"TranSTR\",\n",
    "    \"dataset\": \"CausalVidQA\",\n",
    "    \"epochs\": args.epoch,\n",
    "    \"batch_size\": args.bs,\n",
    "    \"learning_rate\": args.lr,\n",
    "    \"text_encoder_lr\": args.text_encoder_lr,\n",
    "    \"text_encoder\": args.text_encoder_type,\n",
    "    \"d_model\": args.d_model,\n",
    "    \"topK_frame\": args.topK_frame,\n",
    "    \"topK_obj\": args.topK_obj,\n",
    "    \"n_objects\": args.objs,\n",
    "    \"num_encoder_layers\": args.num_encoder_layers,\n",
    "    \"num_decoder_layers\": args.num_decoder_layers,\n",
    "    \"multi_gpu\": args.use_multi_gpu,\n",
    "    \"n_gpus\": torch.cuda.device_count(),\n",
    "    \"train_samples\": len(train_dataset),\n",
    "    \"val_samples\": len(val_dataset),\n",
    "    \"test_samples\": len(test_dataset),\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=args.project_name,\n",
    "    name=args.run_name,\n",
    "    config=wandb_config,\n",
    "    tags=[\"causalvid\", \"multi-gpu\" if args.use_multi_gpu else \"single-gpu\"]\n",
    ")\n",
    "\n",
    "# Log dataset info\n",
    "wandb.log({\n",
    "    \"data/train_videos\": len(train_dataset.vids) if hasattr(train_dataset, 'vids') else 0,\n",
    "    \"data/val_videos\": len(val_dataset.vids) if hasattr(val_dataset, 'vids') else 0,\n",
    "    \"data/test_videos\": len(test_dataset.vids) if hasattr(test_dataset, 'vids') else 0,\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ W&B initialized: {run.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T17:37:54.348407Z",
     "iopub.status.busy": "2025-12-08T17:37:54.347826Z",
     "iopub.status.idle": "2025-12-08T17:37:55.268152Z",
     "shell.execute_reply": "2025-12-08T17:37:55.267043Z",
     "shell.execute_reply.started": "2025-12-08T17:37:54.348379Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ STARTING TRAINING: causalvid_2gpu\n",
      "   Epochs: 20 | Batch size: 2 | GPUs: 2\n",
      "   Early stopping: 5 epochs without improvement\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìö EPOCH [1/20]\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/model.py\", line 102, in forward\n    frame_local, frame_att = self.frame_decoder(frame_feat,\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/multimodal_transformer.py\", line 59, in forward\n    output, c_att = layer(output, memory, tgt_mask=tgt_mask,\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/multimodal_transformer.py\", line 158, in forward\n    tgt2, c_att = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/attention.py\", line 85, in forward\nRuntimeError: The expanded size of the tensor (36) must match the existing size (18) at non-singleton dimension 3.  Target sizes: [1, 8, 8, 36].  Tensor sizes: [1, 1, 1, 18]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38/1093866606.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# ============ TRAIN ============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# ============ EVALUATE ============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_38/3341790955.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, train_loader, criterion, device, epoch, wandb_run)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_frame_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_obj_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqns_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/model.py\", line 102, in forward\n    frame_local, frame_att = self.frame_decoder(frame_feat,\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/multimodal_transformer.py\", line 59, in forward\n    output, c_att = layer(output, memory, tgt_mask=tgt_mask,\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/multimodal_transformer.py\", line 158, in forward\n    tgt2, c_att = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/tranSTR_Casual/networks/attention.py\", line 85, in forward\nRuntimeError: The expanded size of the tensor (36) must match the existing size (18) at non-singleton dimension 3.  Target sizes: [1, 8, 8, 36].  Tensor sizes: [1, 1, 1, 18]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP WITH EARLY STOPPING\n",
    "# ============================================================\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = f'./models/best_model-{args.run_name}.ckpt'\n",
    "history = {'train': [], 'val': [], 'test': []}\n",
    "epochs_without_improvement = 0  # Early stopping counter\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"üöÄ STARTING TRAINING: {args.run_name}\")\n",
    "print(f\"   Epochs: {args.epoch} | Batch size: {args.bs} | GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"   Early stopping: {args.early_stopping_patience} epochs without improvement\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(1, args.epoch + 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìö EPOCH [{epoch}/{args.epoch}]\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ============ TRAIN ============\n",
    "    train_metrics = train_epoch(model, optimizer, train_loader, criterion, device, epoch, run)\n",
    "    \n",
    "    # ============ EVALUATE ============\n",
    "    val_metrics = evaluate(model, val_loader, device, 'val')\n",
    "    test_metrics = evaluate(model, test_loader, device, 'test')\n",
    "    \n",
    "    # ============ UPDATE SCHEDULER ============\n",
    "    scheduler.step(val_metrics['acc'])\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # ============ SAVE BEST MODEL & EARLY STOPPING ============\n",
    "    is_best = val_metrics['acc'] > best_val_acc\n",
    "    if is_best:\n",
    "        best_val_acc = val_metrics['acc']\n",
    "        best_epoch = epoch\n",
    "        epochs_without_improvement = 0  # Reset counter\n",
    "        # Save model (handle DataParallel)\n",
    "        state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
    "        torch.save(state_dict, best_model_path)\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # ============ LOGGING ============\n",
    "    print(f\"\\n  üìä Results:\")\n",
    "    print(f\"     {'Metric':<15} {'Train':>10} {'Val':>10} {'Test':>10}\")\n",
    "    print(f\"     {'-'*15} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "    print(f\"     {'Loss':<15} {train_metrics['loss']:>10.4f} {'-':>10} {'-':>10}\")\n",
    "    print(f\"     {'Accuracy':<15} {train_metrics['acc']:>9.2f}% {val_metrics['acc']:>9.2f}% {test_metrics['acc']:>9.2f}%\")\n",
    "    \n",
    "    # Per question type accuracy\n",
    "    print(f\"\\n  üìà Per Question Type Accuracy (Val):\")\n",
    "    qtype_order = ['Des', 'Exp', 'Pred-A', 'Pred-R', 'CF-A', 'CF-R']\n",
    "    for qt in qtype_order:\n",
    "        if qt in val_metrics['qtype_acc']:\n",
    "            print(f\"     {qt:<10}: {val_metrics['qtype_acc'][qt]:>6.2f}%\")\n",
    "    \n",
    "    print(f\"\\n  ‚è±Ô∏è  Time: {train_metrics['time']:.1f}s | LR: {current_lr:.2e}\")\n",
    "    print(f\"  üìâ No improvement: {epochs_without_improvement}/{args.early_stopping_patience} epochs\")\n",
    "    if is_best:\n",
    "        print(f\"  üíæ Saved best model! (Val acc: {best_val_acc:.2f}%)\")\n",
    "    \n",
    "    # ============ WANDB LOGGING ============\n",
    "    wandb_log = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train/loss\": train_metrics['loss'],\n",
    "        \"train/acc\": train_metrics['acc'],\n",
    "        \"val/acc\": val_metrics['acc'],\n",
    "        \"test/acc\": test_metrics['acc'],\n",
    "        \"lr\": current_lr,\n",
    "        \"epoch_time\": train_metrics['time'],\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"epochs_without_improvement\": epochs_without_improvement,\n",
    "    }\n",
    "    \n",
    "    # Log per question type accuracy\n",
    "    for qt, acc in train_metrics['qtype_acc'].items():\n",
    "        wandb_log[f\"train/acc_{qt}\"] = acc\n",
    "    for qt, acc in val_metrics['qtype_acc'].items():\n",
    "        wandb_log[f\"val/acc_{qt}\"] = acc\n",
    "    for qt, acc in test_metrics['qtype_acc'].items():\n",
    "        wandb_log[f\"test/acc_{qt}\"] = acc\n",
    "    \n",
    "    wandb.log(wandb_log)\n",
    "    \n",
    "    # Save checkpoint every N epochs\n",
    "    if epoch % args.save_every == 0:\n",
    "        ckpt_path = f'./models/checkpoint-{args.run_name}-ep{epoch}.ckpt'\n",
    "        state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': state_dict,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_metrics['acc'],\n",
    "        }, ckpt_path)\n",
    "        print(f\"  üìÅ Checkpoint saved: {ckpt_path}\")\n",
    "    \n",
    "    # ============ EARLY STOPPING CHECK ============\n",
    "    if epochs_without_improvement >= args.early_stopping_patience:\n",
    "        print(f\"\\n  ‚ö†Ô∏è EARLY STOPPING: No improvement for {args.early_stopping_patience} epochs\")\n",
    "        print(f\"     Best val acc: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "        wandb.log({\"early_stopped\": True, \"stopped_at_epoch\": epoch})\n",
    "        break\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING COMPLETE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Best epoch: {best_epoch}\")\n",
    "print(f\"   Best val accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   Model saved: {best_model_path}\")\n",
    "if epochs_without_improvement >= args.early_stopping_patience:\n",
    "    print(f\"   Stopped early at epoch {epoch}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-08T17:37:55.268954Z",
     "iopub.status.idle": "2025-12-08T17:37:55.269289Z",
     "shell.execute_reply": "2025-12-08T17:37:55.269135Z",
     "shell.execute_reply.started": "2025-12-08T17:37:55.269123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL EVALUATION WITH BEST MODEL\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä FINAL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load best model\n",
    "print(\"\\n  Loading best model...\")\n",
    "base_model = model.module if hasattr(model, 'module') else model\n",
    "base_model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Predict on test set\n",
    "result_path = f'./prediction/{args.run_name}-ep{best_epoch}-val{best_val_acc:.2f}.json'\n",
    "results, test_acc = predict_and_save(model, test_loader, device, result_path)\n",
    "\n",
    "print(f\"\\n  Test accuracy: {test_acc:.2f}%\")\n",
    "print(f\"  Results saved: {result_path}\")\n",
    "\n",
    "# Detailed evaluation by question type\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"  Detailed Results by Question Type:\")\n",
    "print(\"-\" * 70)\n",
    "eval_mc.accuracy_metric_cvid(result_path)\n",
    "\n",
    "# Log final results to wandb\n",
    "wandb.log({\n",
    "    \"final/test_acc\": test_acc,\n",
    "    \"final/best_epoch\": best_epoch,\n",
    "    \"final/best_val_acc\": best_val_acc,\n",
    "})\n",
    "\n",
    "# Save results artifact\n",
    "artifact = wandb.Artifact(f'predictions-{args.run_name}', type='predictions')\n",
    "artifact.add_file(result_path)\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-08T17:37:55.270102Z",
     "iopub.status.idle": "2025-12-08T17:37:55.270336Z",
     "shell.execute_reply": "2025-12-08T17:37:55.270237Z",
     "shell.execute_reply.started": "2025-12-08T17:37:55.270228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE TO KAGGLE OUTPUT & FINISH WANDB\n",
    "# ============================================================\n",
    "import shutil\n",
    "\n",
    "output_dir = '/kaggle/working'\n",
    "if os.path.exists(output_dir):\n",
    "    # Copy best model\n",
    "    shutil.copy(best_model_path, os.path.join(output_dir, f'best_model-{args.run_name}.ckpt'))\n",
    "    # Copy predictions\n",
    "    shutil.copy(result_path, output_dir)\n",
    "    print(f\"‚úÖ Files saved to {output_dir}\")\n",
    "else:\n",
    "    print(\"  Not running on Kaggle, files saved locally\")\n",
    "\n",
    "# Save model artifact to wandb\n",
    "model_artifact = wandb.Artifact(f'model-{args.run_name}', type='model')\n",
    "model_artifact.add_file(best_model_path)\n",
    "wandb.log_artifact(model_artifact)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "print(\"‚úÖ W&B run finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-08T17:37:55.271486Z",
     "iopub.status.idle": "2025-12-08T17:37:55.271746Z",
     "shell.execute_reply": "2025-12-08T17:37:55.271645Z",
     "shell.execute_reply.started": "2025-12-08T17:37:55.271635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Predict on test set\n",
    "result_path = f'./prediction/{args.v}-{best_epoch}-{best_val_acc:.2f}.json'\n",
    "results, test_acc = predict_and_save(model, test_loader, device, result_path)\n",
    "\n",
    "print(f\"\\nüìä Final Test Results:\")\n",
    "print(f\"   Overall Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"   Results saved to: {result_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-08T17:37:55.272982Z",
     "iopub.status.idle": "2025-12-08T17:37:55.273267Z",
     "shell.execute_reply": "2025-12-08T17:37:55.273154Z",
     "shell.execute_reply.started": "2025-12-08T17:37:55.273142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Quick test v·ªõi 10 videos\n",
    "# args.max_samples = 10\n",
    "# args.bs = 4\n",
    "# args.epoch = 2\n",
    "# \n",
    "# # Re-create datasets v·ªõi max_samples\n",
    "# train_dataset = VideoQADataset(\n",
    "#     split='train', n_query=args.n_query, obj_num=args.objs,\n",
    "#     sample_list_path=args.sample_list_path,\n",
    "#     video_feature_path=args.video_feature_path,\n",
    "#     text_annotation_path=args.text_annotation_path,\n",
    "#     qtype=args.qtype, max_samples=args.max_samples\n",
    "# )\n",
    "# print(f\"Quick test dataset: {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating pretrained model B2A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8208331,
     "sourceId": 12969233,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8210299,
     "sourceId": 12972016,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8210716,
     "sourceId": 12972597,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8211749,
     "sourceId": 12977127,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8383289,
     "isSourceIdPinned": true,
     "sourceId": 13226170,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8208446,
     "sourceId": 13229803,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8208348,
     "sourceId": 13919566,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8888719,
     "sourceId": 13946226,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
