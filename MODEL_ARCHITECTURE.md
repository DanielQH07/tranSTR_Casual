# Model Architecture - TranSTR vá»›i TokenMark (SoM)

## ğŸ“‹ Tá»•ng quan

Code Ä‘ang tuning **TranSTR (Transformer-based VideoQA)** vá»›i **TokenMark (Set-of-Mark) injection** cho CausalVidQA dataset.

---

## ğŸ—ï¸ Model Architecture

### **Base Model: VideoQAmodel (TranSTR)**

```
VideoQAmodel
â”œâ”€â”€ Text Encoder: DeBERTa-base
â”œâ”€â”€ Visual Encoders: ViT features (1024 dim)
â”œâ”€â”€ Transformer Decoders (hierarchical)
â”œâ”€â”€ Answer Classifier
â””â”€â”€ TokenMark Injector (optional)
```

---

## ğŸ”§ Components Chi Tiáº¿t

### 1. **Text Encoder**
- **Model**: `microsoft/deberta-base`
- **Output dim**: 768
- **Projection**: 768 â†’ 768 (d_model)
- **Freeze**: `False` (trainable)
- **Pool mode**: 1 (mean pooling)

### 2. **Visual Features**
- **Frame features**: ViT features
  - Input: `[B, 16, 1024]` (16 frames, 1024 dim)
  - Resize: `1024 â†’ 768` (d_model)
  - TopK selection: `16 â†’ 5` frames
- **Object features**: Object detection features
  - Input: `[B, 16, 20, 2053]` (16 frames, 20 objects, 2053 dim)
  - Resize: `2053 â†’ 768` (d_model)
  - TopK selection: `20 â†’ 12` objects per frame

### 3. **Transformer Architecture**

#### **Hierarchical Decoders:**
```
1. Frame Decoder
   - Input: [B, 16, 768] frame features
   - Query: [B, seq_len, 768] question features
   - Output: [B, 16, 768] + attention weights
   - TopK: Select top 5 frames â†’ [B, 5, 768]

2. Object Decoder
   - Input: [B*5, 20, 768] object features (flattened)
   - Query: [B*5, seq_len, 768] question (repeated)
   - Output: [B*5, 20, 768] + attention weights
   - TopK: Select top 12 objects â†’ [B, 5, 12, 768]

3. Frame-Object Decoder (fo_decoder)
   - Input: [B, 5, 768] frame + [B, 5, 12, 768] objects
   - Output: [B, 5, 768] hierarchical features

4. VL Encoder (Vision-Language Fusion)
   - Input: [B, 5+seq_len, 768] (frames + question)
   - Output: [B, 5+seq_len, 768] fused memory

5. Answer Decoder
   - Input: [B, 5, 768] answer queries
   - Memory: [B, 5+seq_len, 768] from VL encoder
   - Output: [B, 5, 768] answer features
```

#### **Transformer Config:**
- **d_model**: 768
- **nheads**: 8
- **num_encoder_layers**: 2
- **num_decoder_layers**: 2
- **activation**: gelu
- **normalize_before**: True
- **dropout**: 0.3
- **encoder_dropout**: 0.3

### 4. **TopK Selection**
- **Frame TopK**: 5 frames (from 16)
  - Method: `PerturbedTopK` (differentiable)
  - Hard eval: `HardtopK` (non-differentiable)
- **Object TopK**: 12 objects (from 20)
  - Method: `PerturbedTopK` (differentiable)
  - Hard eval: `HardtopK` (non-differentiable)

### 5. **TokenMark (SoM) Injector** (Optional)
- **Enabled**: `use_som = True` (if SoM data available)
- **num_marks**: 16
- **Injection points**:
  - After frame resize & topK selection
  - After object resize & topK selection
- **Parameters**:
  - `gamma_frame`: 0.1 (learnable)
  - `gamma_obj`: 0.1 (learnable)
  - `palette`: 16 Ã— 768 learnable embeddings

### 6. **Answer Classifier**
- **Input**: [B, 5, 768] answer features
- **Output**: [B, 5] logits (5 answer choices)
- **Layer**: `Linear(768, 1)` â†’ squeeze

---

## ğŸ“Š Model Hyperparameters

### **Architecture:**
```python
d_model = 768
nheads = 8
num_encoder_layers = 2
num_decoder_layers = 2
activation = 'gelu'
normalize_before = True
dropout = 0.3
encoder_dropout = 0.3
```

### **Feature Dimensions:**
```python
frame_feat_dim = 1024  # ViT features
obj_feat_dim = 2053    # Object detection (2048 + 5 bbox)
word_dim = 768         # DeBERTa output
```

### **Selection:**
```python
topK_frame = 5   # Select 5 frames from 16
topK_obj = 12    # Select 12 objects from 20
frames = 16      # Total frames loaded
objs = 20        # Max objects per frame
```

### **Training:**
```python
batch_size = 8
learning_rate = 1e-5
weight_decay = 1e-4
epochs = 20
patience = 5
gamma = 0.1      # LR scheduler factor
```

---

## ğŸ”„ Forward Pass Flow

```
1. Input Processing
   â”œâ”€â”€ Frame: [B, 16, 1024] â†’ resize â†’ [B, 16, 768]
   â”œâ”€â”€ Object: [B, 16, 20, 2053] â†’ (keep for now)
   â””â”€â”€ Question: text â†’ DeBERTa â†’ [B, seq_len, 768]

2. Frame Decoder + TopK
   â”œâ”€â”€ frame_local: [B, 16, 768]
   â”œâ”€â”€ frame_att: attention weights
   â””â”€â”€ TopK selection â†’ [B, 5, 768]

3. Object Processing
   â”œâ”€â”€ Select objects for top 5 frames â†’ [B, 5, 20, 2053]
   â””â”€â”€ Resize â†’ [B, 5, 20, 768]

4. âš ï¸ SoM Injection (if enabled)
   â”œâ”€â”€ Inject into frame_local: [B, 5, 768]
   â””â”€â”€ Inject into obj_local: [B, 5, 20, 768]

5. Object Decoder + TopK
   â”œâ”€â”€ obj_local: [B*5, 20, 768]
   â””â”€â”€ TopK selection â†’ [B, 5, 12, 768]

6. Hierarchy Grouping
   â”œâ”€â”€ fo_decoder: [B, 5, 768] + [B, 5, 12, 768]
   â””â”€â”€ Output: [B, 5, 768]

7. Vision-Language Fusion
   â”œâ”€â”€ Concatenate: [B, 5, 768] + [B, seq_len, 768]
   â”œâ”€â”€ VL encoder: [B, 5+seq_len, 768]
   â””â”€â”€ Memory: [B, 5+seq_len, 768]

8. Answer Decoding
   â”œâ”€â”€ Answer queries: [B, 5, 768]
   â”œâ”€â”€ Answer decoder: [B, 5, 768]
   â””â”€â”€ Classifier: [B, 5] logits
```

---

## ğŸ“ˆ Model Size

### **Parameters:**
- **Total**: ~110-120M parameters
- **Trainable**: ~110-120M (text encoder not frozen)
- **SoM injector**: ~16 Ã— 768 = 12K additional parameters

### **Breakdown:**
- DeBERTa-base: ~86M
- Transformer layers: ~20M
- Feature resizers: ~2M
- SoM injector: ~12K
- Classifier: ~4K

---

## ğŸ¯ Key Features

### **1. Hierarchical Attention**
- Frame-level attention â†’ Object-level attention
- Multi-scale feature fusion

### **2. Differentiable TopK**
- `PerturbedTopK` for training (soft selection)
- `HardtopK` for evaluation (hard selection)

### **3. TokenMark Injection**
- Learnable entity embeddings
- Spatial mask-based injection
- Frame and object feature enhancement

### **4. Multi-modal Fusion**
- Vision-Language encoder
- Cross-attention between video and text
- Answer-specific decoding

---

## ğŸ” Model Variants

### **Current Configuration:**
- **Text**: DeBERTa-base (trainable)
- **Visual**: ViT features (1024 dim)
- **SoM**: Enabled (if data available)
- **TopK**: 5 frames, 12 objects

### **Alternative Configurations:**
- **Text**: RoBERTa-base, BERT-base (configurable)
- **Visual**: ResNet + Motion (4096 dim) - not used in current setup
- **SoM**: Disabled (use_som=False)
- **TopK**: Configurable (topK_frame, topK_obj)

---

## ğŸ“ Notes

1. **Model name**: TranSTR (Transformer-based VideoQA)
2. **Dataset**: CausalVidQA
3. **Task**: Multiple-choice VideoQA (5 choices)
4. **Evaluation**: Per-question-type accuracy (Description, Explanation, PAR, CAR, Acc_ALL)
5. **Special feature**: TokenMark (SoM) for explicit entity grounding

---

## ğŸ”— References

- TranSTR paper: [link to paper]
- DeBERTa: https://huggingface.co/microsoft/deberta-base
- TokenMark (SoM): Set-of-Mark prompting for visual grounding
- CausalVidQA: Causal Video Question Answering dataset
