{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084035b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Extract Visual Features with ViT for CausalVidQA\n",
    "# ============================================================================\n",
    "# M·ª•c ti√™u:\n",
    "# - ƒê·ªçc video IDs t·ª´ file pkl (train.pkl, valid.pkl, test.pkl)\n",
    "# - Tr√≠ch xu·∫•t visual-feature b·∫±ng ViT `google/vit-large-patch16-224-in21k`\n",
    "# - L∆∞u feature d∆∞·ªõi d·∫°ng .h5 (gi·ªëng format hi·ªán t·∫°i c·ªßa visual-feature)\n",
    "# - Upload l√™n HuggingFace ƒë·ªÉ d√πng l·∫°i sau\n",
    "#\n",
    "# Output format (t∆∞∆°ng th√≠ch DataLoader.py):\n",
    "# - vit_visual_feat.h5: ch·ª©a \"resnet_features\" shape (N_videos, T, D)\n",
    "# - idx2vid.pkl: list video_id theo th·ª© t·ª± index\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# 0. C√†i ƒë·∫∑t th∆∞ vi·ªán\n",
    "# ============================================================================\n",
    "%pip install -q \"transformers>=4.40.0\" \"huggingface_hub>=0.24.0\" opencv-python tqdm h5py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dada6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# ============================================================================\n",
    "# 1. C·∫§U H√åNH - S·ª¨A L·∫†I CHO ƒê√öNG DATASET C·ª¶A B·∫†N\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # ==================== KAGGLE PATHS ====================\n",
    "    # ƒê∆∞·ªùng d·∫´n t·ªõi split files (train.pkl, valid.pkl, test.pkl)\n",
    "    SPLIT_PATH = \"/kaggle/input/casual-vid-data-split/split\"\n",
    "    \n",
    "    # ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c ch·ª©a video g·ªëc\n",
    "    VIDEO_ROOT = \"/kaggle/input/causalvid-videos\"  # TODO: s·ª≠a l·∫°i\n",
    "    \n",
    "    # ==================== OUTPUT ====================\n",
    "    # Th∆∞ m·ª•c l∆∞u feature output\n",
    "    OUTPUT_DIR = \"/kaggle/working/vit_visual_features\"\n",
    "    \n",
    "    # ==================== MODEL ====================\n",
    "    VIT_NAME = \"google/vit-large-patch16-224-in21k\"\n",
    "    \n",
    "    # S·ªë frame t·ªëi ƒëa l·∫•y m·ªói video (l·∫•y m·∫´u ƒë·ªÅu)\n",
    "    NUM_FRAMES = 16\n",
    "    \n",
    "    # Thi·∫øt b·ªã\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # ==================== HUGGINGFACE ====================\n",
    "    HF_TOKEN = \"\"          # ƒêi·ªÅn token ƒë·ªÉ upload\n",
    "    HF_REPO_ID = \"\"        # V√≠ d·ª•: \"your-username/causalvid-vit-features\"\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"SPLIT_PATH : {cfg.SPLIT_PATH}\")\n",
    "print(f\"VIDEO_ROOT : {cfg.VIDEO_ROOT}\")\n",
    "print(f\"OUTPUT_DIR : {cfg.OUTPUT_DIR}\")\n",
    "print(f\"VIT_NAME   : {cfg.VIT_NAME}\")\n",
    "print(f\"NUM_FRAMES : {cfg.NUM_FRAMES}\")\n",
    "print(f\"DEVICE     : {cfg.DEVICE}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e356de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. LOAD VIDEO IDs T·ª™ PKL FILES\n",
    "# ============================================================================\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Load t·∫•t c·∫£ video IDs t·ª´ train, valid, test\n",
    "all_vids = set()\n",
    "\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    split_file = os.path.join(cfg.SPLIT_PATH, f'{split_name}.pkl')\n",
    "    if os.path.exists(split_file):\n",
    "        vids = load_pkl(split_file)\n",
    "        print(f\"Loaded {len(vids)} videos from {split_name}.pkl\")\n",
    "        all_vids.update(vids)\n",
    "    else:\n",
    "        # Th·ª≠ val.pkl thay v√¨ valid.pkl\n",
    "        alt_file = os.path.join(cfg.SPLIT_PATH, 'val.pkl')\n",
    "        if split_name == 'valid' and os.path.exists(alt_file):\n",
    "            vids = load_pkl(alt_file)\n",
    "            print(f\"Loaded {len(vids)} videos from val.pkl\")\n",
    "            all_vids.update(vids)\n",
    "        else:\n",
    "            print(f\"[WARN] Not found: {split_file}\")\n",
    "\n",
    "all_vids = sorted(list(all_vids))\n",
    "print(f\"\\n‚úÖ Total unique videos: {len(all_vids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eda92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. LOAD M√î H√åNH ViT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading ViT model and processor ...\")\n",
    "processor = AutoImageProcessor.from_pretrained(cfg.VIT_NAME)\n",
    "model = AutoModel.from_pretrained(cfg.VIT_NAME)\n",
    "model.to(cfg.DEVICE)\n",
    "model.eval()\n",
    "\n",
    "HIDDEN_DIM = model.config.hidden_size  # 1024 cho vit-large\n",
    "\n",
    "print(f\"‚úÖ Loaded {cfg.VIT_NAME}\")\n",
    "print(f\"   Hidden size: {HIDDEN_DIM}\")\n",
    "print(f\"   Image size : {getattr(model.config, 'image_size', 224)}\")\n",
    "print(f\"   Patch size : {getattr(model.config, 'patch_size', 16)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. H√ÄM TI·ªÜN √çCH\n",
    "# ============================================================================\n",
    "\n",
    "def find_video_path(video_id, video_root):\n",
    "    \"\"\"T√¨m file video theo video_id trong th∆∞ m·ª•c video_root\"\"\"\n",
    "    video_root = Path(video_root)\n",
    "    \n",
    "    # Th·ª≠ c√°c extension ph·ªï bi·∫øn\n",
    "    for ext in ['.mp4', '.avi', '.mkv', '.mov', '.webm']:\n",
    "        # Th·ª≠ tr·ª±c ti·∫øp\n",
    "        path = video_root / f\"{video_id}{ext}\"\n",
    "        if path.exists():\n",
    "            return path\n",
    "        \n",
    "        # Th·ª≠ trong subfolder\n",
    "        for subpath in video_root.rglob(f\"{video_id}{ext}\"):\n",
    "            return subpath\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def load_video_frames(video_path, num_frames=16, target_size=(224, 224)):\n",
    "    \"\"\"ƒê·ªçc video v√† l·∫•y t·ªëi ƒëa `num_frames` frame ƒë∆∞·ª£c l·∫•y m·∫´u ƒë·ªÅu.\"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
    "    if total_frames == 0:\n",
    "        cap.release()\n",
    "        raise RuntimeError(f\"Video has 0 frames: {video_path}\")\n",
    "\n",
    "    # Ch·ªçn index frame theo sampling ƒë·ªÅu\n",
    "    num = min(num_frames, total_frames)\n",
    "    indices = np.linspace(0, total_frames - 1, num=num, dtype=int)\n",
    "\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "        ok, frame_bgr = cap.read()\n",
    "        if not ok:\n",
    "            continue\n",
    "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb = cv2.resize(frame_rgb, target_size, interpolation=cv2.INTER_AREA)\n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        raise RuntimeError(f\"No frames decoded from: {video_path}\")\n",
    "\n",
    "    return frames\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_vit_features(video_path):\n",
    "    \"\"\"Tr√≠ch xu·∫•t visual feature t·ª´ ViT [CLS] token.\n",
    "    \n",
    "    Returns:\n",
    "        frame_feat: (T, D) - visual feature cho m·ªói frame\n",
    "    \"\"\"\n",
    "    # L·∫•y target size t·ª´ processor\n",
    "    if isinstance(processor.size, dict) and \"shortest_edge\" in processor.size:\n",
    "        target_size = (processor.size[\"shortest_edge\"],) * 2\n",
    "    else:\n",
    "        target_size = (224, 224)\n",
    "    \n",
    "    frames = load_video_frames(video_path, num_frames=cfg.NUM_FRAMES, target_size=target_size)\n",
    "\n",
    "    # Chu·∫©n b·ªã batch tensor\n",
    "    inputs = processor(images=frames, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(cfg.DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    hidden = outputs.last_hidden_state  # (T, 1 + P, D)\n",
    "\n",
    "    # Token [CLS] l√† token ƒë·∫ßu ti√™n -> visual feature c·ªßa frame\n",
    "    cls_tok = hidden[:, 0]  # (T, D)\n",
    "    frame_feat = cls_tok.detach().cpu().numpy()\n",
    "\n",
    "    return frame_feat.astype(np.float32)\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410116c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. TR√çCH XU·∫§T FEATURES CHO T·∫§T C·∫¢ VIDEOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTRACTING ViT FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Chu·∫©n b·ªã l∆∞u tr·ªØ\n",
    "features_dict = {}  # video_id -> (T, D)\n",
    "idx2vid = []        # index -> video_id\n",
    "failed_vids = []    # videos kh√¥ng extract ƒë∆∞·ª£c\n",
    "\n",
    "for vid in tqdm(all_vids, desc=\"Extracting ViT features\"):\n",
    "    video_path = find_video_path(vid, cfg.VIDEO_ROOT)\n",
    "    \n",
    "    if video_path is None:\n",
    "        failed_vids.append((vid, \"Video file not found\"))\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        frame_feat = extract_vit_features(video_path)\n",
    "        features_dict[vid] = frame_feat\n",
    "        idx2vid.append(vid)\n",
    "    except Exception as e:\n",
    "        failed_vids.append((vid, str(e)))\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Extracted features for {len(features_dict)} videos\")\n",
    "if failed_vids:\n",
    "    print(f\"‚ùå Failed: {len(failed_vids)} videos\")\n",
    "    for vid, err in failed_vids[:5]:\n",
    "        print(f\"   - {vid}: {err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. L∆ØU FEATURES D∆Ø·ªöI D·∫†NG H5 (t∆∞∆°ng th√≠ch DataLoader.py)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(features_dict) == 0:\n",
    "    print(\"‚ùå No features to save!\")\n",
    "else:\n",
    "    # T√¨m s·ªë frame t·ªëi ƒëa ƒë·ªÉ pad\n",
    "    max_frames = max(feat.shape[0] for feat in features_dict.values())\n",
    "    feat_dim = HIDDEN_DIM\n",
    "    n_videos = len(idx2vid)\n",
    "    \n",
    "    print(f\"Number of videos: {n_videos}\")\n",
    "    print(f\"Max frames: {max_frames}\")\n",
    "    print(f\"Feature dim: {feat_dim}\")\n",
    "    \n",
    "    # T·∫°o array (N, T, D) v·ªõi padding\n",
    "    all_features = np.zeros((n_videos, max_frames, feat_dim), dtype=np.float32)\n",
    "    \n",
    "    for i, vid in enumerate(idx2vid):\n",
    "        feat = features_dict[vid]\n",
    "        T = feat.shape[0]\n",
    "        all_features[i, :T, :] = feat\n",
    "    \n",
    "    # L∆∞u h5 file\n",
    "    h5_path = os.path.join(cfg.OUTPUT_DIR, \"vit_visual_feat.h5\")\n",
    "    with h5py.File(h5_path, 'w') as f:\n",
    "        f.create_dataset('resnet_features', data=all_features, compression='gzip')\n",
    "    print(f\"‚úÖ Saved: {h5_path}\")\n",
    "    print(f\"   Shape: {all_features.shape}\")\n",
    "    \n",
    "    # L∆∞u idx2vid.pkl\n",
    "    idx2vid_path = os.path.join(cfg.OUTPUT_DIR, \"idx2vid.pkl\")\n",
    "    with open(idx2vid_path, 'wb') as f:\n",
    "        pickle.dump(idx2vid, f)\n",
    "    print(f\"‚úÖ Saved: {idx2vid_path}\")\n",
    "    print(f\"   Videos: {len(idx2vid)}\")\n",
    "    \n",
    "    # L∆∞u failed videos ƒë·ªÉ debug\n",
    "    if failed_vids:\n",
    "        failed_path = os.path.join(cfg.OUTPUT_DIR, \"failed_videos.txt\")\n",
    "        with open(failed_path, 'w') as f:\n",
    "            for vid, err in failed_vids:\n",
    "                f.write(f\"{vid}\\t{err}\\n\")\n",
    "        print(f\"‚ö†Ô∏è Saved failed list: {failed_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. UPLOAD L√äN HUGGINGFACE HUB\n",
    "# ============================================================================\n",
    "\n",
    "if cfg.HF_TOKEN and cfg.HF_REPO_ID:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"UPLOADING TO HUGGINGFACE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    login(token=cfg.HF_TOKEN)\n",
    "    api = HfApi()\n",
    "    \n",
    "    # T·∫°o repo n·∫øu ch∆∞a c√≥\n",
    "    api.create_repo(cfg.HF_REPO_ID, repo_type=\"dataset\", exist_ok=True)\n",
    "    \n",
    "    # Upload to√†n b·ªô th∆∞ m·ª•c output\n",
    "    api.upload_folder(\n",
    "        repo_id=cfg.HF_REPO_ID,\n",
    "        folder_path=cfg.OUTPUT_DIR,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Uploaded to: https://huggingface.co/datasets/{cfg.HF_REPO_ID}\")\n",
    "else:\n",
    "    print(\"\\n[INFO] HF_TOKEN ho·∫∑c HF_REPO_ID ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh.\")\n",
    "    print(\"       B·ªè qua b∆∞·ªõc upload. Features ƒë√£ ƒë∆∞·ª£c l∆∞u local t·∫°i:\")\n",
    "    print(f\"       {cfg.OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403e6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. VERIFY - KI·ªÇM TRA L·∫†I FEATURES ƒê√É L∆ØU\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "h5_path = os.path.join(cfg.OUTPUT_DIR, \"vit_visual_feat.h5\")\n",
    "idx2vid_path = os.path.join(cfg.OUTPUT_DIR, \"idx2vid.pkl\")\n",
    "\n",
    "if os.path.exists(h5_path) and os.path.exists(idx2vid_path):\n",
    "    # Load l·∫°i v√† ki·ªÉm tra\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        feats = f['resnet_features']\n",
    "        print(f\"‚úÖ vit_visual_feat.h5\")\n",
    "        print(f\"   Shape: {feats.shape}\")\n",
    "        print(f\"   Dtype: {feats.dtype}\")\n",
    "        \n",
    "        # Sample m·ªôt video\n",
    "        sample_feat = feats[0]\n",
    "        print(f\"   Sample video[0] shape: {sample_feat.shape}\")\n",
    "        print(f\"   Sample video[0] mean: {sample_feat.mean():.4f}\")\n",
    "    \n",
    "    with open(idx2vid_path, 'rb') as f:\n",
    "        loaded_idx2vid = pickle.load(f)\n",
    "    print(f\"\\n‚úÖ idx2vid.pkl\")\n",
    "    print(f\"   Number of videos: {len(loaded_idx2vid)}\")\n",
    "    print(f\"   First 5 videos: {loaded_idx2vid[:5]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìã S·ª¨ D·ª§NG TRONG DATALOADER:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\"\"\n",
    "# Trong DataLoader.py, thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n:\n",
    "video_feature_path = \"{cfg.OUTPUT_DIR}\"\n",
    "\n",
    "# Ho·∫∑c sau khi upload l√™n HuggingFace:\n",
    "# video_feature_path = \"/path/to/downloaded/vit_visual_features\"\n",
    "\n",
    "# File structure:\n",
    "#   {cfg.OUTPUT_DIR}/\n",
    "#   ‚îú‚îÄ‚îÄ vit_visual_feat.h5   # (N, T, {HIDDEN_DIM})\n",
    "#   ‚îî‚îÄ‚îÄ idx2vid.pkl          # list of video_ids\n",
    "\n",
    "# L∆∞u √Ω: C·∫ßn s·ª≠a DataLoader.py ƒë·ªÉ load \"vit_visual_feat.h5\" \n",
    "# thay v√¨ \"appearance_feat.h5\" v√† \"motion_feat.h5\"\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"‚ùå Feature files not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ EXTRACTION COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "üìÇ Output files:\n",
    "   - {cfg.OUTPUT_DIR}/vit_visual_feat.h5\n",
    "   - {cfg.OUTPUT_DIR}/idx2vid.pkl\n",
    "\n",
    "üìä Feature info:\n",
    "   - Model: {cfg.VIT_NAME}\n",
    "   - Feature dim: {HIDDEN_DIM}\n",
    "   - Frames per video: {cfg.NUM_FRAMES}\n",
    "\n",
    "üîÑ ƒê·ªÉ s·ª≠ d·ª•ng features n√†y trong training:\n",
    "   1. Upload l√™n HuggingFace (ƒë√£ l√†m n·∫øu HF_TOKEN ƒë∆∞·ª£c c·∫•u h√¨nh)\n",
    "   2. S·ª≠a DataLoader.py ƒë·ªÉ load vit_visual_feat.h5\n",
    "   3. Ho·∫∑c t·∫°o DataLoader m·ªõi cho ViT features\n",
    "\n",
    "üéØ Next steps:\n",
    "   - Download features t·ª´ HuggingFace\n",
    "   - C·∫≠p nh·∫≠t DataLoader.py\n",
    "   - Ch·∫°y training v·ªõi ViT features\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
