{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TranSTR CausalVid - Final Notebook\n",
    "\n",
    "Notebook ho√†n ch·ªânh cho **training v√† evaluation** model TranSTR tr√™n CausalVidQA.\n",
    "\n",
    "**T√≠nh nƒÉng:**\n",
    "- DeBERTa encode text **real-time** (kh√¥ng c·∫ßn pre-extraction)\n",
    "- Download model t·ª´ HuggingFace\n",
    "- Detailed evaluation theo paper metrics (D, E, PAR, CAR, Acc(ALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# --- Git Clone & Setup ---\n",
    "REPO_URL = \"https://github.com/DanielQH07/tranSTR_Casual.git\" \n",
    "REPO_NAME = \"tranSTR_Casual\"\n",
    "BRANCH = \"origin\" \n",
    "\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    print(f\"Cloning {REPO_URL}...\")\n",
    "    !git clone {REPO_URL} -b {BRANCH}\n",
    "else:\n",
    "    print(\"Repo already exists.\")\n",
    "\n",
    "# Change Directory to the repo root \n",
    "if os.path.basename(os.getcwd()) != REPO_NAME:\n",
    "    try:\n",
    "        target_dir = os.path.join(os.getcwd(), REPO_NAME, \"causalvid\")\n",
    "        if os.path.exists(target_dir):\n",
    "             os.chdir(target_dir)\n",
    "        elif os.path.exists(REPO_NAME):\n",
    "             os.chdir(REPO_NAME)\n",
    "        \n",
    "        print(f\"Changed directory to: {os.getcwd()}\")\n",
    "    except Exception as e:\n",
    "             print(f\"Could not set working directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: HuggingFace\n",
    "print('=== CELL 2 ===')\n",
    "!pip install -q huggingface_hub\n",
    "from huggingface_hub import login, HfApi, hf_hub_download, list_repo_tree\n",
    "# notebook_login()\n",
    "login(token='') # Replace with your actual token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Imports\n",
    "print('=== CELL 3: Imports ===')\n",
    "import os, torch, numpy as np, pandas as pd, tarfile, shutil, json\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.util import set_seed, set_gpu_devices\n",
    "from DataLoader import VideoQADataset\n",
    "from networks.model import VideoQAmodel\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Train/Eval functions (No AMP - Safe for DeBERTa)\n",
    "print('=== CELL 4 ===')\n",
    "\n",
    "def train_epoch(model, optimizer, loader, xe, device, scaler):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for batch in loader:\n",
    "        ff, of, q, a, ans_id, _ = batch\n",
    "        ff, of, tgt = ff.to(device), of.to(device), ans_id.to(device)\n",
    "        \n",
    "        # No autocast - DeBERTa has issues with fp16\n",
    "        out = model(ff, of, q, a)\n",
    "        loss = xe(out, tgt)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += (out.argmax(-1) == tgt).sum().item()\n",
    "        total += tgt.size(0)\n",
    "    return total_loss / len(loader), correct / total * 100\n",
    "\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ff, of, q, a, ans_id, _ = batch\n",
    "            out = model(ff.to(device), of.to(device), q, a)\n",
    "            correct += (out.argmax(-1) == ans_id.to(device)).sum().item()\n",
    "            total += ans_id.size(0)\n",
    "    return correct / total * 100\n",
    "\n",
    "print('Functions defined (No AMP - DeBERTa safe)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 + 6: Setup Paths & Config\n",
    "print('=== CELL 5+6: Paths & Config ===')\n",
    "\n",
    "# ============================================\n",
    "# KAGGLE INPUT PATHS - UPDATE THESE!\n",
    "# ============================================\n",
    "# ViT video features (folder contains video_id.pt files directly)\n",
    "VIT_FEATURE_PATH = '/kaggle/input/vit-features-full-merged'  # Contains: video_id.pt files\n",
    "\n",
    "# Object detection features (direct read from Kaggle)\n",
    "OBJ_FEATURE_PATH = '/kaggle/input/object-detection-causal-full'  # Contains: features_node_X/video.pkl\n",
    "\n",
    "# Annotations (folder contains video_id subfolders with text.json, answer.json)\n",
    "ANNOTATION_PATH = '/kaggle/input/text-annotation/QA'  # Contains: video_id/text.json, answer.json\n",
    "\n",
    "# Split files (train.pkl, valid.pkl, test.pkl)\n",
    "SPLIT_DIR = '/kaggle/input/casual-vid-data-split/split'  # Contains: train.pkl, valid.pkl, test.pkl\n",
    "\n",
    "# ============================================\n",
    "# WORKING DIRECTORIES\n",
    "# ============================================\n",
    "BASE = '/kaggle/working'\n",
    "MODEL_DIR = os.path.join(BASE, 'models')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================\n",
    "# VERIFY PATHS\n",
    "# ============================================\n",
    "print('\\n--- Path Verification ---')\n",
    "\n",
    "def verify_path(name, path, expected_sample=None):\n",
    "    if os.path.exists(path):\n",
    "        items = os.listdir(path)[:5]\n",
    "        print(f'‚úÖ {name}')\n",
    "        print(f'   Path: {path}')\n",
    "        print(f'   Sample: {items}')\n",
    "        return True\n",
    "    else:\n",
    "        print(f'‚ùå {name}: NOT FOUND')\n",
    "        print(f'   Path: {path}')\n",
    "        return False\n",
    "\n",
    "all_ok = True\n",
    "all_ok &= verify_path('ViT Features', VIT_FEATURE_PATH)\n",
    "all_ok &= verify_path('Object Features', OBJ_FEATURE_PATH)\n",
    "all_ok &= verify_path('Annotations', ANNOTATION_PATH)\n",
    "all_ok &= verify_path('Splits', SPLIT_DIR)\n",
    "\n",
    "if not all_ok:\n",
    "    print('\\n‚ö†Ô∏è  Please update paths above!')\n",
    "\n",
    "# ============================================\n",
    "# CONFIG\n",
    "# ============================================\n",
    "RUN_TRAINING = True\n",
    "HF_REPO_ID = 'DanielQ07/transtr-causalvid-weights'\n",
    "HF_MODEL_FILENAME = 'best_model.ckpt'\n",
    "\n",
    "class Config:\n",
    "    # Paths from Kaggle input\n",
    "    video_feature_root = VIT_FEATURE_PATH   # video_id.pt files directly\n",
    "    object_feature_path = OBJ_FEATURE_PATH  # features_node_X/video.pkl\n",
    "    sample_list_path = ANNOTATION_PATH      # video_id/text.json, answer.json\n",
    "    split_dir_txt = SPLIT_DIR               # train.pkl, valid.pkl, test.pkl\n",
    "    \n",
    "    # Model architecture (paper config)\n",
    "    topK_frame = 16\n",
    "    objs = 20\n",
    "    frames = 16\n",
    "    select_frames = 5\n",
    "    topK_obj = 12\n",
    "    frame_feat_dim = 1024\n",
    "    obj_feat_dim = 2053\n",
    "    d_model = 768\n",
    "    word_dim = 768\n",
    "    nheads = 8\n",
    "    num_encoder_layers = 2\n",
    "    num_decoder_layers = 2\n",
    "    normalize_before = True\n",
    "    activation = 'gelu'\n",
    "    dropout = 0.3\n",
    "    encoder_dropout = 0.3\n",
    "    \n",
    "    # Text encoder\n",
    "    text_encoder_type = 'microsoft/deberta-base'\n",
    "    freeze_text_encoder = False\n",
    "    text_encoder_lr = 1e-5\n",
    "    text_pool_mode = 1\n",
    "    \n",
    "    # Training\n",
    "    bs = 8\n",
    "    lr = 1e-5\n",
    "    epoch = 20\n",
    "    gpu = 0\n",
    "    patience = 5\n",
    "    gamma = 0.1\n",
    "    decay = 1e-4\n",
    "    n_query = 5\n",
    "    \n",
    "    # Other\n",
    "    hard_eval = False\n",
    "    pos_ratio = 1.0\n",
    "    neg_ratio = 1.0\n",
    "    a = 1.0\n",
    "    use_amp = True\n",
    "    num_workers = 4\n",
    "\n",
    "args = Config()\n",
    "set_gpu_devices(args.gpu)\n",
    "set_seed(999)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\nDevice: {device}')\n",
    "print('Config loaded!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Create Datasets with Verification\n",
    "print('=== CELL 7: Datasets ===')\n",
    "\n",
    "# Configuration for limiting train samples (set to None for no limit)\n",
    "MAX_TRAIN_SAMPLES = 2000  # Change this to limit training videos, or None for all\n",
    "\n",
    "# Create datasets with detailed logging\n",
    "print('\\n--- Creating TRAIN dataset ---')\n",
    "train_ds = VideoQADataset(\n",
    "    split='train', \n",
    "    n_query=args.n_query, \n",
    "    obj_num=args.objs, \n",
    "    sample_list_path=args.sample_list_path, \n",
    "    video_feature_path=args.video_feature_root, \n",
    "    object_feature_path=args.object_feature_path, \n",
    "    split_dir=args.split_dir_txt, \n",
    "    topK_frame=args.topK_frame,\n",
    "    max_samples=MAX_TRAIN_SAMPLES,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print('\\n--- Creating VAL dataset ---')\n",
    "val_ds = VideoQADataset(\n",
    "    split='val', \n",
    "    n_query=args.n_query, \n",
    "    obj_num=args.objs, \n",
    "    sample_list_path=args.sample_list_path, \n",
    "    video_feature_path=args.video_feature_root, \n",
    "    object_feature_path=args.object_feature_path, \n",
    "    split_dir=args.split_dir_txt, \n",
    "    topK_frame=args.topK_frame,\n",
    "    max_samples=None,  # Don't limit val/test\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print('\\n--- Creating TEST dataset ---')\n",
    "test_ds = VideoQADataset(\n",
    "    split='test', \n",
    "    n_query=args.n_query, \n",
    "    obj_num=args.objs, \n",
    "    sample_list_path=args.sample_list_path, \n",
    "    video_feature_path=args.video_feature_root, \n",
    "    object_feature_path=args.object_feature_path, \n",
    "    split_dir=args.split_dir_txt, \n",
    "    topK_frame=args.topK_frame,\n",
    "    max_samples=None,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, args.bs, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, args.bs, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, args.bs, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "# Summary\n",
    "print('\\n' + '='*60)\n",
    "print('DATASET SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Train: {len(train_ds)} samples -> {len(train_loader)} batches')\n",
    "print(f'Val:   {len(val_ds)} samples -> {len(val_loader)} batches')\n",
    "print(f'Test:  {len(test_ds)} samples -> {len(test_loader)} batches')\n",
    "print('='*60)\n",
    "\n",
    "# Quick sanity check - load one batch\n",
    "if len(train_ds) > 0:\n",
    "    print('\\nSanity check: Loading first batch...')\n",
    "    try:\n",
    "        ff, of, qns, ans, ans_id, keys = next(iter(train_loader))\n",
    "        print(f'  ViT features: {ff.shape}')  # Expected: [batch, topK_frame, feat_dim]\n",
    "        print(f'  Object features: {of.shape}')  # Expected: [batch, topK_frame, obj_num, 2053]\n",
    "        print(f'  Answer IDs: {ans_id}')\n",
    "        print('Sanity check PASSED!')\n",
    "    except Exception as e:\n",
    "        print(f'  ERROR: {e}')\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Model\n",
    "print('=== CELL 8: Model ===')\n",
    "cfg = {k: v for k, v in Config.__dict__.items() if not k.startswith('_')}\n",
    "cfg['device'] = device\n",
    "cfg['topK_frame'] = args.select_frames\n",
    "model = VideoQAmodel(**cfg)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', factor=args.gamma, patience=args.patience)\n",
    "model.to(device)\n",
    "xe = nn.CrossEntropyLoss()\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=True)\n",
    "save_path = os.path.join(MODEL_DIR, HF_MODEL_FILENAME)\n",
    "print(f'Model: {sum(p.numel() for p in model.parameters())/1e6:.1f}M params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Training\n",
    "print('=== CELL 9: Training ===')\n",
    "best_acc = 0\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    for ep in range(1, args.epoch + 1):\n",
    "        loss, acc = train_epoch(model, optimizer, train_loader, xe, device, scaler)\n",
    "        val_acc = eval_epoch(model, val_loader, device)\n",
    "        scheduler.step(val_acc)\n",
    "        print(f'Ep {ep}: Loss={loss:.4f}, Train={acc:.1f}%, Val={val_acc:.1f}%')\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'  Saved!')\n",
    "    print(f'\\nBest Val: {best_acc:.1f}%')\n",
    "    try:\n",
    "        api = HfApi()\n",
    "        api.create_repo(repo_id=HF_REPO_ID, repo_type='model', exist_ok=True)\n",
    "        api.upload_file(path_or_fileobj=save_path, path_in_repo=HF_MODEL_FILENAME, repo_id=HF_REPO_ID, repo_type='model')\n",
    "        print('Uploaded!')\n",
    "    except Exception as e:\n",
    "        print(f'Upload failed: {e}')\n",
    "else:\n",
    "    print('Skipping training (RUN_TRAINING=False)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Detailed Test Evaluation (Download from HuggingFace)\n",
    "print('=== CELL 10: TEST Set Evaluation ===')\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import hf_hub_download\n",
    "from networks.model import VideoQAmodel\n",
    "\n",
    "# --- CONFIG ---\n",
    "HF_REPO_ID = 'DanielQ07/transtr-causalvid-weights'\n",
    "HF_FILENAME = 'best_model.ckpt'\n",
    "LOCAL_MODEL_PATH = os.path.join(MODEL_DIR, HF_FILENAME)\n",
    "\n",
    "# 1. Download Model from HuggingFace if not exists locally\n",
    "if not os.path.exists(LOCAL_MODEL_PATH):\n",
    "    print(f\"\\nüì• Downloading {HF_FILENAME} from HuggingFace ({HF_REPO_ID})...\")\n",
    "    try:\n",
    "        model_path = hf_hub_download(repo_id=HF_REPO_ID, filename=HF_FILENAME, local_dir=MODEL_DIR)\n",
    "        print(f\"‚úÖ Downloaded to: {model_path}\")\n",
    "        LOCAL_MODEL_PATH = model_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to download model: {e}\")\n",
    "        print(\"üí° Make sure Internet is ON and Repo ID is correct.\")\n",
    "else:\n",
    "    print(f\"üìÇ Found local model at: {LOCAL_MODEL_PATH}\")\n",
    "\n",
    "# 2. Load Model weights\n",
    "if os.path.exists(LOCAL_MODEL_PATH):\n",
    "    print(f\"üîß Loading weights...\")\n",
    "    state = torch.load(LOCAL_MODEL_PATH, map_location=device, weights_only=True)\n",
    "    msg = model.load_state_dict(state, strict=False)\n",
    "    print(f\"Load status: {msg}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CRITICAL: No model weights found! Using Random Weights.\")\n",
    "\n",
    "# DEBUG: Check dataset sample_list to see why answer = -1\n",
    "print(\"\\nüîç DEBUG - Checking Dataset sample_list:\")\n",
    "if 'test_ds' in globals():\n",
    "    sample = test_ds.sample_list.iloc[0]\n",
    "    print(f\"  First sample in sample_list:\")\n",
    "    print(f\"    video_id: {sample['video_id']}\")\n",
    "    print(f\"    type: {sample['type']}\")\n",
    "    print(f\"    answer: {sample['answer']} (Type: {type(sample['answer'])})\")\n",
    "    print(f\"    question: {sample['question'][:50]}...\")\n",
    "    \n",
    "    # Check answer distribution\n",
    "    print(f\"\\n  Answer value distribution:\")\n",
    "    print(test_ds.sample_list['answer'].value_counts().head(10))\n",
    "    \n",
    "    # Try to read raw answer.json to see actual format\n",
    "    import json\n",
    "    sample_vid = sample['video_id']\n",
    "    answer_json_path = f\"{args.sample_list_path}/{sample_vid}/answer.json\"\n",
    "    if os.path.exists(answer_json_path):\n",
    "        with open(answer_json_path, 'r') as f:\n",
    "            raw_answer = json.load(f)\n",
    "        print(f\"\\n  Raw answer.json for {sample_vid}:\")\n",
    "        print(f\"    {json.dumps(raw_answer, indent=2)[:500]}...\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ùå answer.json not found at: {answer_json_path}\")\n",
    "\n",
    "# 3. FIXED Evaluation Function - t√≠nh metric TR·ª∞C TI·∫æP t·ª´ batch\n",
    "def evaluate_detailed_v2(model, loader, device):\n",
    "    \"\"\"\n",
    "    T√≠nh metric tr·ª±c ti·∫øp t·ª´ batch, kh√¥ng c·∫ßn join v·ªõi dataframe g·ªëc.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect all results by type\n",
    "    type_results = {}  # {qtype: [(pred, target, video_id), ...]}\n",
    "    \n",
    "    print(\"\\nüìä Running Detailed Evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            # Unpack: ff, of, qns, ans_word, ans_id, qns_keys\n",
    "            ff, of, qns, ans_word, ans_id, qns_keys = batch\n",
    "            ff = ff.to(device)\n",
    "            of = of.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            out = model(ff, of, qns, ans_word)\n",
    "            preds = out.argmax(dim=-1).cpu().numpy()\n",
    "            targets = ans_id.numpy()\n",
    "            \n",
    "            # Parse qns_keys to get video_id and type\n",
    "            for key, pred, target in zip(qns_keys, preds, targets):\n",
    "                # key format: \"video_id_type\" e.g., \"abc123_descriptive\" or \"abc123_predictive_reason\"\n",
    "                # Known types: descriptive, explanatory, predictive, predictive_reason, \n",
    "                #              counterfactual, counterfactual_reason\n",
    "                \n",
    "                # Check for _reason suffix first (2-part type)\n",
    "                if key.endswith('_reason'):\n",
    "                    # Try to find known prefix: predictive_reason or counterfactual_reason\n",
    "                    if '_predictive_reason' in key:\n",
    "                        idx = key.rfind('_predictive_reason')\n",
    "                        video_id = key[:idx]\n",
    "                        qtype = 'predictive_reason'\n",
    "                    elif '_counterfactual_reason' in key:\n",
    "                        idx = key.rfind('_counterfactual_reason')\n",
    "                        video_id = key[:idx]\n",
    "                        qtype = 'counterfactual_reason'\n",
    "                    else:\n",
    "                        # Fallback\n",
    "                        parts = key.rsplit('_', 2)\n",
    "                        video_id = parts[0] if len(parts) > 2 else key\n",
    "                        qtype = '_'.join(parts[1:]) if len(parts) > 1 else 'unknown'\n",
    "                else:\n",
    "                    # Single-part type: descriptive, explanatory, predictive, counterfactual\n",
    "                    parts = key.rsplit('_', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        video_id, qtype = parts\n",
    "                    else:\n",
    "                        video_id, qtype = key, 'unknown'\n",
    "                \n",
    "                if qtype not in type_results:\n",
    "                    type_results[qtype] = []\n",
    "                type_results[qtype].append({\n",
    "                    'video_id': video_id,\n",
    "                    'pred': int(pred),\n",
    "                    'target': int(target),\n",
    "                    'correct': int(pred) == int(target)\n",
    "                })\n",
    "    \n",
    "    # DEBUG: Show first few predictions per type\n",
    "    print(\"\\nüîç DEBUG - Sample Predictions vs Targets:\")\n",
    "    for qtype, results in type_results.items():\n",
    "        if len(results) > 0:\n",
    "            sample = results[0]\n",
    "            correct_count = sum(1 for r in results if r['correct'])\n",
    "            print(f\"  [{qtype}] Count: {len(results)}, Correct: {correct_count}\")\n",
    "            print(f\"    First: pred={sample['pred']}, target={sample['target']}, match={sample['correct']}\")\n",
    "            if len(results) > 1:\n",
    "                sample2 = results[1]\n",
    "                print(f\"    Second: pred={sample2['pred']}, target={sample2['target']}, match={sample2['correct']}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics_map = {\n",
    "        'Description': 'descriptive',\n",
    "        'Explanation': 'explanatory',\n",
    "        'Predictive-Answer': 'predictive',\n",
    "        'Predictive-Reason': 'predictive_reason',\n",
    "        'Counterfactual-Answer': 'counterfactual',\n",
    "        'Counterfactual-Reason': 'counterfactual_reason'\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION RESULTS - TEST SET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Standard metrics\n",
    "    for name, qtype in metrics_map.items():\n",
    "        if qtype in type_results:\n",
    "            results = type_results[qtype]\n",
    "            correct = sum(1 for r in results if r['correct'])\n",
    "            total = len(results)\n",
    "            acc = correct / total * 100 if total > 0 else 0\n",
    "        else:\n",
    "            correct, total, acc = 0, 0, 0\n",
    "        metrics[name] = acc\n",
    "        print(f\"{name:<25} ==>   {acc:.2f}%  ({correct}/{total})\")\n",
    "\n",
    "    # Hard Metrics (AND logic) - c·∫£ answer v√† reason ƒë·ªÅu ƒë√∫ng\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    def calc_hard_metric(type_ans, type_reason, name):\n",
    "        if type_ans not in type_results or type_reason not in type_results:\n",
    "            metrics[name] = 0\n",
    "            print(f\"{name:<25} ==>   0.00%  (0/0 paired)\")\n",
    "            return\n",
    "        \n",
    "        # Build lookup by video_id\n",
    "        ans_by_vid = {r['video_id']: r['correct'] for r in type_results[type_ans]}\n",
    "        reason_by_vid = {r['video_id']: r['correct'] for r in type_results[type_reason]}\n",
    "        \n",
    "        # Find common video_ids\n",
    "        common_vids = set(ans_by_vid.keys()) & set(reason_by_vid.keys())\n",
    "        \n",
    "        both_correct = sum(1 for vid in common_vids if ans_by_vid[vid] and reason_by_vid[vid])\n",
    "        total = len(common_vids)\n",
    "        acc = both_correct / total * 100 if total > 0 else 0\n",
    "        metrics[name] = acc\n",
    "        print(f\"{name:<25} ==>   {acc:.2f}%  ({both_correct}/{total} paired)\")\n",
    "\n",
    "    calc_hard_metric('predictive', 'predictive_reason', 'PAR')\n",
    "    calc_hard_metric('counterfactual', 'counterfactual_reason', 'CAR')\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Acc (ALL) = (D + E + PAR + CAR) / 4 (paper definition)\n",
    "    d_acc = metrics.get('Description', 0)\n",
    "    e_acc = metrics.get('Explanation', 0)\n",
    "    par_acc = metrics.get('PAR', 0)\n",
    "    car_acc = metrics.get('CAR', 0)\n",
    "    \n",
    "    acc_all = (d_acc + e_acc + par_acc + car_acc) / 4\n",
    "    metrics['Acc (ALL)'] = acc_all\n",
    "    print(f\"{'Acc (ALL)':<25} ==>   {acc_all:.2f}%  ((D+E+PAR+CAR)/4)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Plot\n",
    "    plot_metrics(metrics)\n",
    "    return metrics, type_results\n",
    "\n",
    "def plot_metrics(metrics):\n",
    "    keys = ['Description', 'Explanation', 'PAR', 'CAR', 'Acc (ALL)']\n",
    "    values = [metrics.get(k, 0) for k in keys]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(keys, values, color=sns.color_palette(\"viridis\", len(keys)))\n",
    "    plt.ylim(0, 100)\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('VideoQA Performance on Test Set')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for bar in bars:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                f'{bar.get_height():.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test_results.png')\n",
    "    plt.show()\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# ‚ö†Ô∏è TEST SET kh√¥ng c√≥ ground truth labels (to√†n -1)\n",
    "# ‚Üí D√πng VALIDATION SET ƒë·ªÉ evaluate thay th·∫ø\n",
    "\n",
    "RUN_SMALL_TEST = False  # üî¥ Set True to test with 5 batches, False for full run\n",
    "USE_VAL_SET = False     # üî¥ Set True to use VAL set (has labels), False for TEST set\n",
    "\n",
    "if USE_VAL_SET:\n",
    "    print(\"\\nüìå Using VALIDATION SET (has ground truth labels)\")\n",
    "    eval_loader = val_loader\n",
    "else:\n",
    "    print(\"\\nüìå Using TEST SET (‚ö†Ô∏è may have -1 labels if held out)\")\n",
    "    eval_loader = test_loader\n",
    "\n",
    "if 'val_loader' in globals():\n",
    "    from itertools import islice\n",
    "    \n",
    "    loader_to_run = eval_loader\n",
    "    if RUN_SMALL_TEST:\n",
    "        print(\"‚ö†Ô∏è RUNNING SMALL TEST MODE (5 batches only)\")\n",
    "        print(\"To run full evaluation, set RUN_SMALL_TEST = False\")\n",
    "        loader_to_run = list(islice(eval_loader, 5))\n",
    "\n",
    "    metrics, raw_results = evaluate_detailed_v2(model, loader_to_run, device)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è 'val_loader' not defined. Run previous cells to load data first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
