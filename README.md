# CausalVidQA - Video Question Answering with TranSTR

<div align="center">

[![](https://img.shields.io/badge/paper-pink?style=plastic&logo=GitBook)](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Invariant_Grounding_for_Video_Question_Answering_CVPR_2022_paper.pdf)
[![](https://img.shields.io/badge/-github-grey?style=plastic&logo=github)](https://github.com/yl3800/IGV) 

</div>

---

## ğŸ“Š Dataset Statistics

### CausalVidQA Dataset

| Split | Videos | Samples (qtype=-1) | MÃ´ táº£ |
|-------|--------|-------------------|-------|
| Train | ~8,000 | ~48,000 | Training set |
| Valid | 2,695 | 16,170 | Validation set |
| Test | 5,429 | 32,574 | Test set |

> **âš ï¸ LÆ°u Ã½**: Dataset `dataset-split-1` trÃªn Kaggle cÃ³ váº¥n Ä‘á» - file `train.pkl` chá»‰ chá»©a **1 video**! Cáº§n sá»­ dá»¥ng dataset split Ä‘áº§y Ä‘á»§ hoáº·c tá»± táº¡o láº¡i file split.

### Question Types (6 loáº¡i - má»—i video cÃ³ 6 cÃ¢u há»i)

| qtype | TÃªn | MÃ´ táº£ | VÃ­ dá»¥ |
|-------|-----|-------|-------|
| 0 | **Descriptive** | MÃ´ táº£ hÃ nh Ä‘á»™ng/sá»± kiá»‡n | "What is the person doing?" |
| 1 | **Explanatory** | Giáº£i thÃ­ch nguyÃªn nhÃ¢n | "Why did the person do that?" |
| 2 | **Predictive Answer** | Dá»± Ä‘oÃ¡n káº¿t quáº£ | "What will happen next?" |
| 3 | **Predictive Reason** | LÃ½ do dá»± Ä‘oÃ¡n | "Why will that happen?" |
| 4 | **Counterfactual Answer** | Káº¿t quáº£ giáº£ Ä‘á»‹nh | "What would happen if...?" |
| 5 | **Counterfactual Reason** | LÃ½ do giáº£ Ä‘á»‹nh | "Why would that happen?" |

### Visual Features (what the code actually uses)

| Tensor | Shape | Produced from |
|--------|-------|---------------|
| Appearance | `(T, 2048)` or `(T, N, 2048)` | `appearance_feat.h5` (ResNet-101), mean over N if present |
| Motion | `(T, 2048)` or `(T, N, 2048)` | `motion_feat.h5` (3D ResNet), mean over N if present |
| Frame (combined) | `(T, 4096)` | concat(appearance, motion) |
| â€œObjectâ€ | `(T, O, 2053)` | appearance tiled O times + dummy full-frame bbox (no detector) |

---

## ğŸ“ Cáº¥u trÃºc dá»¯ liá»‡u

```
kaggle-input/
â”œâ”€â”€ visual-feature/
â”‚   â”œâ”€â”€ appearance_feat.h5    # (N_videos, T, 2048) hoáº·c (N_videos, T, C, 2048)
â”‚   â”œâ”€â”€ motion_feat.h5        # (N_videos, T, 2048)
â”‚   â””â”€â”€ idx2vid.pkl           # List[video_id] - index to video_id mapping
â”‚
â”œâ”€â”€ text-annotation/
â”‚   â””â”€â”€ QA/
â”‚       â”œâ”€â”€ video_id_1/
â”‚       â”‚   â”œâ”€â”€ text.json     # Questions vÃ  candidate answers
â”‚       â”‚   â””â”€â”€ answer.json   # Ground truth answers (0-4)
â”‚       â””â”€â”€ video_id_2/
â”‚           â””â”€â”€ ...
â”‚
â””â”€â”€ dataset-split-1/
    â”œâ”€â”€ train.pkl             # List[video_id] cho training
    â”œâ”€â”€ valid.pkl             # List[video_id] cho validation  
    â””â”€â”€ test.pkl              # List[video_id] cho testing
```

---

## ğŸ”„ End-to-end flow (what the current code does)

1) **Inputs on disk**
   - `appearance_feat.h5`, `motion_feat.h5`, `idx2vid.pkl`
   - `text-annotation/QA/<video_id>/{text.json, answer.json}`
   - Split files `{train,valid/test,test}.pkl`

2) **Dataset construction (`VideoQADataset` in `DataLoader.py`)**
   - Load video IDs from split pkl (optionally truncated by `max_samples`).
   - Map video IDs â†’ feature indices via `idx2vid.pkl`.
   - Load appearance and motion features for the videos; if a 3rd dim exists, mean-pool over it.
   - Build sample list by `qtype`:
     - `-1`: all qtypes 0â€“5
     - `0` or `1`: single type
     - `2`: predictive answer + predictive reason (2,3)
     - `3`: counterfactual answer + counterfactual reason (4,5)  âš ï¸ naming quirk
   - For each sample: load question/answers from `text.json` + labels from `answer.json`.

3) **Feature preparation (per sample)**
   - Frame feature: concat appearance + motion â†’ `(T, 4096)`.
   - â€œObjectâ€ feature: tile appearance to `(T, O, 2048)`, append dummy bbox `[0,0,1,1,1]` â†’ `(T, O, 2053)`.  
     *No object detector is run; boxes are full-frame placeholders.*
   - Answers are formatted as `[CLS] question [SEP] candidate_i`.
   - Outputs per item: `(vid_frame_feat, vid_obj_feat, qns_word, ans_word, ans_id, qns_key)`.

4) **Dataloaders**
   - `train/val/test` loaders with given batch size, optional workers; `pin_memory=True`.

5) **Model (`networks/model.py`)**
   - Encodes video features and text; uses transformer decoder for answer scoring.
   - Expects both frame and â€œobjectâ€ tensors even though objects are synthesized as above.

6) **Training loop (`transtr.ipynb`)**
   - Supports gradient accumulation, LR scheduling, early stopping, W&B logging.
   - Tracks per-qtype accuracy and wrong samples.

7) **Evaluation / prediction**
   - Best checkpoint evaluated on test loader.
   - Predictions saved to JSON; per-qtype metrics computed; wrong samples logged/saved.

---

### Text Annotation Format

**text.json:**
```json
{
  "descriptive": {
    "question": "What is the man doing?",
    "answer": ["Walking", "Running", "Sitting", "Standing", "Jumping"]
  },
  "explanatory": {
    "question": "Why is the man running?",
    "answer": ["To catch bus", "To exercise", "Being chased", "Late for work", "For fun"]
  },
  "predictive": {
    "question": "What will happen next?",
    "answer": ["He will stop", "He will fall", "He will continue", "He will turn", "He will sit"],
    "reason": ["He is tired", "Road is slippery", "He has energy", "He sees something", "Reached destination"]
  },
  "counterfactual": {
    "question": "What would happen if he stopped?",
    "answer": ["Miss the bus", "Rest", "Fall down", "Get caught", "Nothing"],
    "reason": ["Bus leaves", "He is tired", "Momentum", "Chaser catches up", "No effect"]
  }
}
```

**answer.json:**
```json
{
  "descriptive": {"answer": 1},
  "explanatory": {"answer": 0},
  "predictive": {"answer": 2, "reason": 2},
  "counterfactual": {"answer": 0, "reason": 0}
}
```

---

## ğŸ”§ CÃ i Ä‘áº·t

```bash
cd causalvid
pip install -r requirements.txt
```

### Requirements
- Python 3.8+
- PyTorch 1.11+
- transformers
- h5py
- einops
- numpy

---

## ğŸ“¥ Download dá»¯ liá»‡u

### Option 1: Kaggle API
```python
import kagglehub

visual_feature_path = kagglehub.dataset_download('lusnaw/visual-feature')
split_path = kagglehub.dataset_download('lusnaw/dataset-split-1')
text_annotation_path = kagglehub.dataset_download('lusnaw/text-annotation')
```

### Option 2: Kaggle Notebook
ThÃªm datasets vÃ o notebook:
- `lusnaw/visual-feature`
- `lusnaw/text-annotation`
- `lusnaw/dataset-split-1`

---

## ğŸš€ Training

### Command Line

```bash
python train.py \
    -v causalvid_full \
    -bs 16 \
    -lr 1e-4 \
    -epoch 20 \
    -gpu 0 \
    --sample_list_path "/path/to/dataset-split-1" \
    --video_feature_path "/path/to/visual-feature" \
    --text_annotation_path "/path/to/text-annotation" \
    --qtype -1 \
    -fk 8 -ok 5 -objs 20 \
    -el 1 -dl 1 \
    -t microsoft/deberta-base
```

### Kaggle Notebook
Sá»­ dá»¥ng `train_causalvid.ipynb`:
1. Install dependencies
2. Patch DataLoader (xá»­ lÃ½ dimension mismatch)
3. Configure hyperparameters
4. Train model

### Tham sá»‘ chÃ­nh

| Tham sá»‘ | MÃ´ táº£ | Máº·c Ä‘á»‹nh |
|---------|-------|----------|
| `-v` | TÃªn experiment | required |
| `-bs` | Batch size | 16 |
| `-lr` | Learning rate | 1e-4 |
| `-epoch` | Sá»‘ epochs | 20 |
| `--qtype` | Loáº¡i cÃ¢u há»i (-1=all, 0-5=specific) | -1 |
| `--max_samples` | Giá»›i háº¡n sá»‘ video (None=all) | None |
| `-fk` | Top-K frames | 8 |
| `-ok` | Top-K objects | 5 |
| `-objs` | Sá»‘ objects/frame | 20 |
| `-t` | Text encoder | microsoft/deberta-base |

---

## ğŸ§ª Evaluation

### Cháº¡y Ä‘Ã¡nh giÃ¡

```bash
python test.py \
    -v eval_test \
    -bs 32 \
    --sample_list_path "/path/to/dataset-split-1" \
    --video_feature_path "/path/to/visual-feature" \
    --text_annotation_path "/path/to/text-annotation" \
    --qtype -1 \
    --model_path "./models/best_model-xxx.ckpt"
```

### Evaluation Metrics

| Metric | MÃ´ táº£ | CÃ¡ch tÃ­nh |
|--------|-------|-----------|
| **Des** | Descriptive accuracy | ÄÃºng/Tá»•ng samples vá»›i qtype=0 |
| **Exp** | Explanatory accuracy | ÄÃºng/Tá»•ng samples vá»›i qtype=1 |
| **Pred-A** | Predictive Answer | ÄÃºng/Tá»•ng samples vá»›i qtype=2 |
| **Pred-R** | Predictive Reason | ÄÃºng/Tá»•ng samples vá»›i qtype=3 |
| **CF-A** | Counterfactual Answer | ÄÃºng/Tá»•ng samples vá»›i qtype=4 |
| **CF-R** | Counterfactual Reason | ÄÃºng/Tá»•ng samples vá»›i qtype=5 |
| **Pred** | Predictive Combined | Cáº£ answer VÃ€ reason Ä‘Ãºng cho cÃ¹ng video |
| **CF** | Counterfactual Combined | Cáº£ answer VÃ€ reason Ä‘Ãºng cho cÃ¹ng video |
| **ALL** | Overall | (Des + Exp + Pred + CF) / 4 |

### Evaluation Script

```python
from eval_mc import accuracy_metric_cvid

# ÄÃ¡nh giÃ¡ tá»« file prediction
accuracy_metric_cvid('./prediction/result.json')

# Output example:
# Des: 45.32%
# Exp: 38.21%
# Pred-A: 42.15%  Pred-R: 35.67%  Pred: 28.43%
# CF-A: 40.89%    CF-R: 33.45%    CF: 25.12%
# ALL: 34.22%
```

### Giáº£i thÃ­ch cÃ¡ch tÃ­nh Pred vÃ  CF

```
Pred (Combined) = Sá»‘ videos cÃ³ Cáº¢ Pred-A VÃ€ Pred-R Ä‘Ãºng / Tá»•ng videos
CF (Combined) = Sá»‘ videos cÃ³ Cáº¢ CF-A VÃ€ CF-R Ä‘Ãºng / Tá»•ng videos

ALL = (Des + Exp + Pred + CF) / 4
    = (45.32 + 38.21 + 28.43 + 25.12) / 4
    = 34.27%
```

---

## ğŸ“Š Expected Results

### Baseline (Random)
- 5-way multiple choice: ~20% accuracy

### Trained Model
| Metric | Expected Range |
|--------|----------------|
| Des | 45-55% |
| Exp | 35-45% |
| Pred | 25-35% |
| CF | 20-30% |
| ALL | 30-40% |

---

## âš ï¸ Known Issues & Solutions

### 1. Train split chá»‰ cÃ³ 1 video
**Váº¥n Ä‘á»**: `train.pkl` trÃªn Kaggle `dataset-split-1` chá»‰ chá»©a 1 video

**Giáº£i phÃ¡p**:
```python
# Option 1: Swap train vá»›i valid Ä‘á»ƒ test
train_split = 'valid'  
val_split = 'test'     

# Option 2: Tá»± táº¡o train.pkl tá»« toÃ n bá»™ videos
import pickle
# Load idx2vid.pkl Ä‘á»ƒ láº¥y táº¥t cáº£ video IDs
# Chia theo tá»· lá»‡ 70/15/15 cho train/val/test
```

### 2. DeBERTa FP16 Overflow
**Váº¥n Ä‘á»**: `RuntimeError: value cannot be converted to type at::Half`

**Giáº£i phÃ¡p**: Táº¯t mixed precision
```python
USE_AMP = False
```

### 3. Multiprocessing Error trÃªn Kaggle
**Váº¥n Ä‘á»**: `Bad file descriptor` vá»›i num_workers > 0

**Giáº£i phÃ¡p**: 
```python
DataLoader(..., num_workers=0)
```

### 4. Dimension Mismatch giá»¯a app vÃ  mot features
**Váº¥n Ä‘á»**: `appearance_feat` cÃ³ 3 dims, `motion_feat` cÃ³ 2 dims

**Giáº£i phÃ¡p** (Ä‘Ã£ patch trong DataLoader):
```python
if app_feat.ndim == 3:
    app_feat = app_feat.mean(axis=1)
if mot_feat.ndim == 3:
    mot_feat = mot_feat.mean(axis=1)
```

---

## ğŸ“‚ Output Files

```
causalvid/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model-{version}.ckpt    # Model checkpoint
â”œâ”€â”€ prediction/
â”‚   â””â”€â”€ {version}-{epoch}-{acc}.json # Predictions
â””â”€â”€ log/
    â””â”€â”€ {version}.log                # Training log
```

### Prediction JSON Format
```json
{
  "video_id_0": {"prediction": 2, "answer": 2, "qtype": 0},
  "video_id_1": {"prediction": 0, "answer": 1, "qtype": 1},
  ...
}
```

---

## ğŸ—ï¸ Model Architecture - Answer Decoder

### Tá»•ng quan Answer Decoder

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           ANSWER DECODER ARCHITECTURE                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                                    INPUTS
                                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                             â”‚                             â”‚
        â–¼                             â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Video Memory â”‚           â”‚ Answer Query  â”‚           â”‚  Query Mask   â”‚
â”‚   (v_mem)     â”‚           â”‚   (a_query)   â”‚           â”‚   (q_mask)    â”‚
â”‚ [B, T, d_model]â”‚          â”‚[B*5, L, d_model]â”‚         â”‚  [B*5, L]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                           â”‚                           â”‚
        â”‚                           â–¼                           â”‚
        â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
        â”‚                   â”‚  [CLS] Token  â”‚                   â”‚
        â”‚                   â”‚   Extraction  â”‚                   â”‚
        â”‚                   â”‚  a_query[:,0] â”‚                   â”‚
        â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
        â”‚                           â”‚                           â”‚
        â”‚                           â–¼                           â”‚
        â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
        â”‚               â”‚    Expand v_mem       â”‚               â”‚
        â”‚               â”‚  repeat for 5 answers â”‚               â”‚
        â”‚               â”‚   [B, T, D] â†’         â”‚               â”‚
        â”‚               â”‚   [B*5, T, D]         â”‚               â”‚
        â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
        â”‚                           â”‚                           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRANSFORMER DECODER LAYER                               â”‚
â”‚                              (num_layers = 1)                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                           â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚  â”‚
â”‚  â”‚   â”‚  Self-Attention â”‚  â† Answer query attends to itself                   â”‚  â”‚
â”‚  â”‚   â”‚   (masked)      â”‚                                                     â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚  â”‚
â”‚  â”‚            â”‚                                                              â”‚  â”‚
â”‚  â”‚            â–¼                                                              â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚  â”‚
â”‚  â”‚   â”‚ Cross-Attention â”‚  â† Answer query attends to video memory             â”‚  â”‚
â”‚  â”‚   â”‚  Q: a_query     â”‚                                                     â”‚  â”‚
â”‚  â”‚   â”‚  K: v_mem       â”‚                                                     â”‚  â”‚
â”‚  â”‚   â”‚  V: v_mem       â”‚                                                     â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚  â”‚
â”‚  â”‚            â”‚                                                              â”‚  â”‚
â”‚  â”‚            â–¼                                                              â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚  â”‚
â”‚  â”‚   â”‚   Feed Forward  â”‚                                                     â”‚  â”‚
â”‚  â”‚   â”‚     Network     â”‚                                                     â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚  â”‚
â”‚  â”‚            â”‚                                                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Decoder Outputâ”‚
        â”‚ [B*5, L, D]   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Take [CLS]   â”‚
        â”‚   Position    â”‚
        â”‚  output[:,0]  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Reshape     â”‚
        â”‚ [B*5, D] â†’    â”‚
        â”‚ [B, 5, D]     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            ANSWER CLASSIFIER                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚  â”‚
â”‚  â”‚   â”‚  Linear Layer   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Linear Layer   â”‚       â”‚  â”‚
â”‚  â”‚   â”‚   (D â†’ 1)       â”‚                           â”‚  (squeeze)      â”‚       â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              [B, 5, D] â†’ [B, 5, 1] â†’ [B, 5]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    OUTPUT     â”‚
        â”‚  Logits [B,5] â”‚
        â”‚  (5 answers)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Softmax     â”‚
        â”‚  (in loss)    â”‚
        â”‚ â†’ Prediction  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Parameter Details

| Parameter | Value | Description |
|-----------|-------|-------------|
| `d_model` | 768 | Hidden dimension |
| `nheads` | 8 | Attention heads |
| `num_decoder_layers` | 1 | Number of decoder layers |
| `dropout` | 0.1 | Dropout rate |
| `activation` | relu | Activation function |
| `n_query` | 5 | Number of answer choices |

### Data Flow Example (Batch size B=16)

```
1. INPUT:
   â”œâ”€â”€ v_mem: [16, T, 768]      # Video memory from encoder
   â”œâ”€â”€ a_query: [80, L, 768]    # 16*5 answer embeddings  
   â””â”€â”€ q_mask: [80, L]          # Answer attention masks

2. EXPAND VIDEO MEMORY:
   v_mem: [16, T, 768] â†’ repeat â†’ [80, T, 768]
   (Each video paired with 5 answers)

3. TRANSFORMER DECODER:
   â”œâ”€â”€ Self-Attention: a_query attends to a_query
   â”œâ”€â”€ Cross-Attention: a_query attends to v_mem
   â””â”€â”€ FFN: position-wise feed-forward
   Output: [80, L, 768]

4. EXTRACT [CLS]:
   output[:,0,:]: [80, 768]

5. RESHAPE:
   [80, 768] â†’ [16, 5, 768]

6. CLASSIFIER:
   [16, 5, 768] â†’ Linear â†’ [16, 5, 1] â†’ squeeze â†’ [16, 5]

7. OUTPUT:
   Logits: [16, 5] (score for each of 5 answers)
   Prediction: argmax â†’ answer index (0-4)
```

### Cross-Attention Mechanism

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CROSS-ATTENTION DETAIL                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚         Answer Query                          Video Memory                      â”‚
â”‚         [B*5, L, D]                           [B*5, T, D]                       â”‚
â”‚              â”‚                                     â”‚                            â”‚
â”‚              â–¼                                     â–¼                            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚         â”‚   Wq   â”‚                           â”‚  Wk, Wv  â”‚                       â”‚
â”‚         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚              â”‚                                    â”‚                             â”‚
â”‚              â–¼                                    â–¼                             â”‚
â”‚         Q [B*5, L, D]                    K, V [B*5, T, D]                       â”‚
â”‚              â”‚                                    â”‚                             â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                             â”‚                                                   â”‚
â”‚                             â–¼                                                   â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚                   â”‚  Attention Scores   â”‚                                       â”‚
â”‚                   â”‚  Q @ K^T / sqrt(d)  â”‚                                       â”‚
â”‚                   â”‚  [B*5, L, T]        â”‚                                       â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                              â”‚                                                  â”‚
â”‚                              â–¼                                                  â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚                   â”‚      Softmax        â”‚                                       â”‚
â”‚                   â”‚  [B*5, L, T]        â”‚                                       â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                              â”‚                                                  â”‚
â”‚                              â–¼                                                  â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚                   â”‚  Attention @ V      â”‚                                       â”‚
â”‚                   â”‚  [B*5, L, D]        â”‚                                       â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                              â”‚                                                  â”‚
â”‚                              â–¼                                                  â”‚
â”‚                   Answer-aware Video                                            â”‚
â”‚                      Representation                                             â”‚
â”‚                                                                                 â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚  Ã nghÄ©a: Má»—i answer "nhÃ¬n" vÃ o video Ä‘á»ƒ tÃ¬m evidence há»— trá»£                    â”‚
â”‚  â†’ Answer Ä‘Ãºng sáº½ cÃ³ attention cao vÃ o frames liÃªn quan                         â”‚
â”‚  â†’ Answer sai sáº½ cÃ³ attention tháº¥p hoáº·c khÃ´ng phÃ¹ há»£p                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— References

- [IGV Paper (CVPR 2022)](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Invariant_Grounding_for_Video_Question_Answering_CVPR_2022_paper.pdf)
- [CausalVidQA Dataset](https://github.com/bcmi/Causal-VidQA)
- [Original IGV Code](https://github.com/yl3800/IGV)

---

## ğŸ“ Citation

```bibtex
@InProceedings{Li_2022_CVPR,
    author    = {Li, Yicong and Wang, Xiang and Xiao, Junbin and Ji, Wei and Chua, Tat-Seng},
    title     = {Invariant Grounding for Video Question Answering},
    booktitle = {CVPR},
    year      = {2022},
    pages     = {2928-2937}
}
```