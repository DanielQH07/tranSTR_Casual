<h2 align="center">
Invariant Grounding for Video Question Answering ğŸ”¥
</h2>

<div align="center">

[![](https://img.shields.io/badge/paper-pink?style=plastic&logo=GitBook)](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Invariant_Grounding_for_Video_Question_Answering_CVPR_2022_paper.pdf)
[![](https://img.shields.io/badge/-github-grey?style=plastic&logo=github)](https://github.com/yl3800/IGV) 
[![](https://img.shields.io/badge/video-red?style=plastic&logo=airplayvideo)](https://youtu.be/wJhR9_dcsaM) 
</div>


## Overview 
This repo contains source code for **Invariant Grounding for Video Question Answering** (CVPR 2022 Oral, Best Paper Finalists). In this work, propose a new learning framework, Invariant Grounding for VideoQA (**IGV**), to ground the question-critical scene, whose causal relations with answers are invariant across different interventions on the complement. With IGV, the VideoQA models are forced to shield the answering process from the negative influence of spurious correlations, which significantly improves the reasoning ability.
    
<figure> <img src="figures/interventional-distributions.png" height="220"></figure>

---

# CausalVidQA - Training Guide

## ğŸ“ Cáº¥u trÃºc dá»¯ liá»‡u

Dá»¯ liá»‡u CausalVidQA Ä‘Æ°á»£c táº£i tá»« Kaggle:

```
visual-feature/
â”œâ”€â”€ appearance_feat.h5    # Appearance features (ResNet)
â”œâ”€â”€ motion_feat.h5        # Motion features (ResNet)
â””â”€â”€ idx2vid.pkl           # Video ID mapping

text-annotation/
â”œâ”€â”€ video_id_1/
â”‚   â”œâ”€â”€ text.json         # Questions vÃ  candidate answers
â”‚   â””â”€â”€ answer.json       # Ground truth answers
â”œâ”€â”€ video_id_2/
â”‚   â””â”€â”€ ...

dataset-split-1/
â”œâ”€â”€ train.pkl             # Train video IDs
â”œâ”€â”€ val.pkl               # Validation video IDs
â””â”€â”€ test.pkl              # Test video IDs
```

## ğŸ”§ CÃ i Ä‘áº·t

```bash
pip install -r requirements.txt
pip install kagglehub
```

## ğŸ“¥ Download dá»¯ liá»‡u

```python
import kagglehub

text_feature_path = kagglehub.dataset_download('lusnaw/text-feature')
visual_feature_path = kagglehub.dataset_download('lusnaw/visual-feature')
split_path = kagglehub.dataset_download('lusnaw/dataset-split-1')
text_annotation_path = kagglehub.dataset_download('lusnaw/text-annotation')
```

## ğŸš€ Training

### Train Ä‘áº§y Ä‘á»§

```bash
python train.py \
    -v full_train \
    -bs 32 \
    -lr 1e-5 \
    -epoch 15 \
    -gpu 0 \
    --sample_list_path "/path/to/dataset-split-1" \
    --video_feature_path "/path/to/visual-feature" \
    --text_annotation_path "/path/to/text-annotation" \
    --qtype -1 \
    -fk 8 \
    -ok 5 \
    -objs 20 \
    -el 1 \
    -dl 1 \
    -t microsoft/deberta-base
```

### Train nhanh (test vá»›i sá»‘ video giá»›i háº¡n)

```bash
# Train vá»›i 10 videos (60 samples vÃ¬ má»—i video cÃ³ 6 loáº¡i cÃ¢u há»i)
python train.py \
    -v quick_test \
    -bs 4 \
    -lr 1e-4 \
    -epoch 2 \
    -gpu 0 \
    --sample_list_path "/path/to/dataset-split-1" \
    --video_feature_path "/path/to/visual-feature" \
    --text_annotation_path "/path/to/text-annotation" \
    --qtype -1 \
    --max_samples 10 \
    -fk 4 \
    -ok 5 \
    -objs 10
```

### Train theo loáº¡i cÃ¢u há»i cá»¥ thá»ƒ

```bash
# Chá»‰ train vá»›i cÃ¢u há»i descriptive (qtype=0)
python train.py -v descriptive_only --qtype 0 ...
```

## ğŸ§ª Testing

```bash
python test.py \
    -v test_eval \
    -bs 32 \
    -gpu 0 \
    --sample_list_path "/path/to/dataset-split-1" \
    --video_feature_path "/path/to/visual-feature" \
    --text_annotation_path "/path/to/text-annotation" \
    --qtype -1 \
    -fk 8 \
    -ok 5 \
    -objs 20 \
    -t microsoft/deberta-base \
    --model_path "./models/best_model-xxx.ckpt"
```

## ğŸ¯ Script cháº¡y nhanh

```bash
# Tá»± Ä‘á»™ng download data vÃ  train
python run_small_test.py --run

# Train vá»›i sá»‘ video tÃ¹y chá»‰nh
python run_small_test.py --run --max_samples 50
```

## ğŸ“‹ Tham sá»‘ chÃ­nh

| Tham sá»‘ | MÃ´ táº£ | Máº·c Ä‘á»‹nh |
|---------|-------|----------|
| `-v` | TÃªn version/experiment | (required) |
| `-bs` | Batch size | 32 |
| `-lr` | Learning rate | 1e-5 |
| `-epoch` | Sá»‘ epochs | 15 |
| `-gpu` | GPU ID | 0 |
| `--qtype` | Loáº¡i cÃ¢u há»i (-1=all, 0-5=specific) | -1 |
| `--max_samples` | Giá»›i háº¡n sá»‘ video | None (all) |
| `-fk` | Top-K frames | 8 |
| `-ok` | Top-K objects | 5 |
| `-objs` | Sá»‘ objects per frame | 20 |
| `-el` | Encoder layers | 1 |
| `-dl` | Decoder layers | 1 |
| `-t` | Text encoder model | microsoft/deberta-base |

## ğŸ“Š Loáº¡i cÃ¢u há»i (qtype)

| qtype | Loáº¡i cÃ¢u há»i | MÃ´ táº£ |
|-------|--------------|-------|
| -1 | All | Táº¥t cáº£ 6 loáº¡i |
| 0 | Descriptive | MÃ´ táº£ |
| 1 | Explanatory | Giáº£i thÃ­ch |
| 2 | Predictive Answer | Dá»± Ä‘oÃ¡n (cÃ¢u tráº£ lá»i) |
| 3 | Predictive Reason | Dá»± Ä‘oÃ¡n (lÃ½ do) |
| 4 | Counterfactual Answer | Pháº£n thá»±c (cÃ¢u tráº£ lá»i) |
| 5 | Counterfactual Reason | Pháº£n thá»±c (lÃ½ do) |

## ğŸ“‚ Output

- **Models**: `./models/best_model-{version}.ckpt`
- **Predictions**: `./prediction/{version}-{epoch}-{acc}.json`
- **Logs**: `./log/{version}.log`

## ğŸ’¡ VÃ­ dá»¥ Windows PowerShell

```powershell
cd d:\KLTN\TranSTR\causalvid

# Train vá»›i 10 videos
python train.py -v test10 -bs 4 -epoch 2 -gpu 0 `
    --sample_list_path "C:\Users\xxx\.cache\kagglehub\datasets\lusnaw\dataset-split-1\versions\1" `
    --video_feature_path "C:\Users\xxx\.cache\kagglehub\datasets\lusnaw\visual-feature\versions\1" `
    --text_annotation_path "C:\Users\xxx\.cache\kagglehub\datasets\lusnaw\text-annotation\versions\1" `
    --max_samples 10 -fk 4 -ok 5 -objs 10
```

## ğŸ” Evaluation Metrics

Káº¿t quáº£ Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ theo tá»«ng loáº¡i cÃ¢u há»i:

- **Des**: Descriptive accuracy
- **Exp**: Explanatory accuracy  
- **Pred-A**: Predictive Answer accuracy
- **Pred-R**: Predictive Reason accuracy
- **CF-A**: Counterfactual Answer accuracy
- **CF-R**: Counterfactual Reason accuracy
- **Pred**: Predictive (cáº£ answer vÃ  reason Ä‘Ãºng)
- **CF**: Counterfactual (cáº£ answer vÃ  reason Ä‘Ãºng)
- **ALL**: Overall accuracy (Des + Exp + Pred + CF)

---

## Installation (Original)
- Main packages: PyTorch = 1.11 
- See `requirements.txt` for other packages.

## Data Preparation (Original)
We use MSVD-QA as an example to help get farmiliar with the code. Please download the pre-computed features and trained models [here](https://drive.google.com/file/d/1MrupFq8jubEA4nEl4CppR5Rddz9rW_6Z/view?usp=sharing)

After downloading the data, please modify your data path in `run.py`.

## Run IGV

Simply run `run.sh` to reproduce the results in the paper. 


## Reference 
```
@InProceedings{Li_2022_CVPR,
    author    = {Li, Yicong and Wang, Xiang and Xiao, Junbin and Ji, Wei and Chua, Tat-Seng},
    title     = {Invariant Grounding for Video Question Answering},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {2928-2937}
}
```