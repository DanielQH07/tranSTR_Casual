{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TranSTR CausalVid - W&B Version (CLIP Features)\n",
    "\n",
    "Notebook hoÃ n chá»‰nh cho **training vÃ  evaluation** model TranSTR trÃªn CausalVidQA.\n",
    "\n",
    "**TÃ­nh nÄƒng:**\n",
    "- **CLIP ViT-Large-Patch14 features** (D=1024) thay vÃ¬ ViT features (768)\n",
    "- DeBERTa encode text **real-time** (khÃ´ng cáº§n pre-extraction)\n",
    "- **Weights & Biases (W&B)** Ä‘á»ƒ log metrics vÃ  checkpoints\n",
    "- Detailed evaluation theo paper metrics (D, E, PAR, CAR, Acc(ALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Git Clone & Setup\n",
    "import os\n",
    "REPO_URL = \"https://github.com/DanielQH07/tranSTR_Casual.git\" \n",
    "REPO_NAME = \"tranSTR_Casual\"\n",
    "BRANCH = \"origin\" \n",
    "\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    print(f\"Cloning {REPO_URL}...\")\n",
    "    !git clone {REPO_URL} -b {BRANCH}\n",
    "else:\n",
    "    print(\"Repo already exists.\")\n",
    "\n",
    "# Change Directory to the repo root \n",
    "if os.path.basename(os.getcwd()) != REPO_NAME:\n",
    "    try:\n",
    "        target_dir = os.path.join(os.getcwd(), REPO_NAME, \"causalvid\")\n",
    "        if os.path.exists(target_dir):\n",
    "             os.chdir(target_dir)\n",
    "        elif os.path.exists(REPO_NAME):\n",
    "             os.chdir(REPO_NAME)\n",
    "        \n",
    "        print(f\"Changed directory to: {os.getcwd()}\")\n",
    "    except Exception as e:\n",
    "             print(f\"Could not set working directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Install & Login W&B (vá»›i API Key trá»±c tiáº¿p)\n",
    "print('=== CELL 2: W&B Setup ===')\n",
    "!pip install -q wandb\n",
    "\n",
    "import wandb\n",
    "\n",
    "# ============================================\n",
    "# WANDB CONFIG - THAY THáº¾ Báº°NG API KEY Cá»¦A Báº N\n",
    "# ============================================\n",
    "WANDB_API_KEY = 'YOUR_WANDB_API_KEY_HERE'  # ðŸ”´ Paste your W&B API key here\n",
    "WANDB_PROJECT = 'transtr-causalvid-clip'   # Project name on W&B\n",
    "WANDB_ENTITY = None                        # Your W&B username/team (None = default)\n",
    "\n",
    "# Login with API key (no interactive prompt)\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "print('âœ… W&B logged in successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Imports\n",
    "print('=== CELL 3: Imports ===')\n",
    "import os, torch, numpy as np, pandas as pd, json\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.util import set_seed, set_gpu_devices\n",
    "from DataLoader import VideoQADataset\n",
    "from networks.model import VideoQAmodel\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Train/Eval functions (vá»›i W&B logging)\n",
    "print('=== CELL 4: Functions ===')\n",
    "\n",
    "def train_epoch(model, optimizer, loader, xe, device, epoch):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f'Epoch {epoch}', leave=False)\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        ff, of, q, a, ans_id, _ = batch\n",
    "        ff, of, tgt = ff.to(device), of.to(device), ans_id.to(device)\n",
    "        \n",
    "        out = model(ff, of, q, a)\n",
    "        loss = xe(out, tgt)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += (out.argmax(-1) == tgt).sum().item()\n",
    "        total += tgt.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': total_loss/(batch_idx+1), 'acc': correct/total*100})\n",
    "        \n",
    "        # Log batch metrics to W&B (every 50 batches)\n",
    "        if batch_idx % 50 == 0:\n",
    "            wandb.log({\n",
    "                'batch_loss': loss.item(),\n",
    "                'batch_acc': (out.argmax(-1) == tgt).float().mean().item() * 100,\n",
    "                'batch': epoch * len(loader) + batch_idx\n",
    "            })\n",
    "    \n",
    "    return total_loss / len(loader), correct / total * 100\n",
    "\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ff, of, q, a, ans_id, _ = batch\n",
    "            out = model(ff.to(device), of.to(device), q, a)\n",
    "            correct += (out.argmax(-1) == ans_id.to(device)).sum().item()\n",
    "            total += ans_id.size(0)\n",
    "    return correct / total * 100\n",
    "\n",
    "print('Functions defined with W&B logging!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge CLIP Features (train/test/valid -> merged folder)\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ============================================\n",
    "# ðŸ”´ UPDATE THESE PATHS\n",
    "# ============================================\n",
    "CLIP_SPLIT_PATH = '/kaggle/input/clip-vit-large-patch14-features'  # Folder chá»©a train/test/valid\n",
    "CLIP_MERGED_PATH = '/kaggle/working/clip_features_merged'          # Folder output Ä‘Ã£ gá»™p\n",
    "\n",
    "# Check structure\n",
    "print(f'\\nSource: {CLIP_SPLIT_PATH}')\n",
    "if os.path.exists(CLIP_SPLIT_PATH):\n",
    "    subfolders = os.listdir(CLIP_SPLIT_PATH)\n",
    "    print(f'Subfolders found: {subfolders}')\n",
    "else:\n",
    "    print('âŒ Source path not found!')\n",
    "    subfolders = []\n",
    "\n",
    "# Merge folders\n",
    "os.makedirs(CLIP_MERGED_PATH, exist_ok=True)\n",
    "\n",
    "total_copied = 0\n",
    "for split in ['train', 'test', 'valid', 'val']:  # Try common folder names\n",
    "    split_folder = os.path.join(CLIP_SPLIT_PATH, split)\n",
    "    if not os.path.exists(split_folder):\n",
    "        continue\n",
    "    \n",
    "    pt_files = [f for f in os.listdir(split_folder) if f.endswith('.pt')]\n",
    "    print(f'\\nðŸ“ {split}: {len(pt_files)} files')\n",
    "    \n",
    "    for fname in tqdm(pt_files, desc=f'Copying {split}'):\n",
    "        src = os.path.join(split_folder, fname)\n",
    "        dst = os.path.join(CLIP_MERGED_PATH, fname)\n",
    "        \n",
    "        # Skip if already exists\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copy2(src, dst)\n",
    "            total_copied += 1\n",
    "\n",
    "# Summary\n",
    "final_count = len([f for f in os.listdir(CLIP_MERGED_PATH) if f.endswith('.pt')])\n",
    "print(f'\\nâœ… Merge complete!')\n",
    "print(f'   Total .pt files in merged folder: {final_count}')\n",
    "print(f'   Merged path: {CLIP_MERGED_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: Setup Paths & Config\n",
    "print('=== CELL 5: Paths & Config ===')\n",
    "\n",
    "# ============================================\n",
    "# KAGGLE INPUT PATHS - UPDATE THESE!\n",
    "# ============================================\n",
    "# ðŸ”´ THAY Äá»”I: Sá»­ dá»¥ng CLIP features (1024 dim) thay vÃ¬ ViT features (768 dim)\n",
    "CLIP_FEATURE_PATH = CLIP_MERGED_PATH  # ðŸ”´ UPDATE PATH TO YOUR CLIP FEATURES\n",
    "OBJ_FEATURE_PATH = '/kaggle/input/object-detection-causal-full'\n",
    "ANNOTATION_PATH = '/kaggle/input/text-annotation/QA'\n",
    "SPLIT_DIR = '/kaggle/input/casual-vid-data-split/split'\n",
    "\n",
    "# Working directories\n",
    "BASE = '/kaggle/working'\n",
    "MODEL_DIR = os.path.join(BASE, 'models')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Verify paths\n",
    "print('\\n--- Path Verification ---')\n",
    "def verify_path(name, path):\n",
    "    if os.path.exists(path):\n",
    "        items = os.listdir(path)[:3]\n",
    "        print(f'âœ… {name}: {items}')\n",
    "        return True\n",
    "    else:\n",
    "        print(f'âŒ {name}: NOT FOUND - {path}')\n",
    "        return False\n",
    "\n",
    "all_ok = True\n",
    "all_ok &= verify_path('CLIP Features (1024D)', CLIP_FEATURE_PATH)\n",
    "all_ok &= verify_path('Object Features', OBJ_FEATURE_PATH)\n",
    "all_ok &= verify_path('Annotations', ANNOTATION_PATH)\n",
    "all_ok &= verify_path('Splits', SPLIT_DIR)\n",
    "\n",
    "if not all_ok:\n",
    "    print('\\nâš ï¸ Please update paths above!')\n",
    "\n",
    "# ============================================\n",
    "# CONFIG - CLIP ViT-Large-Patch14 (1024 dim)\n",
    "# ============================================\n",
    "RUN_TRAINING = True\n",
    "MODEL_FILENAME = 'best_model_clip.ckpt'\n",
    "\n",
    "# ðŸ”´ Feature dimension for CLIP ViT-Large-Patch14\n",
    "FEAT_DIM = 1024  # openai/clip-vit-large-patch14\n",
    "print(f'\\nðŸ”§ Backbone: openai/clip-vit-large-patch14 (D={FEAT_DIM})')\n",
    "\n",
    "class Config:\n",
    "    # Paths - ðŸ”´ CHANGED: Using CLIP features\n",
    "    video_feature_root = CLIP_FEATURE_PATH\n",
    "    object_feature_path = OBJ_FEATURE_PATH\n",
    "    sample_list_path = ANNOTATION_PATH\n",
    "    split_dir_txt = SPLIT_DIR\n",
    "    \n",
    "    # Model architecture\n",
    "    topK_frame = 16\n",
    "    objs = 20\n",
    "    frames = 16\n",
    "    select_frames = 5\n",
    "    topK_obj = 12\n",
    "    \n",
    "    # ðŸ”´ CHANGED: frame_feat_dim = 1024 (CLIP) instead of 768 (ViT)\n",
    "    frame_feat_dim = FEAT_DIM  # 1024 for CLIP ViT-Large-Patch14\n",
    "    \n",
    "    obj_feat_dim = 2053\n",
    "    d_model = 768\n",
    "    word_dim = 768\n",
    "    nheads = 8\n",
    "    num_encoder_layers = 2\n",
    "    num_decoder_layers = 2\n",
    "    normalize_before = True\n",
    "    activation = 'gelu'\n",
    "    dropout = 0.3\n",
    "    encoder_dropout = 0.3\n",
    "    \n",
    "    # Text encoder\n",
    "    text_encoder_type = 'microsoft/deberta-base'\n",
    "    freeze_text_encoder = False\n",
    "    text_encoder_lr = 1e-5\n",
    "    text_pool_mode = 1\n",
    "    \n",
    "    # Training\n",
    "    bs = 8\n",
    "    lr = 1e-5\n",
    "    epoch = 20\n",
    "    gpu = 0\n",
    "    patience = 5\n",
    "    gamma = 0.1\n",
    "    decay = 1e-4\n",
    "    n_query = 5\n",
    "    \n",
    "    # Other\n",
    "    hard_eval = False\n",
    "    pos_ratio = 1.0\n",
    "    neg_ratio = 1.0\n",
    "    a = 1.0\n",
    "    num_workers = 4\n",
    "\n",
    "args = Config()\n",
    "set_gpu_devices(args.gpu)\n",
    "set_seed(999)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\nDevice: {device}')\n",
    "print(f'Config loaded! frame_feat_dim = {args.frame_feat_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: Create Datasets\n",
    "print('=== CELL 6: Datasets ===')\n",
    "\n",
    "MAX_TRAIN_SAMPLES = 2000  # Change to None for all samples\n",
    "\n",
    "print('\\n--- Creating TRAIN dataset ---')\n",
    "train_ds = VideoQADataset(\n",
    "    split='train', n_query=args.n_query, obj_num=args.objs,\n",
    "    sample_list_path=args.sample_list_path,\n",
    "    video_feature_path=args.video_feature_root,\n",
    "    object_feature_path=args.object_feature_path,\n",
    "    split_dir=args.split_dir_txt, topK_frame=args.topK_frame,\n",
    "    max_samples=MAX_TRAIN_SAMPLES, verbose=True\n",
    ")\n",
    "\n",
    "print('\\n--- Creating VAL dataset ---')\n",
    "val_ds = VideoQADataset(\n",
    "    split='val', n_query=args.n_query, obj_num=args.objs,\n",
    "    sample_list_path=args.sample_list_path,\n",
    "    video_feature_path=args.video_feature_root,\n",
    "    object_feature_path=args.object_feature_path,\n",
    "    split_dir=args.split_dir_txt, topK_frame=args.topK_frame,\n",
    "    max_samples=None, verbose=True\n",
    ")\n",
    "\n",
    "print('\\n--- Creating TEST dataset ---')\n",
    "test_ds = VideoQADataset(\n",
    "    split='test', n_query=args.n_query, obj_num=args.objs,\n",
    "    sample_list_path=args.sample_list_path,\n",
    "    video_feature_path=args.video_feature_root,\n",
    "    object_feature_path=args.object_feature_path,\n",
    "    split_dir=args.split_dir_txt, topK_frame=args.topK_frame,\n",
    "    max_samples=None, verbose=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, args.bs, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, args.bs, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, args.bs, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('DATASET SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Train: {len(train_ds)} samples -> {len(train_loader)} batches')\n",
    "print(f'Val:   {len(val_ds)} samples -> {len(val_loader)} batches')\n",
    "print(f'Test:  {len(test_ds)} samples -> {len(test_loader)} batches')\n",
    "print(f'Feature dim: {args.frame_feat_dim} (CLIP ViT-Large-Patch14)')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Model\n",
    "print('=== CELL 7: Model ===')\n",
    "cfg = {k: v for k, v in Config.__dict__.items() if not k.startswith('_')}\n",
    "cfg['device'] = device\n",
    "cfg['topK_frame'] = args.select_frames\n",
    "\n",
    "print(f'Creating model with frame_feat_dim = {cfg[\"frame_feat_dim\"]}')\n",
    "\n",
    "model = VideoQAmodel(**cfg)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', factor=args.gamma, patience=args.patience)\n",
    "xe = nn.CrossEntropyLoss()\n",
    "save_path = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total params: {total_params/1e6:.1f}M')\n",
    "print(f'Trainable:    {trainable_params/1e6:.1f}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Initialize W&B Run\n",
    "print('=== CELL 8: Initialize W&B Run ===')\n",
    "\n",
    "# Create W&B config dict\n",
    "wandb_config = {\n",
    "    'model': 'TranSTR',\n",
    "    'backbone': 'openai/clip-vit-large-patch14',  # ðŸ”´ CHANGED\n",
    "    'frame_feat_dim': args.frame_feat_dim,         # ðŸ”´ ADDED\n",
    "    'text_encoder': args.text_encoder_type,\n",
    "    'batch_size': args.bs,\n",
    "    'learning_rate': args.lr,\n",
    "    'epochs': args.epoch,\n",
    "    'max_train_samples': MAX_TRAIN_SAMPLES,\n",
    "    'd_model': args.d_model,\n",
    "    'nheads': args.nheads,\n",
    "    'num_encoder_layers': args.num_encoder_layers,\n",
    "    'num_decoder_layers': args.num_decoder_layers,\n",
    "    'dropout': args.dropout,\n",
    "    'topK_frame': args.select_frames,\n",
    "    'topK_obj': args.topK_obj,\n",
    "    'train_samples': len(train_ds),\n",
    "    'val_samples': len(val_ds),\n",
    "    'test_samples': len(test_ds),\n",
    "}\n",
    "\n",
    "# Initialize W&B run\n",
    "run = wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    entity=WANDB_ENTITY,\n",
    "    config=wandb_config,\n",
    "    name=f'transtr-clip1024-ep{args.epoch}-bs{args.bs}',  # ðŸ”´ CHANGED name\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# Watch model for gradient logging\n",
    "wandb.watch(model, log='gradients', log_freq=100)\n",
    "\n",
    "print(f'âœ… W&B Run initialized: {run.url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Training with W&B Logging\n",
    "print('=== CELL 9: Training ===')\n",
    "\n",
    "best_acc = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    for ep in range(1, args.epoch + 1):\n",
    "        print(f'\\nEpoch {ep}/{args.epoch}')\n",
    "        \n",
    "        # Train\n",
    "        loss, train_acc = train_epoch(model, optimizer, train_loader, xe, device, ep)\n",
    "        \n",
    "        # Validate\n",
    "        val_acc = eval_epoch(model, val_loader, device)\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Log to history\n",
    "        history['train_loss'].append(loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # ðŸ“Š LOG TO W&B\n",
    "        wandb.log({\n",
    "            'epoch': ep,\n",
    "            'train_loss': loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc,\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "            'best_val_acc': max(best_acc, val_acc)\n",
    "        })\n",
    "        \n",
    "        print(f'Loss: {loss:.4f} | Train: {train_acc:.1f}% | Val: {val_acc:.1f}%')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'âœ… New best! Saved to {save_path}')\n",
    "            \n",
    "            # ðŸ“¦ LOG CHECKPOINT TO W&B\n",
    "            artifact = wandb.Artifact(\n",
    "                name='best-model-clip',  # ðŸ”´ CHANGED\n",
    "                type='model',\n",
    "                description=f'Best CLIP model at epoch {ep} with val_acc={val_acc:.2f}%',\n",
    "                metadata={\n",
    "                    'epoch': ep,\n",
    "                    'val_acc': val_acc,\n",
    "                    'train_acc': train_acc,\n",
    "                    'train_loss': loss,\n",
    "                    'backbone': 'clip-vit-large-patch14',\n",
    "                    'frame_feat_dim': FEAT_DIM\n",
    "                }\n",
    "            )\n",
    "            artifact.add_file(save_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "            print(f'ðŸ“¤ Checkpoint uploaded to W&B!')\n",
    "    \n",
    "    print(f'\\nðŸ† Best Val Accuracy: {best_acc:.1f}%')\n",
    "    \n",
    "    # Log final summary\n",
    "    wandb.run.summary['best_val_acc'] = best_acc\n",
    "    wandb.run.summary['final_train_loss'] = history['train_loss'][-1]\n",
    "    wandb.run.summary['total_epochs'] = args.epoch\n",
    "    wandb.run.summary['backbone'] = 'clip-vit-large-patch14'\n",
    "    wandb.run.summary['frame_feat_dim'] = FEAT_DIM\n",
    "    \n",
    "else:\n",
    "    print('Skipping training (RUN_TRAINING=False)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Detailed Evaluation with W&B Logging\n",
    "print('=== CELL 10: Detailed Evaluation ===')\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_detailed_v2(model, loader, device, log_to_wandb=True):\n",
    "    \"\"\"TÃ­nh metric trá»±c tiáº¿p tá»« batch vá»›i W&B logging.\"\"\"\n",
    "    model.eval()\n",
    "    type_results = {}\n",
    "    \n",
    "    print(\"\\nðŸ“Š Running Detailed Evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            ff, of, qns, ans_word, ans_id, qns_keys = batch\n",
    "            ff, of = ff.to(device), of.to(device)\n",
    "            \n",
    "            out = model(ff, of, qns, ans_word)\n",
    "            preds = out.argmax(dim=-1).cpu().numpy()\n",
    "            targets = ans_id.numpy()\n",
    "            \n",
    "            for key, pred, target in zip(qns_keys, preds, targets):\n",
    "                # Parse key to get video_id and type\n",
    "                if key.endswith('_reason'):\n",
    "                    if '_predictive_reason' in key:\n",
    "                        idx = key.rfind('_predictive_reason')\n",
    "                        video_id, qtype = key[:idx], 'predictive_reason'\n",
    "                    elif '_counterfactual_reason' in key:\n",
    "                        idx = key.rfind('_counterfactual_reason')\n",
    "                        video_id, qtype = key[:idx], 'counterfactual_reason'\n",
    "                    else:\n",
    "                        parts = key.rsplit('_', 2)\n",
    "                        video_id = parts[0] if len(parts) > 2 else key\n",
    "                        qtype = '_'.join(parts[1:]) if len(parts) > 1 else 'unknown'\n",
    "                else:\n",
    "                    parts = key.rsplit('_', 1)\n",
    "                    video_id, qtype = parts if len(parts) == 2 else (key, 'unknown')\n",
    "                \n",
    "                if qtype not in type_results:\n",
    "                    type_results[qtype] = []\n",
    "                type_results[qtype].append({\n",
    "                    'video_id': video_id,\n",
    "                    'pred': int(pred),\n",
    "                    'target': int(target),\n",
    "                    'correct': int(pred) == int(target)\n",
    "                })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    metrics_map = {\n",
    "        'Description': 'descriptive',\n",
    "        'Explanation': 'explanatory',\n",
    "        'Predictive-Answer': 'predictive',\n",
    "        'Predictive-Reason': 'predictive_reason',\n",
    "        'Counterfactual-Answer': 'counterfactual',\n",
    "        'Counterfactual-Reason': 'counterfactual_reason'\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for name, qtype in metrics_map.items():\n",
    "        if qtype in type_results:\n",
    "            results = type_results[qtype]\n",
    "            correct = sum(1 for r in results if r['correct'])\n",
    "            total = len(results)\n",
    "            acc = correct / total * 100 if total > 0 else 0\n",
    "        else:\n",
    "            correct, total, acc = 0, 0, 0\n",
    "        metrics[name] = acc\n",
    "        print(f\"{name:<25} ==>   {acc:.2f}%  ({correct}/{total})\")\n",
    "    \n",
    "    # Hard Metrics\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    def calc_hard_metric(type_ans, type_reason, name):\n",
    "        if type_ans not in type_results or type_reason not in type_results:\n",
    "            metrics[name] = 0\n",
    "            print(f\"{name:<25} ==>   0.00%  (0/0 paired)\")\n",
    "            return\n",
    "        \n",
    "        ans_by_vid = {r['video_id']: r['correct'] for r in type_results[type_ans]}\n",
    "        reason_by_vid = {r['video_id']: r['correct'] for r in type_results[type_reason]}\n",
    "        common_vids = set(ans_by_vid.keys()) & set(reason_by_vid.keys())\n",
    "        \n",
    "        both_correct = sum(1 for vid in common_vids if ans_by_vid[vid] and reason_by_vid[vid])\n",
    "        total = len(common_vids)\n",
    "        acc = both_correct / total * 100 if total > 0 else 0\n",
    "        metrics[name] = acc\n",
    "        print(f\"{name:<25} ==>   {acc:.2f}%  ({both_correct}/{total} paired)\")\n",
    "    \n",
    "    calc_hard_metric('predictive', 'predictive_reason', 'PAR')\n",
    "    calc_hard_metric('counterfactual', 'counterfactual_reason', 'CAR')\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Acc (ALL)\n",
    "    d_acc = metrics.get('Description', 0)\n",
    "    e_acc = metrics.get('Explanation', 0)\n",
    "    par_acc = metrics.get('PAR', 0)\n",
    "    car_acc = metrics.get('CAR', 0)\n",
    "    acc_all = (d_acc + e_acc + par_acc + car_acc) / 4\n",
    "    metrics['Acc_ALL'] = acc_all\n",
    "    print(f\"{'Acc (ALL)':<25} ==>   {acc_all:.2f}%  ((D+E+PAR+CAR)/4)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ðŸ“Š LOG FINAL METRICS TO W&B\n",
    "    if log_to_wandb:\n",
    "        wandb.log({\n",
    "            'eval/Description': metrics['Description'],\n",
    "            'eval/Explanation': metrics['Explanation'],\n",
    "            'eval/Predictive_Answer': metrics['Predictive-Answer'],\n",
    "            'eval/Predictive_Reason': metrics['Predictive-Reason'],\n",
    "            'eval/Counterfactual_Answer': metrics['Counterfactual-Answer'],\n",
    "            'eval/Counterfactual_Reason': metrics['Counterfactual-Reason'],\n",
    "            'eval/PAR': metrics['PAR'],\n",
    "            'eval/CAR': metrics['CAR'],\n",
    "            'eval/Acc_ALL': acc_all\n",
    "        })\n",
    "        \n",
    "        # Log summary\n",
    "        wandb.run.summary['eval_Description'] = metrics['Description']\n",
    "        wandb.run.summary['eval_Explanation'] = metrics['Explanation']\n",
    "        wandb.run.summary['eval_PAR'] = metrics['PAR']\n",
    "        wandb.run.summary['eval_CAR'] = metrics['CAR']\n",
    "        wandb.run.summary['eval_Acc_ALL'] = acc_all\n",
    "        print('ðŸ“¤ Metrics logged to W&B!')\n",
    "    \n",
    "    # Plot\n",
    "    keys = ['Description', 'Explanation', 'PAR', 'CAR', 'Acc_ALL']\n",
    "    values = [metrics.get(k, 0) for k in keys]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax.bar(keys, values, color=sns.color_palette(\"viridis\", len(keys)))\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title('VideoQA Performance (CLIP ViT-Large-Patch14)')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    for bar in bars:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                f'{bar.get_height():.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Log chart to W&B\n",
    "    if log_to_wandb:\n",
    "        wandb.log({'eval_chart': wandb.Image(fig)})\n",
    "    \n",
    "    plt.savefig('eval_results_clip.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics, type_results\n",
    "\n",
    "# Run evaluation on VALIDATION set (has ground truth)\n",
    "print(\"\\nðŸ“Œ Using VALIDATION SET for evaluation\")\n",
    "metrics, raw_results = evaluate_detailed_v2(model, val_loader, device, log_to_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Finish W&B Run\n",
    "print('=== CELL 11: Finish W&B ===')\n",
    "\n",
    "# Save metrics locally\n",
    "with open('final_metrics_clip.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print('Saved: final_metrics_clip.json')\n",
    "\n",
    "# Log final artifact with all results\n",
    "final_artifact = wandb.Artifact(\n",
    "    name='final-results-clip',\n",
    "    type='results',\n",
    "    description='Final evaluation results and metrics (CLIP ViT-Large-Patch14)',\n",
    "    metadata={\n",
    "        'backbone': 'clip-vit-large-patch14',\n",
    "        'frame_feat_dim': FEAT_DIM\n",
    "    }\n",
    ")\n",
    "final_artifact.add_file('final_metrics_clip.json')\n",
    "final_artifact.add_file('eval_results_clip.png')\n",
    "if os.path.exists(save_path):\n",
    "    final_artifact.add_file(save_path)\n",
    "wandb.log_artifact(final_artifact)\n",
    "\n",
    "# Finish run\n",
    "wandb.finish()\n",
    "print('\\nâœ… W&B run finished!')\n",
    "print(f'View results at: https://wandb.ai/{WANDB_ENTITY or \"your-username\"}/{WANDB_PROJECT}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
