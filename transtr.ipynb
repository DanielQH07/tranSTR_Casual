{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TranSTR CausalVid - Paper Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Clone\n",
    "import os\n",
    "print('=== CELL 1 ===')\n",
    "if not os.path.exists('tranSTR_Casual'):\n",
    "    !git clone https://github.com/DanielQH07/tranSTR_Casual.git\n",
    "os.chdir('tranSTR_Casual/causalvid' if os.path.exists('tranSTR_Casual/causalvid') else 'tranSTR_Casual')\n",
    "print(f'CWD: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: HuggingFace\n",
    "print('=== CELL 2 ===')\n",
    "!pip install -q huggingface_hub\n",
    "from huggingface_hub import login, HfApi, hf_hub_download, list_repo_tree\n",
    "# notebook_login()\n",
    "login(token='YOUR_HF_TOKEN') # Replace with your actual token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Imports\n",
    "print('=== CELL 3: Imports ===')\n",
    "import os, torch, numpy as np, pandas as pd, tarfile, shutil, json\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.util import set_seed, set_gpu_devices\n",
    "from DataLoader import VideoQADataset\n",
    "from networks.model import VideoQAmodel\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Train/Eval functions\n",
    "print('=== CELL 4 ===')\n",
    "\n",
    "def train_epoch(model, optimizer, loader, xe, device, scaler):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for batch in loader:\n",
    "        ff, of, q, a, ans_id, _ = batch\n",
    "        ff, of, tgt = ff.to(device), of.to(device), ans_id.to(device)\n",
    "        with torch.amp.autocast('cuda', enabled=True):\n",
    "            out = model(ff, of, q, a)\n",
    "            loss = xe(out, tgt)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "        correct += (out.argmax(-1) == tgt).sum().item()\n",
    "        total += tgt.size(0)\n",
    "    return total_loss / len(loader), correct / total * 100\n",
    "\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ff, of, q, a, ans_id, _ = batch\n",
    "            out = model(ff.to(device), of.to(device), q, a)\n",
    "            correct += (out.argmax(-1) == ans_id.to(device)).sum().item()\n",
    "            total += ans_id.size(0)\n",
    "    return correct / total * 100\n",
    "\n",
    "print('Functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 + 6: Setup Paths & Config\n",
    "print('=== CELL 5+6: Paths & Config ===')\n",
    "\n",
    "# ============================================\n",
    "# KAGGLE INPUT PATHS - UPDATE THESE!\n",
    "# ============================================\n",
    "# ViT video features (folder contains video_id.pt files directly)\n",
    "VIT_FEATURE_PATH = '/kaggle/input/YOUR_VIT_DATASET'  # Contains: video_id.pt files\n",
    "\n",
    "# Object detection features (direct read from Kaggle)\n",
    "OBJ_FEATURE_PATH = '/kaggle/input/object-detection-causal-full'  # Contains: features_node_X/video.pkl\n",
    "\n",
    "# Annotations (folder contains video_id subfolders with text.json, answer.json)\n",
    "ANNOTATION_PATH = '/kaggle/input/YOUR_ANNOTATION_DATASET'  # Contains: video_id/text.json, answer.json\n",
    "\n",
    "# Split files (train.pkl, valid.pkl, test.pkl)\n",
    "SPLIT_DIR = '/kaggle/input/YOUR_SPLITS_DATASET'  # Contains: train.pkl, valid.pkl, test.pkl\n",
    "\n",
    "# ============================================\n",
    "# WORKING DIRECTORIES\n",
    "# ============================================\n",
    "BASE = '/kaggle/working'\n",
    "MODEL_DIR = os.path.join(BASE, 'models')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================\n",
    "# VERIFY PATHS\n",
    "# ============================================\n",
    "print('\\n--- Path Verification ---')\n",
    "\n",
    "def verify_path(name, path, expected_sample=None):\n",
    "    if os.path.exists(path):\n",
    "        items = os.listdir(path)[:5]\n",
    "        print(f'✅ {name}')\n",
    "        print(f'   Path: {path}')\n",
    "        print(f'   Sample: {items}')\n",
    "        return True\n",
    "    else:\n",
    "        print(f'❌ {name}: NOT FOUND')\n",
    "        print(f'   Path: {path}')\n",
    "        return False\n",
    "\n",
    "all_ok = True\n",
    "all_ok &= verify_path('ViT Features', VIT_FEATURE_PATH)\n",
    "all_ok &= verify_path('Object Features', OBJ_FEATURE_PATH)\n",
    "all_ok &= verify_path('Annotations', ANNOTATION_PATH)\n",
    "all_ok &= verify_path('Splits', SPLIT_DIR)\n",
    "\n",
    "if not all_ok:\n",
    "    print('\\n⚠️  Please update paths above!')\n",
    "\n",
    "# ============================================\n",
    "# CONFIG\n",
    "# ============================================\n",
    "RUN_TRAINING = True\n",
    "HF_REPO_ID = 'DanielQ07/transtr-causalvid-weights'\n",
    "HF_MODEL_FILENAME = 'best_model.ckpt'\n",
    "\n",
    "class Config:\n",
    "    # Paths from Kaggle input\n",
    "    video_feature_root = VIT_FEATURE_PATH   # video_id.pt files directly\n",
    "    object_feature_path = OBJ_FEATURE_PATH  # features_node_X/video.pkl\n",
    "    sample_list_path = ANNOTATION_PATH      # video_id/text.json, answer.json\n",
    "    split_dir_txt = SPLIT_DIR               # train.pkl, valid.pkl, test.pkl\n",
    "    \n",
    "    # Model architecture (paper config)\n",
    "    topK_frame = 16\n",
    "    objs = 20\n",
    "    frames = 16\n",
    "    select_frames = 5\n",
    "    topK_obj = 12\n",
    "    frame_feat_dim = 1024\n",
    "    obj_feat_dim = 2053\n",
    "    d_model = 768\n",
    "    word_dim = 768\n",
    "    nheads = 8\n",
    "    num_encoder_layers = 2\n",
    "    num_decoder_layers = 2\n",
    "    normalize_before = True\n",
    "    activation = 'gelu'\n",
    "    dropout = 0.3\n",
    "    encoder_dropout = 0.3\n",
    "    \n",
    "    # Text encoder\n",
    "    text_encoder_type = 'microsoft/deberta-base'\n",
    "    freeze_text_encoder = False\n",
    "    text_encoder_lr = 1e-5\n",
    "    text_pool_mode = 1\n",
    "    \n",
    "    # Training\n",
    "    bs = 8\n",
    "    lr = 1e-5\n",
    "    epoch = 20\n",
    "    gpu = 0\n",
    "    patience = 5\n",
    "    gamma = 0.1\n",
    "    decay = 1e-4\n",
    "    n_query = 5\n",
    "    \n",
    "    # Other\n",
    "    hard_eval = False\n",
    "    pos_ratio = 1.0\n",
    "    neg_ratio = 1.0\n",
    "    a = 1.0\n",
    "    use_amp = True\n",
    "    num_workers = 4\n",
    "\n",
    "args = Config()\n",
    "set_gpu_devices(args.gpu)\n",
    "set_seed(999)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\nDevice: {device}')\n",
    "print('Config loaded!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Create Datasets with Verification\n",
    "print('=== CELL 7: Datasets ===')\n",
    "\n",
    "# Configuration for limiting train samples (set to None for no limit)\n",
    "MAX_TRAIN_SAMPLES = 1000  # Change this to limit training videos, or None for all\n",
    "\n",
    "# Create datasets with detailed logging\n",
    "print('\\n--- Creating TRAIN dataset ---')\n",
    "train_ds = VideoQADataset(\n",
    "    split='train', \n",
    "    n_query=args.n_query, \n",
    "    obj_num=args.objs, \n",
    "    sample_list_path=args.sample_list_path, \n",
    "    video_feature_path=args.video_feature_root, \n",
    "    object_feature_path=args.object_feature_path, \n",
    "    split_dir=args.split_dir_txt, \n",
    "    topK_frame=args.topK_frame,\n",
    "    max_samples=MAX_TRAIN_SAMPLES,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print('\\n--- Creating VAL dataset ---')\n",
    "val_ds = VideoQADataset(\n",
    "    split='val', \n",
    "    n_query=args.n_query, \n",
    "    obj_num=args.objs, \n",
    "    sample_list_path=args.sample_list_path, \n",
    "    video_feature_path=args.video_feature_root, \n",
    "    object_feature_path=args.object_feature_path, \n",
    "    split_dir=args.split_dir_txt, \n",
    "    topK_frame=args.topK_frame,\n",
    "    max_samples=None,  # Don't limit val/test\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print('\\n--- Creating TEST dataset ---')\n",
    "test_ds = VideoQADataset(\n",
    "    split='test', \n",
    "    n_query=args.n_query, \n",
    "    obj_num=args.objs, \n",
    "    sample_list_path=args.sample_list_path, \n",
    "    video_feature_path=args.video_feature_root, \n",
    "    object_feature_path=args.object_feature_path, \n",
    "    split_dir=args.split_dir_txt, \n",
    "    topK_frame=args.topK_frame,\n",
    "    max_samples=None,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, args.bs, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, args.bs, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, args.bs, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "# Summary\n",
    "print('\\n' + '='*60)\n",
    "print('DATASET SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Train: {len(train_ds)} samples -> {len(train_loader)} batches')\n",
    "print(f'Val:   {len(val_ds)} samples -> {len(val_loader)} batches')\n",
    "print(f'Test:  {len(test_ds)} samples -> {len(test_loader)} batches')\n",
    "print('='*60)\n",
    "\n",
    "# Quick sanity check - load one batch\n",
    "if len(train_ds) > 0:\n",
    "    print('\\nSanity check: Loading first batch...')\n",
    "    try:\n",
    "        ff, of, qns, ans, ans_id, keys = next(iter(train_loader))\n",
    "        print(f'  ViT features: {ff.shape}')  # Expected: [batch, topK_frame, feat_dim]\n",
    "        print(f'  Object features: {of.shape}')  # Expected: [batch, topK_frame, obj_num, 2053]\n",
    "        print(f'  Answer IDs: {ans_id}')\n",
    "        print('Sanity check PASSED!')\n",
    "    except Exception as e:\n",
    "        print(f'  ERROR: {e}')\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Model\n",
    "print('=== CELL 8: Model ===')\n",
    "cfg = {k: v for k, v in Config.__dict__.items() if not k.startswith('_')}\n",
    "cfg['device'] = device\n",
    "cfg['topK_frame'] = args.select_frames\n",
    "model = VideoQAmodel(**cfg)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', factor=args.gamma, patience=args.patience)\n",
    "model.to(device)\n",
    "xe = nn.CrossEntropyLoss()\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=True)\n",
    "save_path = os.path.join(MODEL_DIR, HF_MODEL_FILENAME)\n",
    "print(f'Model: {sum(p.numel() for p in model.parameters())/1e6:.1f}M params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Training\n",
    "print('=== CELL 9: Training ===')\n",
    "best_acc = 0\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    for ep in range(1, args.epoch + 1):\n",
    "        loss, acc = train_epoch(model, optimizer, train_loader, xe, device, scaler)\n",
    "        val_acc = eval_epoch(model, val_loader, device)\n",
    "        scheduler.step(val_acc)\n",
    "        print(f'Ep {ep}: Loss={loss:.4f}, Train={acc:.1f}%, Val={val_acc:.1f}%')\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'  Saved!')\n",
    "    print(f'\\nBest Val: {best_acc:.1f}%')\n",
    "    try:\n",
    "        api = HfApi()\n",
    "        api.create_repo(repo_id=HF_REPO_ID, repo_type='model', exist_ok=True)\n",
    "        api.upload_file(path_or_fileobj=save_path, path_in_repo=HF_MODEL_FILENAME, repo_id=HF_REPO_ID, repo_type='model')\n",
    "        print('Uploaded!')\n",
    "    except Exception as e:\n",
    "        print(f'Upload failed: {e}')\n",
    "else:\n",
    "    print('Skipping training (RUN_TRAINING=False)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Detailed Evaluation Function (with ALL score)\n",
    "print('=== CELL 10: Evaluation Functions ===')\n",
    "\n",
    "def run_detailed_evaluation(model, loader, device, split_name='test', save_file='failure_cases.json'):\n",
    "    # Load weights\n",
    "    loaded = False\n",
    "    if RUN_TRAINING:\n",
    "        if os.path.exists(save_path):\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            print(f'Loaded Locally Trained Model: {save_path}')\n",
    "            loaded = True\n",
    "    else:\n",
    "        try:\n",
    "            print(f'Downloading {HF_MODEL_FILENAME} from {HF_REPO_ID}...')\n",
    "            local_model_path = hf_hub_download(repo_id=HF_REPO_ID, filename=HF_MODEL_FILENAME, local_dir=MODEL_DIR)\n",
    "            model.load_state_dict(torch.load(local_model_path))\n",
    "            print(f'Loaded HF Model: {local_model_path}')\n",
    "            loaded = True\n",
    "        except Exception as e:\n",
    "            print(f'Could not download model: {e}')\n",
    "\n",
    "    model.eval()\n",
    "    vid_results = {}\n",
    "    failures = []\n",
    "    \n",
    "    type_map = {\n",
    "        'descriptive': 'd', 'explanatory': 'e',\n",
    "        'predictive': 'p', 'predictive_reason': 'pr',\n",
    "        'counterfactual': 'c', 'counterfactual_reason': 'cr'\n",
    "    }\n",
    "    \n",
    "    print(f'\\nRunning Detailed Evaluation on {split_name.upper()} Set...')\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            vid_frame_feat, vid_obj_feat, qns_word, ans_word, ans_id, qns_keys = batch\n",
    "            vid_frame_feat = vid_frame_feat.to(device)\n",
    "            vid_obj_feat = vid_obj_feat.to(device)\n",
    "            out = model(vid_frame_feat, vid_obj_feat, qns_word, ans_word)\n",
    "            preds = out.argmax(dim=-1).cpu().numpy()\n",
    "            targets = ans_id.numpy()\n",
    "            \n",
    "            for i, qkey in enumerate(qns_keys):\n",
    "                found_type = None\n",
    "                vid_id = None\n",
    "                for t_str, t_short in type_map.items():\n",
    "                    if qkey.endswith('_' + t_str):\n",
    "                        found_type = t_short\n",
    "                        vid_id = qkey[:-(len(t_str)+1)]\n",
    "                        break\n",
    "                if not found_type:\n",
    "                    continue\n",
    "                if vid_id not in vid_results:\n",
    "                    vid_results[vid_id] = {}\n",
    "                \n",
    "                is_correct = (preds[i] == targets[i])\n",
    "                vid_results[vid_id][found_type] = {'correct': is_correct}\n",
    "                \n",
    "                if not is_correct:\n",
    "                    failures.append({\n",
    "                        'video_id': vid_id,\n",
    "                        'type': found_type,\n",
    "                        'question': qns_word[i],\n",
    "                        'pred': int(preds[i]),\n",
    "                        'ground_truth': int(targets[i])\n",
    "                    })\n",
    "\n",
    "    # Calculate stats (including ALL)\n",
    "    stats = {k: {'correct': 0, 'total': 0} for k in ['d', 'e', 'p', 'pr', 'c', 'cr', 'par', 'car', 'all']}\n",
    "    for vid, res in vid_results.items():\n",
    "        for t in ['d', 'e', 'p', 'pr', 'c', 'cr']:\n",
    "            if t in res:\n",
    "                stats[t]['total'] += 1\n",
    "                stats['all']['total'] += 1\n",
    "                if res[t]['correct']:\n",
    "                    stats[t]['correct'] += 1\n",
    "                    stats['all']['correct'] += 1\n",
    "        # Combined PAR\n",
    "        if 'p' in res and 'pr' in res:\n",
    "            stats['par']['total'] += 1\n",
    "            if res['p']['correct'] and res['pr']['correct']:\n",
    "                stats['par']['correct'] += 1\n",
    "        # Combined CAR\n",
    "        if 'c' in res and 'cr' in res:\n",
    "            stats['car']['total'] += 1\n",
    "            if res['c']['correct'] and res['cr']['correct']:\n",
    "                stats['car']['correct'] += 1\n",
    "\n",
    "    # Print results with ALL\n",
    "    labels, accs = [], []\n",
    "    print(f\"\\n{'Type':<6} {'Acc %':<10} {'Cor':<6} {'Tot':<6}\")\n",
    "    print('-' * 35)\n",
    "    for k in ['d', 'e', 'p', 'pr', 'par', 'c', 'cr', 'car']:\n",
    "        s = stats[k]\n",
    "        acc = s['correct'] / s['total'] * 100 if s['total'] > 0 else 0\n",
    "        print(f\"{k.upper():<6} {acc:<10.2f} {s['correct']:<6} {s['total']:<6}\")\n",
    "        if s['total'] > 0:\n",
    "            labels.append(k.upper())\n",
    "            accs.append(acc)\n",
    "    print('-' * 35)\n",
    "    # Print ALL score\n",
    "    all_s = stats['all']\n",
    "    all_acc = all_s['correct'] / all_s['total'] * 100 if all_s['total'] > 0 else 0\n",
    "    print(f\"{'ALL':<6} {all_acc:<10.2f} {all_s['correct']:<6} {all_s['total']:<6}\")\n",
    "    print('=' * 35)\n",
    "    labels.append('ALL')\n",
    "    accs.append(all_acc)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    colors = ['steelblue'] * (len(labels) - 1) + ['darkgreen']\n",
    "    bars = plt.bar(labels, accs, color=colors)\n",
    "    plt.ylim(0, 105)\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title(f'Performance by Question Type ({split_name.upper()} Set)')\n",
    "    for bar in bars:\n",
    "        y = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, y + 1, f'{y:.1f}', ha='center', va='bottom')\n",
    "    plt.axhline(y=20, color='gray', linestyle='--', alpha=0.5, label='Random (20%)')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{split_name}_results.png', dpi=150)\n",
    "    print(f'\\nSaved: {split_name}_results.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Save failures\n",
    "    failure_file = f'{split_name}_{save_file}'\n",
    "    with open(failure_file, 'w') as f:\n",
    "        json.dump(failures, f, indent=4)\n",
    "    print(f'Saved {len(failures)} failure cases to {failure_file}')\n",
    "    \n",
    "    return stats, failures\n",
    "\n",
    "print('Evaluation function defined (with ALL score)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Evaluate on TEST set\n",
    "print('=== CELL 11: TEST Set Evaluation ===')\n",
    "test_stats, test_failures = run_detailed_evaluation(model, test_loader, device, split_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12: Evaluate on VALIDATION set\n",
    "print('=== CELL 12: VALIDATION Set Evaluation ===')\n",
    "val_stats, val_failures = run_detailed_evaluation(model, val_loader, device, split_name='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13: Summary CSV\n",
    "print('=== CELL 13: Summary ===')\n",
    "\n",
    "type_keys = ['d', 'e', 'p', 'pr', 'par', 'c', 'cr', 'car', 'all']\n",
    "type_names = ['D', 'E', 'P', 'PR', 'PAR', 'C', 'CR', 'CAR', 'ALL']\n",
    "\n",
    "summary_data = []\n",
    "for k, name in zip(type_keys, type_names):\n",
    "    v = val_stats[k]\n",
    "    t = test_stats[k]\n",
    "    summary_data.append({\n",
    "        'Type': name,\n",
    "        'Val_Correct': v['correct'],\n",
    "        'Val_Total': v['total'],\n",
    "        'Val_Acc%': round(v['correct']/v['total']*100, 2) if v['total'] > 0 else 0,\n",
    "        'Test_Correct': t['correct'],\n",
    "        'Test_Total': t['total'],\n",
    "        'Test_Acc%': round(t['correct']/t['total']*100, 2) if t['total'] > 0 else 0\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "df.to_csv('evaluation_summary.csv', index=False)\n",
    "print('Saved: evaluation_summary.csv\\n')\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
