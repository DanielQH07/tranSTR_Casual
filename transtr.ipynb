{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-09T15:30:10.607668Z",
     "iopub.status.busy": "2025-12-09T15:30:10.607030Z",
     "iopub.status.idle": "2025-12-09T15:30:11.022002Z",
     "shell.execute_reply": "2025-12-09T15:30:11.020873Z",
     "shell.execute_reply.started": "2025-12-09T15:30:10.607641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/DanielQH07/tranSTR_Casual.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:30:11.024634Z",
     "iopub.status.busy": "2025-12-09T15:30:11.024356Z",
     "iopub.status.idle": "2025-12-09T15:30:11.030862Z",
     "shell.execute_reply": "2025-12-09T15:30:11.030126Z",
     "shell.execute_reply.started": "2025-12-09T15:30:11.024608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/tranSTR_Casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:30:11.032090Z",
     "iopub.status.busy": "2025-12-09T15:30:11.031822Z",
     "iopub.status.idle": "2025-12-09T15:30:11.048078Z",
     "shell.execute_reply": "2025-12-09T15:30:11.047374Z",
     "shell.execute_reply.started": "2025-12-09T15:30:11.032071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFY PATCHED FILES (No need to re-write - already patched)\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç VERIFYING PATCHED FILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "checks = []\n",
    "\n",
    "# Check 1: attention.py patch\n",
    "attention_file = 'networks/attention.py'\n",
    "if os.path.exists(attention_file):\n",
    "    with open(attention_file, 'r') as f:\n",
    "        content = f.read()\n",
    "    has_fix = 'DataParallel-safe' in content or 'new_mask' in content\n",
    "    status = \"‚úÖ PATCHED\" if has_fix else \"‚ùå NOT PATCHED\"\n",
    "    checks.append(has_fix)\n",
    "    print(f\"  {status}: {attention_file}\")\n",
    "else:\n",
    "    print(f\"  ‚ùå MISSING: {attention_file}\")\n",
    "    checks.append(False)\n",
    "\n",
    "# Check 2: model.py patch\n",
    "model_file = 'networks/model.py'\n",
    "if os.path.exists(model_file):\n",
    "    with open(model_file, 'r') as f:\n",
    "        content = f.read()\n",
    "    has_fix = 'repeat_interleave' in content\n",
    "    status = \"‚úÖ PATCHED\" if has_fix else \"‚ùå NOT PATCHED\"\n",
    "    checks.append(has_fix)\n",
    "    print(f\"  {status}: {model_file}\")\n",
    "else:\n",
    "    print(f\"  ‚ùå MISSING: {model_file}\")\n",
    "    checks.append(False)\n",
    "\n",
    "# Check 3: DataLoader.py patch\n",
    "dataloader_file = 'DataLoader.py'\n",
    "if os.path.exists(dataloader_file):\n",
    "    with open(dataloader_file, 'r') as f:\n",
    "        content = f.read()\n",
    "    has_fix = 'dummy_bbox' in content or 'Handle different feature shapes' in content\n",
    "    status = \"‚úÖ PATCHED\" if has_fix else \"‚ùå NOT PATCHED\"\n",
    "    checks.append(has_fix)\n",
    "    print(f\"  {status}: {dataloader_file}\")\n",
    "else:\n",
    "    print(f\"  ‚ùå MISSING: {dataloader_file}\")\n",
    "    checks.append(False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "if all(checks):\n",
    "    print(\"‚úÖ ALL FILES VERIFIED! Ready for training.\")\n",
    "else:\n",
    "    print(\"‚ùå SOME FILES ARE MISSING OR NOT PATCHED!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:30:11.089002Z",
     "iopub.status.busy": "2025-12-09T15:30:11.088754Z",
     "iopub.status.idle": "2025-12-09T15:30:11.969337Z",
     "shell.execute_reply": "2025-12-09T15:30:11.968443Z",
     "shell.execute_reply.started": "2025-12-09T15:30:11.088974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "# ============================================================\n",
    "# DATA PATHS (Kaggle Input)\n",
    "# ============================================================\n",
    "text_feature_path = '/kaggle/input/text-feature'\n",
    "visual_feature_path = '/kaggle/input/visual-feature'\n",
    "split_path = '/kaggle/input/casual-vid-data-split/split'\n",
    "text_annotation_path = '/kaggle/input/text-annotation'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìÇ DATA PATHS\")\n",
    "print(\"=\" * 70)\n",
    "for name, path in [(\"Visual features\", visual_feature_path), \n",
    "                   (\"Split files\", split_path), \n",
    "                   (\"Text annotations\", text_annotation_path)]:\n",
    "    status = \"‚úì\" if os.path.exists(path) else \"‚úó\"\n",
    "    print(f\"  {status} {name}: {path}\")\n",
    "\n",
    "# ============================================================\n",
    "# DATA STATISTICS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä DATASET STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Split files\n",
    "print(\"\\nüìÅ Split Files:\")\n",
    "split_stats = {}\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    split_file = f'{split_path}/{split_name}.pkl'\n",
    "    if os.path.exists(split_file):\n",
    "        with open(split_file, 'rb') as f:\n",
    "            vids = pickle.load(f)\n",
    "        split_stats[split_name] = len(vids)\n",
    "        samples = len(vids) * 6  # 6 question types per video\n",
    "        print(f\"  {split_name:>6}: {len(vids):>6} videos ‚Üí {samples:>6} samples\")\n",
    "\n",
    "# 2. Visual features\n",
    "print(\"\\nüé¨ Visual Features:\")\n",
    "idx2vid_file = f'{visual_feature_path}/idx2vid.pkl'\n",
    "if os.path.exists(idx2vid_file):\n",
    "    with open(idx2vid_file, 'rb') as f:\n",
    "        idx2vid = pickle.load(f)\n",
    "    print(f\"  Indexed videos: {len(idx2vid)}\")\n",
    "\n",
    "for feat_name in ['appearance_feat.h5', 'motion_feat.h5']:\n",
    "    feat_file = f'{visual_feature_path}/{feat_name}'\n",
    "    if os.path.exists(feat_file):\n",
    "        with h5py.File(feat_file, 'r') as f:\n",
    "            shape = f['resnet_features'].shape\n",
    "        print(f\"  {feat_name}: {shape}\")\n",
    "\n",
    "# 3. Question types\n",
    "print(\"\\n‚ùì Question Types (qtype):\")\n",
    "qtype_info = [\n",
    "    (\"0\", \"Descriptive\", \"What is happening?\"),\n",
    "    (\"1\", \"Explanatory\", \"Why did it happen?\"),\n",
    "    (\"2\", \"Predictive-Ans\", \"What will happen?\"),\n",
    "    (\"3\", \"Predictive-Reason\", \"Why will it happen?\"),\n",
    "    (\"4\", \"Counterfactual-Ans\", \"What if X didn't happen?\"),\n",
    "    (\"5\", \"Counterfactual-Reason\", \"Why would that result?\"),\n",
    "]\n",
    "for qt, name, desc in qtype_info:\n",
    "    print(f\"  {qt}: {name:<20} - {desc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:30:11.984319Z",
     "iopub.status.busy": "2025-12-09T15:30:11.984025Z",
     "iopub.status.idle": "2025-12-09T15:30:16.633264Z",
     "shell.execute_reply": "2025-12-09T15:30:16.632462Z",
     "shell.execute_reply.started": "2025-12-09T15:30:11.984300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers einops h5py wandb\n",
    "\n",
    "# Login to W&B (uncomment v√† th√™m API key c·ªßa b·∫°n)\n",
    "my_key = \"80b5a02ccaed80f35a2e893aed6446d4467c0c45\"\n",
    "import wandb\n",
    "wandb.login(key=my_key, relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:30:16.635027Z",
     "iopub.status.busy": "2025-12-09T15:30:16.634691Z",
     "iopub.status.idle": "2025-12-09T15:30:16.650199Z",
     "shell.execute_reply": "2025-12-09T15:30:16.649418Z",
     "shell.execute_reply.started": "2025-12-09T15:30:16.634992Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Training configuration for CausalVidQA\"\"\"\n",
    "    \n",
    "    # Experiment\n",
    "    project_name = \"CausalVidQA-TranSTR\"\n",
    "    run_name = \"causalvid_1gpu\"\n",
    "    \n",
    "    # Data paths\n",
    "    sample_list_path = split_path\n",
    "    video_feature_path = visual_feature_path\n",
    "    text_annotation_path = text_annotation_path\n",
    "    \n",
    "    # Training\n",
    "    bs = 2                     # Batch size per step\n",
    "    lr = 1e-4                  # Learning rate\n",
    "    text_encoder_lr = 1e-5     # Text encoder LR (lower)\n",
    "    epoch = 20\n",
    "    warmup_epochs = 2          # Warmup epochs\n",
    "    \n",
    "    # üî• Gradient Accumulation (simulate 2 GPUs on 1 GPU)\n",
    "    ACCUM_STEPS = 2            # Effective batch size = bs * ACCUM_STEPS\n",
    "    \n",
    "    # Dataset\n",
    "    dataset = 'causal-vid'\n",
    "    qtype = 3                  # -1 = all question types\n",
    "    max_samples = 2000         # None = use all data\n",
    "    \n",
    "    # Model architecture\n",
    "    d_model = 768\n",
    "    word_dim = 768\n",
    "    nheads = 8\n",
    "    num_encoder_layers = 1\n",
    "    num_decoder_layers = 1\n",
    "    dropout = 0.1\n",
    "    encoder_dropout = 0.1\n",
    "    activation = 'relu'\n",
    "    normalize_before = False\n",
    "    \n",
    "    # Video features\n",
    "    objs = 20                  # Objects per frame\n",
    "    topK_frame = 8             # Top-K frames to select\n",
    "    topK_obj = 5               # Top-K objects to select\n",
    "    frame_feat_dim = 4096      # app(2048) + mot(2048)\n",
    "    obj_feat_dim = 2053        # feat(2048) + bbox(5)\n",
    "    n_query = 5                # 5-way multiple choice\n",
    "    \n",
    "    # Text encoder\n",
    "    text_encoder_type = \"microsoft/deberta-base\"\n",
    "    freeze_text_encoder = False\n",
    "    text_pool_mode = 0\n",
    "    hard_eval = False\n",
    "    \n",
    "    # Optimizer\n",
    "    decay = 0.001              # Weight decay\n",
    "    patience = 3               # LR scheduler patience\n",
    "    gamma = 0.5                # LR decay factor\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_patience = 5  # Stop after 5 epochs without improvement\n",
    "    \n",
    "    # Contrastive learning\n",
    "    pos_ratio = 0.7\n",
    "    neg_ratio = 0.3\n",
    "    a = 1\n",
    "    \n",
    "    # Single GPU mode (no DataParallel)\n",
    "    use_multi_gpu = False      # Disabled - using gradient accumulation instead\n",
    "    num_workers = 0            # DataLoader workers\n",
    "    \n",
    "    # Logging\n",
    "    log_interval = 50          # Log every N batches\n",
    "    save_every = 5             # Save checkpoint every N epochs\n",
    "\n",
    "args = Config()\n",
    "\n",
    "# ============================================================\n",
    "# GPU SETUP (Single GPU)\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üñ•Ô∏è GPU CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"  Available GPUs: {n_gpus}\")\n",
    "for i in range(n_gpus):\n",
    "    print(f\"    GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    mem = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "    print(f\"           Memory: {mem:.1f} GB\")\n",
    "\n",
    "print(f\"\\n  ‚úì Single GPU mode with Gradient Accumulation\")\n",
    "print(f\"  ‚úì Accumulation steps: {args.ACCUM_STEPS}\")\n",
    "print(f\"  ‚úì Effective batch size: {args.bs * args.ACCUM_STEPS}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"  Primary device: {device}\")\n",
    "\n",
    "# ============================================================\n",
    "# PRINT CONFIG\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚öôÔ∏è TRAINING CONFIG\")\n",
    "print(\"=\" * 70)\n",
    "config_items = [\n",
    "    (\"Batch size\", args.bs),\n",
    "    (\"Accumulation steps\", args.ACCUM_STEPS),\n",
    "    (\"Effective batch size\", args.bs * args.ACCUM_STEPS),\n",
    "    (\"Learning rate\", args.lr),\n",
    "    (\"Text encoder LR\", args.text_encoder_lr),\n",
    "    (\"Epochs\", args.epoch),\n",
    "    (\"Early stopping\", f\"{args.early_stopping_patience} epochs\"),\n",
    "    (\"d_model\", args.d_model),\n",
    "    (\"TopK frames\", args.topK_frame),\n",
    "    (\"TopK objects\", args.topK_obj),\n",
    "    (\"Objects/frame\", args.objs),\n",
    "    (\"Text encoder\", args.text_encoder_type),\n",
    "]\n",
    "\n",
    "for name, val in config_items:\n",
    "    print(f\"  {name:<20}: {val}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:30:16.651409Z",
     "iopub.status.busy": "2025-12-09T15:30:16.651138Z",
     "iopub.status.idle": "2025-12-09T15:30:16.676176Z",
     "shell.execute_reply": "2025-12-09T15:30:16.675477Z",
     "shell.execute_reply.started": "2025-12-09T15:30:16.651391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Local imports\n",
    "from DataLoader import VideoQADataset\n",
    "from networks.model import VideoQAmodel\n",
    "import eval_mc\n",
    "\n",
    "# ============================================================\n",
    "# REPRODUCIBILITY\n",
    "# ============================================================\n",
    "def set_seed(seed=999):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(999)\n",
    "print(\"‚úÖ Modules imported, seed set to 999\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:30:16.677300Z",
     "iopub.status.busy": "2025-12-09T15:30:16.677037Z",
     "iopub.status.idle": "2025-12-09T15:33:37.715491Z",
     "shell.execute_reply": "2025-12-09T15:33:37.714667Z",
     "shell.execute_reply.started": "2025-12-09T15:30:16.677274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Creating datasets...\")\n",
    "\n",
    "# ============================================================\n",
    "# CREATE DATASETS\n",
    "# ============================================================\n",
    "dataset_kwargs = dict(\n",
    "    n_query=args.n_query,\n",
    "    obj_num=args.objs,\n",
    "    sample_list_path=args.sample_list_path,\n",
    "    video_feature_path=args.video_feature_path,\n",
    "    text_annotation_path=args.text_annotation_path,\n",
    "    qtype=args.qtype,\n",
    "    max_samples=args.max_samples\n",
    ")\n",
    "\n",
    "train_dataset = VideoQADataset(split='train', **dataset_kwargs)\n",
    "val_dataset = VideoQADataset(split='val', **dataset_kwargs)\n",
    "\n",
    "# Test set LU√îN d√πng to√†n b·ªô data (kh√¥ng gi·ªõi h·∫°n max_samples)\n",
    "test_kwargs = dataset_kwargs.copy()\n",
    "test_kwargs['max_samples'] = None  # Force full test set\n",
    "test_dataset = VideoQADataset(split='test', **test_kwargs)\n",
    "\n",
    "# ============================================================\n",
    "# CREATE DATALOADERS (optimized for multi-GPU)\n",
    "# ============================================================\n",
    "loader_kwargs = dict(\n",
    "    batch_size=args.bs,\n",
    "    num_workers=args.num_workers if args.use_multi_gpu else 0,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2 if args.num_workers > 0 else None,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True,drop_last=True, **loader_kwargs)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, **loader_kwargs)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, **loader_kwargs)\n",
    "\n",
    "# ============================================================\n",
    "# DATASET SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä DATALOADER SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  {'Split':<10} {'Videos':>10} {'Samples':>10} {'Batches':>10}\")\n",
    "print(f\"  {'-'*10} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "for name, dataset, loader in [\n",
    "    (\"Train\", train_dataset, train_loader),\n",
    "    (\"Val\", val_dataset, val_loader),\n",
    "    (\"Test (FULL)\", test_dataset, test_loader)\n",
    "]:\n",
    "    n_vids = len(dataset.vids) if hasattr(dataset, 'vids') else \"?\"\n",
    "    print(f\"  {name:<10} {n_vids:>10} {len(dataset):>10} {len(loader):>10}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  ‚ÑπÔ∏è  Test set always uses ALL data regardless of max_samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:33:37.716788Z",
     "iopub.status.busy": "2025-12-09T15:33:37.716505Z",
     "iopub.status.idle": "2025-12-09T15:33:37.753008Z",
     "shell.execute_reply": "2025-12-09T15:33:37.752153Z",
     "shell.execute_reply.started": "2025-12-09T15:33:37.716767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFY DATA SAMPLE\n",
    "# ============================================================\n",
    "print(\"üîç Verifying data sample...\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    vid_frame_feat, vid_obj_feat, qns_word, ans_word, ans_id, qns_key = batch\n",
    "    \n",
    "    print(f\"\\n  Frame features:  {vid_frame_feat.shape}\")\n",
    "    print(f\"  Object features: {vid_obj_feat.shape}\")\n",
    "    print(f\"  Batch size:      {len(qns_word)}\")\n",
    "    print(f\"\\n  Sample question: {qns_word[0][:80]}...\")\n",
    "    print(f\"  Sample answer:   {ans_word[0][0][:60]}...\")\n",
    "    print(f\"  Ground truth:    {ans_id[0].item()}\")\n",
    "    print(f\"  Question key:    {qns_key[0]}\")\n",
    "    break\n",
    "\n",
    "print(\"\\n‚úÖ Data verification complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:33:37.754346Z",
     "iopub.status.busy": "2025-12-09T15:33:37.754073Z",
     "iopub.status.idle": "2025-12-09T15:33:39.270087Z",
     "shell.execute_reply": "2025-12-09T15:33:39.269266Z",
     "shell.execute_reply.started": "2025-12-09T15:33:37.754326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE MODEL (Single GPU - No DataParallel)\n",
    "# ============================================================\n",
    "print(\"üèóÔ∏è Creating model...\")\n",
    "\n",
    "model_config = {\n",
    "    'd_model': args.d_model,\n",
    "    'word_dim': args.word_dim,\n",
    "    'encoder_dropout': args.encoder_dropout,\n",
    "    'dropout': args.dropout,\n",
    "    'num_encoder_layers': args.num_encoder_layers,\n",
    "    'num_decoder_layers': args.num_decoder_layers,\n",
    "    'nheads': args.nheads,\n",
    "    'normalize_before': args.normalize_before,\n",
    "    'activation': args.activation,\n",
    "    'text_encoder_type': args.text_encoder_type,\n",
    "    'freeze_text_encoder': args.freeze_text_encoder,\n",
    "    'text_pool_mode': args.text_pool_mode,\n",
    "    'n_query': args.n_query,\n",
    "    'objs': args.objs,\n",
    "    'topK_frame': args.topK_frame,\n",
    "    'topK_obj': args.topK_obj,\n",
    "    'hard_eval': args.hard_eval,\n",
    "    'frame_feat_dim': args.frame_feat_dim,\n",
    "    'obj_feat_dim': args.obj_feat_dim,\n",
    "    'device': device,\n",
    "}\n",
    "\n",
    "model = VideoQAmodel(**model_config)\n",
    "\n",
    "# Single GPU - no DataParallel wrapper needed\n",
    "model.to(device)\n",
    "\n",
    "# ============================================================\n",
    "# MODEL SUMMARY\n",
    "# ============================================================\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üß† MODEL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Total parameters:     {total_params / 1e6:.2f}M\")\n",
    "print(f\"  Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
    "print(f\"  Device:               {device}\")\n",
    "print(f\"  Gradient Accumulation: {args.ACCUM_STEPS} steps\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:33:39.273389Z",
     "iopub.status.busy": "2025-12-09T15:33:39.273120Z",
     "iopub.status.idle": "2025-12-09T15:33:39.294891Z",
     "shell.execute_reply": "2025-12-09T15:33:39.294002Z",
     "shell.execute_reply.started": "2025-12-09T15:33:39.273370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING FUNCTIONS WITH GRADIENT ACCUMULATION\n",
    "# ============================================================\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader, criterion, device, epoch,\n",
    "                wandb_run=None, accum_steps=2):\n",
    "    \"\"\"Train for one epoch with gradient accumulation and wrong sample tracking\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    answers = []\n",
    "    batch_times = []\n",
    "    \n",
    "    # üî¥ Track wrong predictions\n",
    "    wrong_samples = []\n",
    "    \n",
    "    # Per question type tracking\n",
    "    qtype_correct = defaultdict(int)\n",
    "    qtype_total = defaultdict(int)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, inputs in enumerate(train_loader):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "        vid_frame_feat = vid_frame_feat.to(device)\n",
    "        vid_obj_feat = vid_obj_feat.to(device)\n",
    "        ans_targets = ans_id.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "        loss = criterion(out, ans_targets)\n",
    "        \n",
    "        # üî• Gradient Accumulation: scale loss\n",
    "        loss = loss / accum_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Step optimizer every accum_steps batches\n",
    "        if (batch_idx + 1) % accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Track metrics (use unscaled loss for logging)\n",
    "        total_loss += loss.item() * accum_steps\n",
    "        \n",
    "        # Predictions\n",
    "        pred = torch.argmax(out, dim=1).detach().cpu()\n",
    "        ans = ans_id.detach().cpu()\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        answers.append(ans)\n",
    "        \n",
    "        # Track per question type accuracy\n",
    "        for qkey, p, a in zip(qns_keys, pred.numpy(), ans.numpy()):\n",
    "            qtype = int(qkey.split('_')[-1])\n",
    "            qtype_total[qtype] += 1\n",
    "            if p == a:\n",
    "                qtype_correct[qtype] += 1\n",
    "        \n",
    "        # üî¥ Track wrong predictions\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i].item() != ans[i].item():\n",
    "                wrong_samples.append({\n",
    "                    \"qid\": qns_keys[i],\n",
    "                    \"prediction\": int(pred[i]),\n",
    "                    \"answer\": int(ans[i]),\n",
    "                    \"question\": qns_w[i] if isinstance(qns_w, list) else qns_w,\n",
    "                    \"epoch\": epoch\n",
    "                })\n",
    "        \n",
    "        batch_times.append(time.time() - batch_start)\n",
    "        \n",
    "        # Logging\n",
    "        if (batch_idx + 1) % args.log_interval == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            avg_time = np.mean(batch_times[-args.log_interval:])\n",
    "            print(f\"    Batch [{batch_idx+1:>4}/{len(train_loader)}] \"\n",
    "                  f\"Loss: {loss.item() * accum_steps:.4f} (avg: {avg_loss:.4f}) \"\n",
    "                  f\"Time: {avg_time:.3f}s/batch\")\n",
    "            \n",
    "            if wandb_run:\n",
    "                wandb_run.log({\n",
    "                    \"train/batch_loss\": loss.item() * accum_steps,\n",
    "                    \"train/avg_loss\": avg_loss,\n",
    "                    \"train/batch_time\": avg_time,\n",
    "                }, step=epoch * len(train_loader) + batch_idx)\n",
    "    \n",
    "    # Handle remaining gradients if not divisible by accum_steps\n",
    "    if (batch_idx + 1) % accum_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    all_preds = torch.cat(predictions, dim=0).long()\n",
    "    all_ans = torch.cat(answers, dim=0).long()\n",
    "    epoch_acc = (all_preds == all_ans).sum().item() * 100.0 / len(all_ans)\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Per question type accuracy\n",
    "    qtype_acc = {}\n",
    "    qtype_names = ['Des', 'Exp', 'Pred-A', 'Pred-R', 'CF-A', 'CF-R']\n",
    "    for qt in range(6):\n",
    "        if qtype_total[qt] > 0:\n",
    "            qtype_acc[qtype_names[qt]] = qtype_correct[qt] * 100.0 / qtype_total[qt]\n",
    "    \n",
    "    return {\n",
    "        'loss': epoch_loss,\n",
    "        'acc': epoch_acc,\n",
    "        'time': epoch_time,\n",
    "        'qtype_acc': qtype_acc,\n",
    "        'wrong_samples': wrong_samples  # üî¥ Return wrong samples\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device, split_name='val'):\n",
    "    \"\"\"Evaluate with detailed per-type accuracy and wrong sample tracking\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    answers = []\n",
    "    qtype_correct = defaultdict(int)\n",
    "    qtype_total = defaultdict(int)\n",
    "    \n",
    "    # üî¥ Track wrong predictions\n",
    "    wrong_samples = []\n",
    "    all_questions = []\n",
    "    all_qns_keys = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "            vid_frame_feat = vid_frame_feat.to(device)\n",
    "            vid_obj_feat = vid_obj_feat.to(device)\n",
    "            \n",
    "            out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "            pred = out.max(-1)[1].cpu()\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            answers.append(ans_id)\n",
    "            \n",
    "            for qkey, p, a, q in zip(qns_keys, pred.numpy(), ans_id.numpy(), qns_w):\n",
    "                qtype = int(qkey.split('_')[-1])\n",
    "                qtype_total[qtype] += 1\n",
    "                if p == a:\n",
    "                    qtype_correct[qtype] += 1\n",
    "                else:\n",
    "                    # üî¥ Track wrong predictions\n",
    "                    wrong_samples.append({\n",
    "                        \"qid\": qkey,\n",
    "                        \"video_id\": qkey.rsplit('_', 1)[0],\n",
    "                        \"qtype\": qtype,\n",
    "                        \"question\": q,\n",
    "                        \"prediction\": int(p),\n",
    "                        \"answer\": int(a),\n",
    "                    })\n",
    "    \n",
    "    all_preds = torch.cat(predictions, dim=0).long()\n",
    "    all_ans = torch.cat(answers, dim=0).long()\n",
    "    overall_acc = (all_preds == all_ans).sum().item() * 100.0 / len(all_ans)\n",
    "    \n",
    "    # Per question type accuracy\n",
    "    qtype_names = ['Des', 'Exp', 'Pred-A', 'Pred-R', 'CF-A', 'CF-R']\n",
    "    qtype_acc = {}\n",
    "    for qt in range(6):\n",
    "        if qtype_total[qt] > 0:\n",
    "            qtype_acc[qtype_names[qt]] = qtype_correct[qt] * 100.0 / qtype_total[qt]\n",
    "    \n",
    "    return {\n",
    "        'acc': overall_acc,\n",
    "        'qtype_acc': qtype_acc,\n",
    "        'n_samples': len(all_ans),\n",
    "        'wrong_samples': wrong_samples  # üî¥ Return wrong samples\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_and_save(model, data_loader, device, save_path):\n",
    "    \"\"\"Generate predictions and save to JSON with wrong sample tracking\"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    wrong_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            vid_frame_feat, vid_obj_feat, qns_w, ans_w, ans_id, qns_keys = inputs\n",
    "            vid_frame_feat = vid_frame_feat.to(device)\n",
    "            vid_obj_feat = vid_obj_feat.to(device)\n",
    "            \n",
    "            out = model(vid_frame_feat, vid_obj_feat, qns_w, ans_w)\n",
    "            pred = out.max(-1)[1].cpu()\n",
    "            \n",
    "            for qid, p, a, q in zip(qns_keys, pred.numpy(), ans_id.numpy(), qns_w):\n",
    "                results[qid] = {'prediction': int(p), 'answer': int(a)}\n",
    "                \n",
    "                if int(p) != int(a):\n",
    "                    wrong_samples.append({\n",
    "                        \"qid\": qid,\n",
    "                        \"video_id\": qid.rsplit('_', 1)[0],\n",
    "                        \"qtype\": int(qid.split('_')[-1]),\n",
    "                        \"question\": q,\n",
    "                        \"prediction\": int(p),\n",
    "                        \"answer\": int(a),\n",
    "                    })\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    correct = sum(1 for v in results.values() if v['prediction'] == v['answer'])\n",
    "    acc = correct * 100.0 / len(results)\n",
    "    \n",
    "    return results, acc, wrong_samples\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training functions defined with gradient accumulation and wrong sample tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:33:39.295997Z",
     "iopub.status.busy": "2025-12-09T15:33:39.295713Z",
     "iopub.status.idle": "2025-12-09T15:33:39.335673Z",
     "shell.execute_reply": "2025-12-09T15:33:39.334703Z",
     "shell.execute_reply.started": "2025-12-09T15:33:39.295977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP OPTIMIZER, SCHEDULER, CRITERION\n",
    "# ============================================================\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "os.makedirs('./prediction', exist_ok=True)\n",
    "\n",
    "# Optimizer with different LR for text encoder\n",
    "param_groups = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() \n",
    "                   if \"text_encoder\" not in n and p.requires_grad],\n",
    "        \"lr\": args.lr\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() \n",
    "                   if \"text_encoder\" in n and p.requires_grad],\n",
    "        \"lr\": args.text_encoder_lr\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, weight_decay=args.decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=args.gamma, \n",
    "                               patience=args.patience, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"‚úÖ Optimizer and scheduler created\")\n",
    "print(f\"   Main LR: {args.lr}\")\n",
    "print(f\"   Text encoder LR: {args.text_encoder_lr}\")\n",
    "print(f\"   Gradient Accumulation: {args.ACCUM_STEPS} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:33:39.336997Z",
     "iopub.status.busy": "2025-12-09T15:33:39.336703Z",
     "iopub.status.idle": "2025-12-09T15:33:46.877869Z",
     "shell.execute_reply": "2025-12-09T15:33:46.877187Z",
     "shell.execute_reply.started": "2025-12-09T15:33:39.336971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INITIALIZE WANDB\n",
    "# ============================================================\n",
    "wandb_config = {\n",
    "    \"architecture\": \"TranSTR\",\n",
    "    \"dataset\": \"CausalVidQA\",\n",
    "    \"epochs\": args.epoch,\n",
    "    \"batch_size\": args.bs,\n",
    "    \"learning_rate\": args.lr,\n",
    "    \"text_encoder_lr\": args.text_encoder_lr,\n",
    "    \"text_encoder\": args.text_encoder_type,\n",
    "    \"d_model\": args.d_model,\n",
    "    \"topK_frame\": args.topK_frame,\n",
    "    \"topK_obj\": args.topK_obj,\n",
    "    \"n_objects\": args.objs,\n",
    "    \"num_encoder_layers\": args.num_encoder_layers,\n",
    "    \"num_decoder_layers\": args.num_decoder_layers,\n",
    "    \"multi_gpu\": args.use_multi_gpu,\n",
    "    \"n_gpus\": torch.cuda.device_count(),\n",
    "    \"train_samples\": len(train_dataset),\n",
    "    \"val_samples\": len(val_dataset),\n",
    "    \"test_samples\": len(test_dataset),\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=args.project_name,\n",
    "    name=args.run_name,\n",
    "    config=wandb_config,\n",
    "    tags=[\"causalvid\", \"multi-gpu\" if args.use_multi_gpu else \"single-gpu\"]\n",
    ")\n",
    "\n",
    "# Log dataset info\n",
    "wandb.log({\n",
    "    \"data/train_videos\": len(train_dataset.vids) if hasattr(train_dataset, 'vids') else 0,\n",
    "    \"data/val_videos\": len(val_dataset.vids) if hasattr(val_dataset, 'vids') else 0,\n",
    "    \"data/test_videos\": len(test_dataset.vids) if hasattr(test_dataset, 'vids') else 0,\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ W&B initialized: {run.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T15:33:46.878963Z",
     "iopub.status.busy": "2025-12-09T15:33:46.878722Z",
     "iopub.status.idle": "2025-12-09T18:16:19.191549Z",
     "shell.execute_reply": "2025-12-09T18:16:19.190908Z",
     "shell.execute_reply.started": "2025-12-09T15:33:46.878944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP WITH GRADIENT ACCUMULATION & EARLY STOPPING\n",
    "# ============================================================\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = f'./models/best_model-{args.run_name}.ckpt'\n",
    "history = {'train': [], 'val': [], 'test': []}\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# üî¥ Track all wrong samples across epochs\n",
    "all_wrong_samples = {\n",
    "    'train': [],\n",
    "    'val': [],\n",
    "    'test': []\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"üöÄ STARTING TRAINING: {args.run_name}\")\n",
    "print(f\"   Epochs: {args.epoch} | Batch size: {args.bs} | Accum steps: {args.ACCUM_STEPS}\")\n",
    "print(f\"   Effective batch size: {args.bs * args.ACCUM_STEPS}\")\n",
    "print(f\"   Early stopping: {args.early_stopping_patience} epochs without improvement\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(1, args.epoch + 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìö EPOCH [{epoch}/{args.epoch}]\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ============ TRAIN with gradient accumulation ============\n",
    "    train_metrics = train_epoch(\n",
    "        model, optimizer, train_loader, criterion, device, epoch, \n",
    "        wandb_run=run, accum_steps=args.ACCUM_STEPS\n",
    "    )\n",
    "    \n",
    "    # ============ EVALUATE ============\n",
    "    val_metrics = evaluate(model, val_loader, device, 'val')\n",
    "    test_metrics = evaluate(model, test_loader, device, 'test')\n",
    "    \n",
    "    # ============ UPDATE SCHEDULER ============\n",
    "    scheduler.step(val_metrics['acc'])\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # ============ SAVE BEST MODEL & EARLY STOPPING ============\n",
    "    is_best = val_metrics['acc'] > best_val_acc\n",
    "    if is_best:\n",
    "        best_val_acc = val_metrics['acc']\n",
    "        best_epoch = epoch\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        # üî¥ Save wrong samples from best epoch\n",
    "        all_wrong_samples['train'] = train_metrics['wrong_samples']\n",
    "        all_wrong_samples['val'] = val_metrics['wrong_samples']\n",
    "        all_wrong_samples['test'] = test_metrics['wrong_samples']\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # ============ LOGGING ============\n",
    "    print(f\"\\n  üìä Results:\")\n",
    "    print(f\"     {'Metric':<15} {'Train':>10} {'Val':>10} {'Test':>10}\")\n",
    "    print(f\"     {'-'*15} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "    print(f\"     {'Loss':<15} {train_metrics['loss']:>10.4f} {'-':>10} {'-':>10}\")\n",
    "    print(f\"     {'Accuracy':<15} {train_metrics['acc']:>9.2f}% {val_metrics['acc']:>9.2f}% {test_metrics['acc']:>9.2f}%\")\n",
    "    print(f\"     {'Wrong samples':<15} {len(train_metrics['wrong_samples']):>10} {len(val_metrics['wrong_samples']):>10} {len(test_metrics['wrong_samples']):>10}\")\n",
    "    \n",
    "    # Per question type accuracy\n",
    "    print(f\"\\n  üìà Per Question Type Accuracy (Val):\")\n",
    "    qtype_order = ['Des', 'Exp', 'Pred-A', 'Pred-R', 'CF-A', 'CF-R']\n",
    "    for qt in qtype_order:\n",
    "        if qt in val_metrics['qtype_acc']:\n",
    "            print(f\"     {qt:<10}: {val_metrics['qtype_acc'][qt]:>6.2f}%\")\n",
    "    \n",
    "    print(f\"\\n  ‚è±Ô∏è  Time: {train_metrics['time']:.1f}s | LR: {current_lr:.2e}\")\n",
    "    print(f\"  üìâ No improvement: {epochs_without_improvement}/{args.early_stopping_patience} epochs\")\n",
    "    if is_best:\n",
    "        print(f\"  üíæ Saved best model! (Val acc: {best_val_acc:.2f}%)\")\n",
    "    \n",
    "    # ============ WANDB LOGGING ============\n",
    "    wandb_log = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train/loss\": train_metrics['loss'],\n",
    "        \"train/acc\": train_metrics['acc'],\n",
    "        \"train/wrong_count\": len(train_metrics['wrong_samples']),\n",
    "        \"val/acc\": val_metrics['acc'],\n",
    "        \"val/wrong_count\": len(val_metrics['wrong_samples']),\n",
    "        \"test/acc\": test_metrics['acc'],\n",
    "        \"test/wrong_count\": len(test_metrics['wrong_samples']),\n",
    "        \"lr\": current_lr,\n",
    "        \"epoch_time\": train_metrics['time'],\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"epochs_without_improvement\": epochs_without_improvement,\n",
    "    }\n",
    "    \n",
    "    # Log per question type accuracy\n",
    "    for qt, acc in train_metrics['qtype_acc'].items():\n",
    "        wandb_log[f\"train/acc_{qt}\"] = acc\n",
    "    for qt, acc in val_metrics['qtype_acc'].items():\n",
    "        wandb_log[f\"val/acc_{qt}\"] = acc\n",
    "    for qt, acc in test_metrics['qtype_acc'].items():\n",
    "        wandb_log[f\"test/acc_{qt}\"] = acc\n",
    "    \n",
    "    wandb.log(wandb_log)\n",
    "    \n",
    "    # Save checkpoint every N epochs\n",
    "    if epoch % args.save_every == 0:\n",
    "        ckpt_path = f'./models/checkpoint-{args.run_name}-ep{epoch}.ckpt'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_metrics['acc'],\n",
    "        }, ckpt_path)\n",
    "        print(f\"  üìÅ Checkpoint saved: {ckpt_path}\")\n",
    "    \n",
    "    # ============ EARLY STOPPING CHECK ============\n",
    "    if epochs_without_improvement >= args.early_stopping_patience:\n",
    "        print(f\"\\n  ‚ö†Ô∏è EARLY STOPPING: No improvement for {args.early_stopping_patience} epochs\")\n",
    "        print(f\"     Best val acc: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "        wandb.log({\"early_stopped\": True, \"stopped_at_epoch\": epoch})\n",
    "        break\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING COMPLETE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Best epoch: {best_epoch}\")\n",
    "print(f\"   Best val accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   Model saved: {best_model_path}\")\n",
    "print(f\"   Wrong samples tracked: Train={len(all_wrong_samples['train'])}, Val={len(all_wrong_samples['val'])}, Test={len(all_wrong_samples['test'])}\")\n",
    "if epochs_without_improvement >= args.early_stopping_patience:\n",
    "    print(f\"   Stopped early at epoch {epoch}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T18:16:19.192640Z",
     "iopub.status.busy": "2025-12-09T18:16:19.192369Z",
     "iopub.status.idle": "2025-12-09T18:20:15.644585Z",
     "shell.execute_reply": "2025-12-09T18:20:15.643849Z",
     "shell.execute_reply.started": "2025-12-09T18:16:19.192624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL EVALUATION WITH BEST MODEL\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä FINAL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load best model\n",
    "print(\"\\n  Loading best model...\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Predict on test set with wrong sample tracking\n",
    "result_path = f'./prediction/{args.run_name}-ep{best_epoch}-val{best_val_acc:.2f}.json'\n",
    "results, test_acc, test_wrong_samples = predict_and_save(model, test_loader, device, result_path)\n",
    "\n",
    "print(f\"\\n  Test accuracy: {test_acc:.2f}%\")\n",
    "print(f\"  Results saved: {result_path}\")\n",
    "print(f\"  Wrong predictions: {len(test_wrong_samples)}\")\n",
    "\n",
    "# Save wrong samples to JSON\n",
    "wrong_samples_path = f'./prediction/{args.run_name}-wrong_samples.json'\n",
    "with open(wrong_samples_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'test': test_wrong_samples,\n",
    "        'val': all_wrong_samples['val'],\n",
    "        'metadata': {\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'run_name': args.run_name,\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "print(f\"  Wrong samples saved: {wrong_samples_path}\")\n",
    "\n",
    "# Detailed evaluation by question type\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"  Detailed Results by Question Type:\")\n",
    "print(\"-\" * 70)\n",
    "eval_mc.accuracy_metric_cvid(result_path)\n",
    "\n",
    "# Log final results to wandb\n",
    "wandb.log({\n",
    "    \"final/test_acc\": test_acc,\n",
    "    \"final/best_epoch\": best_epoch,\n",
    "    \"final/best_val_acc\": best_val_acc,\n",
    "    \"final/wrong_test_count\": len(test_wrong_samples),\n",
    "})\n",
    "\n",
    "# Save results artifact\n",
    "artifact = wandb.Artifact(f'predictions-{args.run_name}', type='predictions')\n",
    "artifact.add_file(result_path)\n",
    "artifact.add_file(wrong_samples_path)\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T18:20:15.645692Z",
     "iopub.status.busy": "2025-12-09T18:20:15.645448Z",
     "iopub.status.idle": "2025-12-09T18:20:32.971169Z",
     "shell.execute_reply": "2025-12-09T18:20:32.970633Z",
     "shell.execute_reply.started": "2025-12-09T18:20:15.645665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE TO KAGGLE OUTPUT & FINISH WANDB\n",
    "# ============================================================\n",
    "import shutil\n",
    "\n",
    "output_dir = '/kaggle/working'\n",
    "if os.path.exists(output_dir):\n",
    "    # Copy best model\n",
    "    shutil.copy(best_model_path, os.path.join(output_dir, f'best_model-{args.run_name}.ckpt'))\n",
    "    # Copy predictions\n",
    "    shutil.copy(result_path, output_dir)\n",
    "    # Copy wrong samples\n",
    "    shutil.copy(wrong_samples_path, output_dir)\n",
    "    print(f\"‚úÖ Files saved to {output_dir}\")\n",
    "else:\n",
    "    print(\"  Not running on Kaggle, files saved locally\")\n",
    "\n",
    "# Save model artifact to wandb\n",
    "model_artifact = wandb.Artifact(f'model-{args.run_name}', type='model')\n",
    "model_artifact.add_file(best_model_path)\n",
    "wandb.log_artifact(model_artifact)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "print(\"‚úÖ W&B run finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T18:20:32.972185Z",
     "iopub.status.busy": "2025-12-09T18:20:32.971966Z",
     "iopub.status.idle": "2025-12-09T18:24:29.968023Z",
     "shell.execute_reply": "2025-12-09T18:24:29.967372Z",
     "shell.execute_reply.started": "2025-12-09T18:20:32.972169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UPLOAD WRONG PREDICTIONS TO HUGGINGFACE DATASET\n",
    "# ============================================================\n",
    "# Install huggingface_hub if not already installed\n",
    "!pip install -q huggingface_hub datasets\n",
    "\n",
    "from huggingface_hub import HfApi, login\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # Replace with your HuggingFace token\n",
    "HF_REPO_NAME = \"your-username/causalvid-wrong-predictions\"  # Replace with your repo name\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# ============================================================\n",
    "# PREPARE WRONG SAMPLES DATA\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üì§ PREPARING WRONG PREDICTIONS FOR HUGGINGFACE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load wrong samples\n",
    "with open(wrong_samples_path, 'r') as f:\n",
    "    wrong_data = json.load(f)\n",
    "\n",
    "# Question type names\n",
    "qtype_names = {\n",
    "    0: 'descriptive',\n",
    "    1: 'explanatory', \n",
    "    2: 'predictive_answer',\n",
    "    3: 'predictive_reason',\n",
    "    4: 'counterfactual_answer',\n",
    "    5: 'counterfactual_reason'\n",
    "}\n",
    "\n",
    "# Prepare test wrong samples as DataFrame\n",
    "test_wrong = wrong_data.get('test', [])\n",
    "if test_wrong:\n",
    "    df_test = pd.DataFrame(test_wrong)\n",
    "    df_test['qtype_name'] = df_test['qtype'].map(qtype_names)\n",
    "    df_test['split'] = 'test'\n",
    "    print(f\"  Test wrong samples: {len(df_test)}\")\n",
    "else:\n",
    "    df_test = pd.DataFrame()\n",
    "    print(\"  No test wrong samples\")\n",
    "\n",
    "# Prepare val wrong samples as DataFrame\n",
    "val_wrong = wrong_data.get('val', [])\n",
    "if val_wrong:\n",
    "    df_val = pd.DataFrame(val_wrong)\n",
    "    df_val['qtype_name'] = df_val['qtype'].map(qtype_names)\n",
    "    df_val['split'] = 'val'\n",
    "    print(f\"  Val wrong samples: {len(df_val)}\")\n",
    "else:\n",
    "    df_val = pd.DataFrame()\n",
    "    print(\"  No val wrong samples\")\n",
    "\n",
    "# Combine all wrong samples\n",
    "df_all = pd.concat([df_test, df_val], ignore_index=True) if not df_test.empty or not df_val.empty else pd.DataFrame()\n",
    "\n",
    "if df_all.empty:\n",
    "    print(\"‚ö†Ô∏è No wrong samples to upload!\")\n",
    "else:\n",
    "    # Add metadata columns\n",
    "    df_all['model_name'] = args.run_name\n",
    "    df_all['best_epoch'] = best_epoch\n",
    "    df_all['best_val_acc'] = best_val_acc\n",
    "    \n",
    "    print(f\"\\n  Total wrong samples: {len(df_all)}\")\n",
    "    print(f\"  Columns: {list(df_all.columns)}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\n  Sample wrong predictions:\")\n",
    "    print(df_all.head(3).to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# CREATE HUGGINGFACE DATASET\n",
    "# ============================================================\n",
    "if not df_all.empty:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üì§ UPLOADING TO HUGGINGFACE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create HuggingFace Dataset\n",
    "    hf_dataset = Dataset.from_pandas(df_all)\n",
    "    \n",
    "    # Create DatasetDict with splits\n",
    "    dataset_dict = DatasetDict({\n",
    "        'test': Dataset.from_pandas(df_test) if not df_test.empty else None,\n",
    "        'val': Dataset.from_pandas(df_val) if not df_val.empty else None,\n",
    "        'all': hf_dataset\n",
    "    })\n",
    "    \n",
    "    # Remove None splits\n",
    "    dataset_dict = DatasetDict({k: v for k, v in dataset_dict.items() if v is not None})\n",
    "    \n",
    "    print(f\"  Dataset splits: {list(dataset_dict.keys())}\")\n",
    "    print(f\"  Total samples: {sum(len(v) for v in dataset_dict.values())}\")\n",
    "    \n",
    "    # Push to HuggingFace Hub\n",
    "    try:\n",
    "        dataset_dict.push_to_hub(\n",
    "            HF_REPO_NAME,\n",
    "            private=True,  # Set to False if you want public dataset\n",
    "            token=HF_TOKEN\n",
    "        )\n",
    "        print(f\"\\n‚úÖ Successfully uploaded to: https://huggingface.co/datasets/{HF_REPO_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error uploading to HuggingFace: {e}\")\n",
    "        print(\"  Make sure to set HF_TOKEN and HF_REPO_NAME correctly\")\n",
    "        \n",
    "        # Save locally as backup\n",
    "        backup_path = f'./prediction/{args.run_name}-wrong_samples_hf.parquet'\n",
    "        df_all.to_parquet(backup_path)\n",
    "        print(f\"  Saved backup to: {backup_path}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANALYZE WRONG PREDICTIONS\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä WRONG PREDICTIONS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load wrong samples\n",
    "with open(wrong_samples_path, 'r') as f:\n",
    "    wrong_data = json.load(f)\n",
    "\n",
    "test_wrong = wrong_data.get('test', [])\n",
    "\n",
    "if test_wrong:\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(test_wrong)\n",
    "    \n",
    "    # Question type distribution\n",
    "    print(\"\\nüîç Wrong predictions by Question Type:\")\n",
    "    print(\"-\" * 40)\n",
    "    qtype_names = {\n",
    "        0: 'Descriptive',\n",
    "        1: 'Explanatory', \n",
    "        2: 'Predictive-Ans',\n",
    "        3: 'Predictive-Reason',\n",
    "        4: 'Counterfactual-Ans',\n",
    "        5: 'Counterfactual-Reason'\n",
    "    }\n",
    "    \n",
    "    qtype_counts = df['qtype'].value_counts().sort_index()\n",
    "    for qt, count in qtype_counts.items():\n",
    "        print(f\"  {qtype_names.get(qt, qt):<20}: {count:>5} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Most common video IDs with wrong predictions\n",
    "    print(\"\\nüé¨ Videos with most wrong predictions:\")\n",
    "    print(\"-\" * 40)\n",
    "    video_counts = df['video_id'].value_counts().head(10)\n",
    "    for vid, count in video_counts.items():\n",
    "        print(f\"  {vid}: {count} wrong\")\n",
    "    \n",
    "    # Prediction vs Answer distribution\n",
    "    print(\"\\nüìà Prediction distribution (wrong samples):\")\n",
    "    print(\"-\" * 40)\n",
    "    pred_dist = df['prediction'].value_counts().sort_index()\n",
    "    for pred, count in pred_dist.items():\n",
    "        print(f\"  Choice {pred}: {count} times predicted\")\n",
    "    \n",
    "    print(\"\\nüìâ Answer distribution (ground truth for wrong samples):\")\n",
    "    print(\"-\" * 40)\n",
    "    ans_dist = df['answer'].value_counts().sort_index()\n",
    "    for ans, count in ans_dist.items():\n",
    "        print(f\"  Choice {ans}: {count} times correct\")\n",
    "        \n",
    "else:\n",
    "    print(\"  No wrong samples to analyze\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating pretrained model B2A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8208331,
     "sourceId": 12969233,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8210299,
     "sourceId": 12972016,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8210716,
     "sourceId": 12972597,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8211749,
     "sourceId": 12977127,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8383289,
     "isSourceIdPinned": true,
     "sourceId": 13226170,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8208446,
     "sourceId": 13229803,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8208348,
     "sourceId": 13919566,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8888719,
     "sourceId": 13946226,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
