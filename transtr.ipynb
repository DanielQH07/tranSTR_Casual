{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Environment Setup (Kaggle)\n",
                "This cell clones the repository and sets the working directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "# --- Git Clone & Setup ---\n",
                "REPO_URL = \"https://github.com/DanielQH07/tranSTR_Casual.git\" # Replace with your repo url if different\n",
                "REPO_NAME = \"tranSTR_Casual\"\n",
                "BRANCH = \"main\" # or master\n",
                "\n",
                "if not os.path.exists(REPO_NAME):\n",
                "    print(f\"Cloning {REPO_URL}...\")\n",
                "    !git clone {REPO_URL}\n",
                "else:\n",
                "    print(\"Repo already exists.\")\n",
                "\n",
                "# Change Directory to the repo root for relative imports to work\n",
                "if os.path.basename(os.getcwd()) != REPO_NAME:\n",
                "    try:\n",
                "        # Try searching for causalvid subdirectory first as per common structures\n",
                "        target_dir = os.path.join(os.getcwd(), REPO_NAME, \"causalvid\")\n",
                "        if os.path.exists(target_dir):\n",
                "             os.chdir(target_dir)\n",
                "        elif os.path.exists(REPO_NAME):\n",
                "             os.chdir(REPO_NAME)\n",
                "        \n",
                "        print(f\"Changed directory to: {os.getcwd()}\")\n",
                "    except Exception as e:\n",
                "             print(f\"Could not set working directory: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import argparse\n",
                "from torch.utils.data import DataLoader\n",
                "from utils.util import set_seed, set_gpu_devices, save_file\n",
                "from networks.model import VideoQAmodel\n",
                "from DataLoader import VideoQADataset\n",
                "import torch.nn as nn\n",
                "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
                "from utils.logger import logger\n",
                "\n",
                "# Import training functions from train.py to ensure consistency\n",
                "from train import train, eval, predict\n",
                "\n",
                "print(\"Libraries imported successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Download Data (Kaggle Setup)\n",
                "We need to download:\n",
                "1. **Object Features**: `objbox.tar.gz` from Hugging Face.\n",
                "2. **Annotations & Splits**: Assuming these are in another dataset or need to be downloaded. **PLEASE UPDATE `ANNO_URL` IF NEEDED**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Data Configuration ---\n",
                "HF_OBJ_URL = \"https://huggingface.co/datasets/DanielQ07/kltn/resolve/main/objbox.tar.gz\"\n",
                "# TODO: Add URL for Annotations/Splits if they are not in the git repo\n",
                "# HF_ANNO_URL = \"...\"\n",
                "\n",
                "import os\n",
                "import tarfile\n",
                "import urllib.request\n",
                "\n",
                "# Define Output Directories\n",
                "BASE_WORK_DIR = \"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else os.getcwd()\n",
                "OBJ_DIR = os.path.join(BASE_WORK_DIR, \"features\", \"objects\")\n",
                "ANNO_DIR = os.path.join(BASE_WORK_DIR, \"data\", \"annotations\") # For sample_list_path\n",
                "\n",
                "for d in [OBJ_DIR, ANNO_DIR]:\n",
                "    if not os.path.exists(d): os.makedirs(d)\n",
                "\n",
                "print(f\"Object Features will be in: {OBJ_DIR}\")\n",
                "print(f\"Annotations should be in: {ANNO_DIR}\")\n",
                "\n",
                "# 1. Download Object Features\n",
                "try:\n",
                "    tar_name = \"objbox.tar.gz\"\n",
                "    tar_path = os.path.join(OBJ_DIR, tar_name)\n",
                "    if HF_OBJ_URL:\n",
                "        print(f\"Downloading {tar_name}...\")\n",
                "        urllib.request.urlretrieve(HF_OBJ_URL, tar_path)\n",
                "        \n",
                "        if os.path.exists(tar_path):\n",
                "            print(\"Extracting...\")\n",
                "            with tarfile.open(tar_path, \"r:gz\") as tar:\n",
                "                tar.extractall(path=OBJ_DIR)\n",
                "            print(\"Extraction Complete.\")\n",
                "            # Cleanup\n",
                "            os.remove(tar_path)\n",
                "except Exception as e:\n",
                "    print(f\"Error downloading/extracting objects: {e}\")\n",
                "\n",
                "# 2. (Optional) Download Annotations if needed\n",
                "# If your annotations are in the same repo, you can point to them directly.\n",
                "# Otherwise, implement similar download logic here."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Configuration and Path Setup\n",
                "\n",
                "**Crucial Updates:**\n",
                "- **start_list_path**: Updated to point to the parent directory containing video_id folders (`text.json`, `answer.json`).\n",
                "- **object_feature_path**: Points to the extracted object features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Config:\n",
                "    # --- PATHS ---\n",
                "    # 1. Video Features (ViT features)\n",
                "    # Ensure this points to where you have stored/downloaded the .pt files\n",
                "    video_feature_root = r\"D:\\KLTN\\TranSTR\\causalvid\\features\\vit\"  \n",
                "    if not os.path.exists(video_feature_root) and os.path.exists(\"/kaggle/input\"):\n",
                "        # Example Kaggle Input path - UPDATE THIS\n",
                "        video_feature_root = \"/kaggle/input/your-dataset-name/features/vit\"\n",
                "\n",
                "    # 2. Object Features (Downloaded above)\n",
                "    object_feature_path = OBJ_DIR\n",
                "\n",
                "    # 3. Annotations (sample_list_path)\n",
                "    # Needs to point to directory containing video_id folders with json files\n",
                "    # Defaulting to a likely location in the repo or the created ANNO_DIR\n",
                "    sample_list_path = r\"D:\\KLTN\\TranSTR\\causalvid\\data\\vqa\\causal\\anno\"\n",
                "    if not os.path.exists(sample_list_path):\n",
                "        # Fallback to repo location if cloned\n",
                "        repo_anno = os.path.join(os.getcwd(), \"..\", \"data\", \"vqa\", \"causal\", \"anno\")\n",
                "        if os.path.exists(repo_anno):\n",
                "            sample_list_path = repo_anno\n",
                "        else:\n",
                "            # Use the directory we created earlier (user might need to upload data there)\n",
                "            sample_list_path = ANNO_DIR\n",
                "\n",
                "    # 4. Splits (train.txt, valid.txt, test.txt)\n",
                "    split_dir_txt = r\"D:\\KLTN\\TranSTR\\causalvid\\data\\splits\"\n",
                "    if not os.path.exists(split_dir_txt):\n",
                "         repo_split = os.path.join(os.getcwd(), \"..\", \"data\", \"splits\")\n",
                "         if os.path.exists(repo_split):\n",
                "             split_dir_txt = repo_split\n",
                "    \n",
                "    print(f\"Configured Paths:\\n - Video Feat: {video_feature_root}\\n - Object Feat: {object_feature_path}\\n - Anno Path: {sample_list_path}\\n - Split Dir: {split_dir_txt}\")\n",
                "\n",
                "    # Model Params\n",
                "    v = \"v1\"\n",
                "    bs = 8 \n",
                "    lr = 1e-4\n",
                "    epoch = 20\n",
                "    gpu = 0\n",
                "    dropout = 0.3\n",
                "    encoder_dropout = 0.3\n",
                "    patience = 5\n",
                "    gamma = 0.1\n",
                "    decay = 1e-4\n",
                "    \n",
                "    # Dataset Params\n",
                "    dataset = 'causal-vid'\n",
                "    objs = 10 \n",
                "    n_query = 5\n",
                "    \n",
                "    # Transformer\n",
                "    d_model = 512\n",
                "    word_dim = 768\n",
                "    topK_frame = 20\n",
                "    topK_obj = 10\n",
                "    num_encoder_layers = 2\n",
                "    num_decoder_layers = 2\n",
                "    nheads = 8\n",
                "    normalize_before = True\n",
                "    activation = 'gelu'\n",
                "    \n",
                "    # Text Encoder\n",
                "    text_encoder_lr = 1e-5\n",
                "    freeze_text_encoder = False\n",
                "    text_encoder_type = \"bert-base-uncased\"\n",
                "    text_pool_mode = 1\n",
                "    \n",
                "    # Misc\n",
                "    hard_eval = False\n",
                "    pos_ratio = 1.0\n",
                "    neg_ratio = 1.0\n",
                "    a = 1.0\n",
                "    use_amp = True\n",
                "    num_workers = 2 # Reduced for stability\n",
                "    frame_feat_dim = 768 \n",
                "    obj_feat_dim = 2048 + 5 \n",
                "\n",
                "args = Config()\n",
                "set_gpu_devices(args.gpu)\n",
                "set_seed(999)\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize DataLoaders\n",
                "The DataLoader has been updated to parse the new JSON folder structure automatically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Initializing Datasets...\")\n",
                "\n",
                "try:\n",
                "    # TRAIN\n",
                "    train_dataset = VideoQADataset(\n",
                "        split='train', \n",
                "        n_query=args.n_query, \n",
                "        obj_num=args.objs,\n",
                "        sample_list_path=args.sample_list_path,\n",
                "        video_feature_path=args.video_feature_root,\n",
                "        object_feature_path=args.object_feature_path,\n",
                "        split_dir=args.split_dir_txt \n",
                "    )\n",
                "\n",
                "    # VAL\n",
                "    val_dataset = VideoQADataset(\n",
                "        split='val', \n",
                "        n_query=args.n_query, \n",
                "        obj_num=args.objs,\n",
                "        sample_list_path=args.sample_list_path,\n",
                "        video_feature_path=args.video_feature_root,\n",
                "        object_feature_path=args.object_feature_path,\n",
                "        split_dir=args.split_dir_txt\n",
                "    )\n",
                "\n",
                "    # TEST\n",
                "    test_dataset = VideoQADataset(\n",
                "        split='test', \n",
                "        n_query=args.n_query, \n",
                "        obj_num=args.objs,\n",
                "        sample_list_path=args.sample_list_path,\n",
                "        video_feature_path=args.video_feature_root,\n",
                "        object_feature_path=args.object_feature_path,\n",
                "        split_dir=args.split_dir_txt\n",
                "    )\n",
                "\n",
                "    train_loader = DataLoader(dataset=train_dataset, batch_size=args.bs, shuffle=True, \n",
                "                              num_workers=args.num_workers, pin_memory=True)\n",
                "    val_loader = DataLoader(dataset=val_dataset, batch_size=args.bs, shuffle=False, \n",
                "                            num_workers=args.num_workers, pin_memory=True)\n",
                "    test_loader = DataLoader(dataset=test_dataset, batch_size=args.bs, shuffle=False, \n",
                "                             num_workers=args.num_workers, pin_memory=True)\n",
                "\n",
                "    print(\"\\nDatasets initialized successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"\\nError initializing datasets: {e}\")\n",
                "    import traceback\n",
                "    traceback.print_exc()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Verify Data Loading\n",
                "We verify that a batch can be loaded correctly, ensuring dimensions are as expected."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    if len(train_loader) > 0:\n",
                "        for sample in train_loader:\n",
                "            vid_frame_feat, vid_obj_feat, qns_word, ans_word, ans_id, qns_key = sample\n",
                "            print(\"--- Data Verification ---\")\n",
                "            print(f\"Frame Features Shape: {vid_frame_feat.shape}\")\n",
                "            print(f\"Object Features Shape: {vid_obj_feat.shape}\")\n",
                "            print(f\"Question Batch Size: {len(qns_word)}\")\n",
                "            print(f\"Sample Question: {qns_word[0]}\")\n",
                "            print(\"--- Verification Passed ---\")\n",
                "            break\n",
                "    else:\n",
                "        print(\"Warning: Train loader is empty. Check your data paths and splits.\")\n",
                "except Exception as e:\n",
                "    print(f\"Data Verification Failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Setup and Training\n",
                "We initialize the model with the configuration and start the training loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config_dict = {k: v for k, v in Config.__dict__.items() if not k.startswith('__')}\n",
                "config_dict['device'] = device\n",
                "# Ensure compatibility with Model constructor\n",
                "model = VideoQAmodel(**config_dict)\n",
                "\n",
                "# Optimizer config\n",
                "param_dicts = [\n",
                "    {\"params\": [p for n, p in model.named_parameters() if \"text_encoder\" not in n and p.requires_grad]},\n",
                "    {\"params\": [p for n, p in model.named_parameters() if \"text_encoder\" in n and p.requires_grad], \"lr\": args.text_encoder_lr}\n",
                "]\n",
                "optimizer = torch.optim.AdamW(params=param_dicts, lr=args.lr, weight_decay=args.decay)\n",
                "scheduler = ReduceLROnPlateau(optimizer, 'max', factor=args.gamma, patience=args.patience, verbose=True)\n",
                "model.to(device)\n",
                "xe = nn.CrossEntropyLoss().to(device)\n",
                "scaler = torch.cuda.amp.GradScaler(enabled=args.use_amp)\n",
                "\n",
                "print(\"Model and Optimizer created.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Starting Training Loop...\")\n",
                "best_eval_score = 0.0\n",
                "best_epoch = 1\n",
                "\n",
                "# Initialize logger dummy/wrapper if needed or use imported\n",
                "# logger.debug = print # Simple redirect if logger not configured\n",
                "\n",
                "if len(train_loader) > 0:\n",
                "    for epoch in range(1, args.epoch + 1):\n",
                "        print(f\"Epoch {epoch}/{args.epoch}\")\n",
                "        train_loss, train_acc = train(model, optimizer, train_loader, xe, device, use_amp=args.use_amp)\n",
                "        eval_score = eval(model, val_loader, device)\n",
                "        scheduler.step(eval_score)\n",
                "        \n",
                "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f} | Val Acc: {eval_score:.2f}\")\n",
                "        \n",
                "        if eval_score > best_eval_score:\n",
                "            best_eval_score = eval_score\n",
                "            best_epoch = epoch\n",
                "            save_path = f'./models/best_model_epoch_{epoch}.ckpt'\n",
                "            if not os.path.exists('./models'): os.makedirs('./models')\n",
                "            torch.save(model.state_dict(), save_path)\n",
                "            print(f\"  Saved new best model to {save_path}\")\n",
                "        \n",
                "        # Optional: Run test every epoch\n",
                "        test_score = eval(model, test_loader, device)\n",
                "        print(f\"  Test Acc: {test_score:.2f}\")\n",
                "\n",
                "    print(\"Training Complete.\")\n",
                "    print(f\"Best Val Acc: {best_eval_score:.2f} at Epoch {best_epoch}\")\n",
                "else:\n",
                "    print(\"Cannot start training because train_loader is empty.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Final Test Evaluation\n",
                "Load the best model and evaluate on the test set, saving predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if os.path.exists(f'./models/best_model_epoch_{best_epoch}.ckpt'):\n",
                "    model.load_state_dict(torch.load(f'./models/best_model_epoch_{best_epoch}.ckpt'))\n",
                "    print(\"Loaded best model.\")\n",
                "    \n",
                "results, test_acc = predict(model, test_loader, device)\n",
                "print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
                "\n",
                "# Save results\n",
                "if not os.path.exists('./prediction'): os.makedirs('./prediction')\n",
                "result_path = f'./prediction/results_best_{best_epoch}.json'\n",
                "save_file(results, result_path)\n",
                "print(f\"Predictions saved to {result_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}