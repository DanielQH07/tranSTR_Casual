{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TranSTR CausalVid - Training Notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "REPO_URL = \"https://github.com/DanielQH07/tranSTR_Casual.git\"\n",
                "REPO_NAME = \"tranSTR_Casual\"\n",
                "if not os.path.exists(REPO_NAME):\n",
                "    !git clone {REPO_URL}\n",
                "os.chdir(os.path.join(REPO_NAME, \"causalvid\") if os.path.exists(os.path.join(REPO_NAME, \"causalvid\")) else REPO_NAME)\n",
                "print(f\"CWD: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === PATCH DataLoader.py to fix frame/object dimension mismatch ===\n",
                "DATALOADER_CODE = '''\n",
                "import torch\n",
                "import os\n",
                "import re\n",
                "import json\n",
                "import pandas as pd\n",
                "import pickle as pkl\n",
                "import os.path as osp\n",
                "import numpy as np\n",
                "from torch.utils.data import Dataset\n",
                "from utils.util import transform_bb\n",
                "\n",
                "class VideoQADataset(Dataset):\n",
                "    def __init__(self, split, n_query=5, obj_num=10, sample_list_path=\"/anno\",\n",
                "         video_feature_path=\"/vit\", object_feature_path=\"/obj\", split_dir=None, topK_frame=16):\n",
                "        super().__init__()\n",
                "        self.split = split\n",
                "        self.mc = n_query\n",
                "        self.obj_num = obj_num\n",
                "        self.video_feature_path = video_feature_path\n",
                "        self.object_feature_path = object_feature_path\n",
                "        self.topK_frame = topK_frame\n",
                "\n",
                "        valid_vids = set()\n",
                "        if split_dir:\n",
                "            txt_name = \"valid\" if split == \"val\" else split\n",
                "            txt_path = osp.join(split_dir, f\"{txt_name}.txt\")\n",
                "            if osp.exists(txt_path):\n",
                "                with open(txt_path) as f:\n",
                "                    valid_vids = {l.strip() for l in f if l.strip()}\n",
                "\n",
                "        if not valid_vids and os.path.isdir(sample_list_path):\n",
                "            valid_vids = {d for d in os.listdir(sample_list_path) if os.path.isdir(osp.join(sample_list_path, d))}\n",
                "\n",
                "        data_rows = []\n",
                "        for vid in valid_vids:\n",
                "            vid_path = osp.join(sample_list_path, vid)\n",
                "            t_json, a_json = osp.join(vid_path, \"text.json\"), osp.join(vid_path, \"answer.json\")\n",
                "            if not (osp.exists(t_json) and osp.exists(a_json)): continue\n",
                "            try:\n",
                "                with open(t_json, encoding=\"utf-8\") as f: t_data = json.load(f)\n",
                "                with open(a_json, encoding=\"utf-8\") as f: a_data = json.load(f)\n",
                "                for key in [\"descriptive\", \"explanatory\", \"predictive\", \"counterfactual\"]:\n",
                "                    if key in t_data and key in a_data:\n",
                "                        q, a = t_data[key], a_data[key]\n",
                "                        if \"question\" in q and \"answer\" in q and \"answer\" in a:\n",
                "                            row = {\"video_id\": vid, \"question\": q[\"question\"], \"answer\": a[\"answer\"], \"type\": key}\n",
                "                            for i, c in enumerate(q[\"answer\"]): row[f\"a{i}\"] = c\n",
                "                            data_rows.append(row)\n",
                "                        if key in [\"predictive\", \"counterfactual\"] and \"reason\" in q and \"reason\" in a:\n",
                "                            row = {\"video_id\": vid, \"question\": \"Why?\", \"answer\": a[\"reason\"], \"type\": f\"{key}_reason\"}\n",
                "                            for i, c in enumerate(q[\"reason\"]): row[f\"a{i}\"] = c\n",
                "                            data_rows.append(row)\n",
                "            except: pass\n",
                "\n",
                "        self.sample_list = pd.DataFrame(data_rows)\n",
                "        print(f\"Loaded {len(self.sample_list)} QA pairs.\")\n",
                "        \n",
                "        if len(self.sample_list) > 0:\n",
                "            existing = {v for v in self.sample_list[\"video_id\"].unique()\n",
                "                        if osp.exists(osp.join(self.video_feature_path, self.split, f\"{v}.pt\"))}\n",
                "            self.sample_list = self.sample_list[self.sample_list[\"video_id\"].isin(existing)]\n",
                "            print(f\"Final: {len(self.sample_list)}\")\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        cur = self.sample_list.iloc[idx]\n",
                "        vid = str(cur[\"video_id\"])\n",
                "        qns = str(cur[\"question\"])\n",
                "        ans_id = int(cur[\"answer\"])\n",
                "        ans_word = [f\"[CLS] {qns} [SEP] {cur[f\\\"a{i}\\\"]}\" for i in range(self.mc)]\n",
                "\n",
                "        # 1. Frame features\n",
                "        frame_feat = torch.load(osp.join(self.video_feature_path, self.split, f\"{vid}.pt\"))\n",
                "        if isinstance(frame_feat, np.ndarray): frame_feat = torch.from_numpy(frame_feat)\n",
                "        frame_feat = frame_feat.float()\n",
                "        \n",
                "        # SAMPLE/PAD to topK_frame\n",
                "        nf = frame_feat.shape[0]\n",
                "        if nf > self.topK_frame:\n",
                "            idx = np.linspace(0, nf-1, self.topK_frame).astype(int)\n",
                "            frame_feat = frame_feat[idx]\n",
                "        elif nf < self.topK_frame:\n",
                "            pad = torch.zeros((self.topK_frame - nf, frame_feat.shape[1]))\n",
                "            frame_feat = torch.cat([frame_feat, pad], 0)\n",
                "\n",
                "        # 2. Object features\n",
                "        obj_dir = osp.join(self.object_feature_path, vid)\n",
                "        obj_feats = []\n",
                "        def num(f): m = re.findall(r\"\\\\d+\", f); return int(m[-1]) if m else -1\n",
                "        if osp.isdir(obj_dir):\n",
                "            pkls = sorted([f for f in os.listdir(obj_dir) if f.endswith(\".pkl\") and not f.startswith(\"._\")], key=num)\n",
                "            npkl = len(pkls)\n",
                "            idxs = np.linspace(0, npkl-1, self.topK_frame).astype(int) if npkl > 0 else []\n",
                "            for i in idxs:\n",
                "                try:\n",
                "                    with open(osp.join(obj_dir, pkls[i]), \"rb\") as fp: c = pkl.load(fp)\n",
                "                    feat = c.get(\"feat\", c.get(\"features\")) if isinstance(c, dict) else c[0]\n",
                "                    bbox = c.get(\"bbox\", c.get(\"boxes\", c.get(\"box\"))) if isinstance(c, dict) else c[1]\n",
                "                    w = c.get(\"img_w\", 640) if isinstance(c, dict) else 640\n",
                "                    h = c.get(\"img_h\", 480) if isinstance(c, dict) else 480\n",
                "                    if isinstance(feat, np.ndarray): feat = torch.from_numpy(feat)\n",
                "                    if isinstance(bbox, np.ndarray): bbox = torch.from_numpy(bbox)\n",
                "                    if feat.shape[0] > self.obj_num: feat, bbox = feat[:self.obj_num], bbox[:self.obj_num]\n",
                "                    elif feat.shape[0] < self.obj_num:\n",
                "                        p = self.obj_num - feat.shape[0]\n",
                "                        feat = torch.cat([feat, torch.zeros(p, feat.shape[1])], 0)\n",
                "                        bbox = torch.cat([bbox, torch.zeros(p, bbox.shape[1])], 0)\n",
                "                    bb = torch.from_numpy(transform_bb(bbox.numpy(), w, h)).float()\n",
                "                    obj_feats.append(torch.cat([feat.float(), bb], -1))\n",
                "                except:\n",
                "                    obj_feats.append(torch.zeros(self.obj_num, 2053))\n",
                "        while len(obj_feats) < self.topK_frame:\n",
                "            obj_feats.append(torch.zeros(self.obj_num, 2053))\n",
                "        obj_feat = torch.stack(obj_feats)  # [topK, obj, 2053]\n",
                "\n",
                "        qns_key = f\"{vid}_{cur[\\\"type\\\"]}\"\n",
                "        return frame_feat, obj_feat, qns, ans_word, ans_id, qns_key\n",
                "\n",
                "    def __len__(self): return len(self.sample_list)\n",
                "'''\n",
                "with open('DataLoader.py', 'w') as f:\n",
                "    f.write(DATALOADER_CODE)\n",
                "print('DataLoader.py patched!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q huggingface_hub\n",
                "from huggingface_hub import notebook_login, HfApi, hf_hub_download, list_repo_tree\n",
                "notebook_login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, torch, numpy as np, pandas as pd\n",
                "from torch.utils.data import DataLoader\n",
                "from utils.util import set_seed, set_gpu_devices\n",
                "from networks.model import VideoQAmodel\n",
                "from DataLoader import VideoQADataset\n",
                "import torch.nn as nn\n",
                "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
                "from huggingface_hub import hf_hub_download, list_repo_tree, HfApi\n",
                "print('Imports OK')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(model, optimizer, loader, xe, device, use_amp=True, scaler=None):\n",
                "    model.train()\n",
                "    total_loss, preds, gts = 0, [], []\n",
                "    for batch in loader:\n",
                "        f, o, q, a, ans_id, _ = batch\n",
                "        f, o, tgt = f.to(device), o.to(device), ans_id.to(device)\n",
                "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
                "            out = model(f, o, q, a)\n",
                "            loss = xe(out, tgt)\n",
                "        optimizer.zero_grad()\n",
                "        if scaler:\n",
                "            scaler.scale(loss).backward()\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "        else:\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "        total_loss += loss.item()\n",
                "        preds.append(out.argmax(-1))\n",
                "        gts.append(ans_id)\n",
                "    preds = torch.cat(preds).cpu()\n",
                "    gts = torch.cat(gts)\n",
                "    return total_loss / len(loader), (preds == gts).float().mean().item() * 100\n",
                "\n",
                "def eval_model(model, loader, device):\n",
                "    model.eval()\n",
                "    preds, gts = [], []\n",
                "    with torch.no_grad():\n",
                "        for batch in loader:\n",
                "            f, o, q, a, ans_id, _ = batch\n",
                "            out = model(f.to(device), o.to(device), q, a)\n",
                "            preds.append(out.argmax(-1))\n",
                "            gts.append(ans_id)\n",
                "    preds = torch.cat(preds).cpu()\n",
                "    gts = torch.cat(gts)\n",
                "    return (preds == gts).float().mean().item() * 100\n",
                "\n",
                "print('Functions defined')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "RUN_TRAINING = True\n",
                "HF_REPO_ID = \"DanielQ07/transtr-causalvid-weights\"\n",
                "HF_MODEL_FILENAME = \"best_model.ckpt\"\n",
                "HF_DATASET_ID = \"DanielQ07/kltn\"\n",
                "HF_SHARD_FOLDER = \"shards\"\n",
                "\n",
                "import tarfile, shutil\n",
                "BASE = \"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else os.getcwd()\n",
                "OBJ_DIR = os.path.join(BASE, \"features\", \"objects\")\n",
                "MODEL_DIR = os.path.join(BASE, \"models\")\n",
                "os.makedirs(OBJ_DIR, exist_ok=True)\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "\n",
                "if not any(os.path.isdir(os.path.join(OBJ_DIR, d)) for d in os.listdir(OBJ_DIR)):\n",
                "    print('Downloading data...')\n",
                "    try:\n",
                "        files = list_repo_tree(HF_DATASET_ID, repo_type='dataset', path_in_repo=HF_SHARD_FOLDER)\n",
                "        tars = [f.path for f in files if f.path.endswith('.tar.gz')]\n",
                "        for t in tars:\n",
                "            print(f'  {os.path.basename(t)}')\n",
                "            p = hf_hub_download(HF_DATASET_ID, t, repo_type='dataset', local_dir=BASE)\n",
                "            with tarfile.open(p, 'r:gz') as tf: tf.extractall(OBJ_DIR)\n",
                "            os.remove(p)\n",
                "        for d in os.listdir(OBJ_DIR):\n",
                "            dp = os.path.join(OBJ_DIR, d)\n",
                "            if os.path.isdir(dp) and ('shard' in d or 'train' in d or 'val' in d):\n",
                "                for s in os.listdir(dp): shutil.move(os.path.join(dp, s), os.path.join(OBJ_DIR, s))\n",
                "                os.rmdir(dp)\n",
                "    except Exception as e: print(f'Error: {e}')\n",
                "print('Data ready')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Config:\n",
                "    # PATHS - Update these!\n",
                "    video_feature_root = \"/kaggle/input/your-vit-features\"  # << SET THIS\n",
                "    object_feature_path = OBJ_DIR\n",
                "    sample_list_path = os.path.join(os.getcwd(), '..', 'data', 'vqa', 'causal', 'anno')\n",
                "    split_dir_txt = os.path.join(os.getcwd(), '..', 'data', 'splits')\n",
                "\n",
                "    # === CRITICAL: Match your actual data! ===\n",
                "    topK_frame = 16      # Number of frames in your ViT features\n",
                "    frame_feat_dim = 1024  # Dimension of your ViT features\n",
                "    obj_feat_dim = 2053  # 2048 + 5 (bbox)\n",
                "    objs = 10            # Objects per frame\n",
                "    # =========================================\n",
                "    \n",
                "    bs = 8\n",
                "    lr = 1e-4\n",
                "    epoch = 20\n",
                "    gpu = 0\n",
                "    dropout = 0.3\n",
                "    encoder_dropout = 0.3\n",
                "    patience = 5\n",
                "    gamma = 0.1\n",
                "    decay = 1e-4\n",
                "    n_query = 5\n",
                "    d_model = 512\n",
                "    word_dim = 768\n",
                "    topK_obj = 10\n",
                "    num_encoder_layers = 2\n",
                "    num_decoder_layers = 2\n",
                "    nheads = 8\n",
                "    normalize_before = True\n",
                "    activation = 'gelu'\n",
                "    text_encoder_lr = 1e-5\n",
                "    freeze_text_encoder = False\n",
                "    text_encoder_type = 'bert-base-uncased'\n",
                "    text_pool_mode = 1\n",
                "    hard_eval = False\n",
                "    pos_ratio = 1.0\n",
                "    neg_ratio = 1.0\n",
                "    a = 1.0\n",
                "    use_amp = True\n",
                "    num_workers = 2\n",
                "\n",
                "args = Config()\n",
                "set_gpu_devices(args.gpu)\n",
                "set_seed(999)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Device: {device}, topK_frame={args.topK_frame}, frame_feat_dim={args.frame_feat_dim}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Creating datasets...')\n",
                "train_ds = VideoQADataset('train', args.n_query, args.objs, args.sample_list_path, args.video_feature_root, args.object_feature_path, args.split_dir_txt, args.topK_frame)\n",
                "val_ds = VideoQADataset('val', args.n_query, args.objs, args.sample_list_path, args.video_feature_root, args.object_feature_path, args.split_dir_txt, args.topK_frame)\n",
                "test_ds = VideoQADataset('test', args.n_query, args.objs, args.sample_list_path, args.video_feature_root, args.object_feature_path, args.split_dir_txt, args.topK_frame)\n",
                "train_loader = DataLoader(train_ds, args.bs, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
                "val_loader = DataLoader(val_ds, args.bs, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
                "test_loader = DataLoader(test_ds, args.bs, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
                "print(f'Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cfg = {k: v for k, v in Config.__dict__.items() if not k.startswith('_')}\n",
                "cfg['device'] = device\n",
                "model = VideoQAmodel(**cfg)\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.decay)\n",
                "scheduler = ReduceLROnPlateau(optimizer, 'max', factor=args.gamma, patience=args.patience)\n",
                "model.to(device)\n",
                "xe = nn.CrossEntropyLoss()\n",
                "scaler = torch.amp.GradScaler('cuda', enabled=args.use_amp)\n",
                "print('Model ready')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "save_path = os.path.join(MODEL_DIR, HF_MODEL_FILENAME)\n",
                "best_acc = 0\n",
                "if RUN_TRAINING:\n",
                "    print('Training...')\n",
                "    for ep in range(1, args.epoch + 1):\n",
                "        loss, acc = train(model, optimizer, train_loader, xe, device, args.use_amp, scaler)\n",
                "        val_acc = eval_model(model, val_loader, device)\n",
                "        scheduler.step(val_acc)\n",
                "        print(f'Ep {ep}: Loss={loss:.4f}, Train={acc:.2f}%, Val={val_acc:.2f}%')\n",
                "        if val_acc > best_acc:\n",
                "            best_acc = val_acc\n",
                "            torch.save(model.state_dict(), save_path)\n",
                "    print(f'Best Val: {best_acc:.2f}%')\n",
                "    try:\n",
                "        api = HfApi()\n",
                "        api.create_repo(HF_REPO_ID, repo_type='model', exist_ok=True)\n",
                "        api.upload_file(save_path, HF_MODEL_FILENAME, HF_REPO_ID, repo_type='model')\n",
                "        print('Uploaded!')\n",
                "    except Exception as e: print(f'Upload failed: {e}')\n",
                "else:\n",
                "    print('Skip training')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt, json\n",
                "if os.path.exists(save_path): model.load_state_dict(torch.load(save_path))\n",
                "model.eval()\n",
                "results = {}\n",
                "type_map = {'descriptive':'d','explanatory':'e','predictive':'p','predictive_reason':'pr','counterfactual':'c','counterfactual_reason':'cr'}\n",
                "with torch.no_grad():\n",
                "    for batch in test_loader:\n",
                "        f,o,q,a,ans_id,keys = batch\n",
                "        out = model(f.to(device), o.to(device), q, a)\n",
                "        preds = out.argmax(-1).cpu().numpy()\n",
                "        for i, k in enumerate(keys):\n",
                "            for ts, tsh in type_map.items():\n",
                "                if k.endswith('_'+ts):\n",
                "                    vid = k[:-(len(ts)+1)]\n",
                "                    if vid not in results: results[vid] = {}\n",
                "                    results[vid][tsh] = preds[i] == ans_id[i].item()\n",
                "                    break\n",
                "stats = {k: [0,0] for k in ['d','e','p','pr','c','cr','par','car']}\n",
                "for vid, r in results.items():\n",
                "    for t in ['d','e','p','pr','c','cr']:\n",
                "        if t in r: stats[t][1] += 1; stats[t][0] += int(r[t])\n",
                "    if 'p' in r and 'pr' in r: stats['par'][1] += 1; stats['par'][0] += int(r['p'] and r['pr'])\n",
                "    if 'c' in r and 'cr' in r: stats['car'][1] += 1; stats['car'][0] += int(r['c'] and r['cr'])\n",
                "print('Type   Acc%    Cor/Tot')\n",
                "for k in ['d','e','p','pr','par','c','cr','car']:\n",
                "    c, t = stats[k]\n",
                "    print(f'{k.upper():<6} {c/t*100 if t else 0:<8.2f} {c}/{t}')\n",
                "labels = [k.upper() for k in ['d','e','p','pr','par','c','cr','car'] if stats[k][1]]\n",
                "accs = [stats[k.lower()][0]/stats[k.lower()][1]*100 for k in labels]\n",
                "plt.figure(figsize=(10,4)); plt.bar(labels, accs); plt.ylim(0,100); plt.ylabel('Accuracy %'); plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}