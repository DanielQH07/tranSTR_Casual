{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "gpu",
            "dataSources": [],
            "dockerImageVersionId": 31259,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": true
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "# TranSTR + Token Mark - Inference & Visualization\n\n**This notebook:**\n1. Loads best model checkpoint from W&B\n2. Runs inference on test samples with/without Token Mark\n3. Visualizes:\n   - 16 sampled frames from raw video\n   - Selected frames after TopK filtering\n   - Object bounding boxes\n   - Token Mark entity masks (if available)\n4. Shows detailed Q&A predictions\n\n---\n\n## üî¥ REQUIREMENTS\n- Raw video path\n- Best model checkpoint on W&B",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 1: Setup & Clone\n# ==============================================================================\nimport os\nimport sys\n\nREPO_URL = \"https://github.com/DanielQH07/tranSTR_Casual.git\" \nREPO_NAME = \"tranSTR_Casual\"\nBRANCH = \"daniel_setmark\"\n\nif not os.path.exists(REPO_NAME):\n    print(f\"Cloning {REPO_URL}...\")\n    !git clone {REPO_URL} -b {BRANCH}\nelse:\n    print(\"Repo already exists.\")\n\n# Change Directory\nif os.path.basename(os.getcwd()) != \"causalvid\":\n    target_dir = os.path.join(os.getcwd(), REPO_NAME, \"causalvid\")\n    if os.path.exists(target_dir):\n        os.chdir(target_dir)\n    elif os.path.exists(REPO_NAME):\n        os.chdir(REPO_NAME)\nprint(f\"Working directory: {os.getcwd()}\")",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 2: Install & W&B Login\n# ==============================================================================\n!pip install -q wandb decord opencv-python matplotlib seaborn\nimport wandb\n\n# ============================================\n# üî¥ W&B CONFIG\n# ============================================\nWANDB_API_KEY = 'YOUR_WANDB_API_KEY_HERE'  # üî¥ UPDATE\nWANDB_PROJECT = 'transtr-causalvid'\nWANDB_ENTITY = None\n\n# Model artifact to load\nARTIFACT_NAME = 'best-model-som:latest'  # üî¥ UPDATE if needed\n\nwandb.login(key=WANDB_API_KEY, relogin=True)\nprint('‚úÖ W&B logged in!')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 3: Imports\n# ==============================================================================\nimport torch\nimport numpy as np\nimport pandas as pd\nimport json\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom einops import rearrange\n\ntry:\n    from decord import VideoReader, cpu\n    USE_DECORD = True\nexcept ImportError:\n    USE_DECORD = False\n    print('‚ö†Ô∏è decord not available, using OpenCV')\n\nfrom torch.utils.data import DataLoader\nfrom utils.util import set_seed, set_gpu_devices\nfrom DataLoader import VideoQADataset\nfrom networks.model import VideoQAmodel\n\nprint('‚úÖ Imports OK')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 4: Paths Configuration\n# ==============================================================================\nprint('=== CELL 4: Paths ===')\n\n# ============================================\n# üî¥ UPDATE THESE PATHS\n# ============================================\nVIT_FEATURE_PATH = '/kaggle/input/vit-features-full-merged'\nOBJ_FEATURE_PATH = '/kaggle/input/object-detection-causal-full'\nANNOTATION_PATH = '/kaggle/input/text-annotation/QA'\nSPLIT_DIR = '/kaggle/input/casual-vid-data-split/split'\nSOM_FEATURE_PATH = '/kaggle/input/causal-vqa-object-masks-full/obj_mask_causal_full'\n\n# üî¥ RAW VIDEO PATH - for visualization\nRAW_VIDEO_PATH = '/kaggle/input/causal-vid-qa-raw-videos/videos'  # üî¥ UPDATE\n\n# Verify paths\ndef verify_path(name, path):\n    if os.path.exists(path):\n        items = os.listdir(path)[:3]\n        print(f'‚úÖ {name}: {len(os.listdir(path))} items')\n        return True\n    else:\n        print(f'‚ùå {name}: NOT FOUND - {path}')\n        return False\n\nverify_path('ViT Features', VIT_FEATURE_PATH)\nverify_path('Object Features', OBJ_FEATURE_PATH)\nverify_path('Annotations', ANNOTATION_PATH)\nverify_path('SoM Masks', SOM_FEATURE_PATH)\nvideo_ok = verify_path('Raw Videos', RAW_VIDEO_PATH)\n\nif not video_ok:\n    print('\\n‚ö†Ô∏è Raw videos not found! Frame visualization will be limited.')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 5: Config & Device\n# ==============================================================================\nprint('=== CELL 5: Config ===')\n\nclass Config:\n    # Paths\n    video_feature_root = VIT_FEATURE_PATH\n    object_feature_path = OBJ_FEATURE_PATH\n    sample_list_path = ANNOTATION_PATH\n    split_dir_txt = SPLIT_DIR\n    som_feature_path = SOM_FEATURE_PATH\n    raw_video_path = RAW_VIDEO_PATH\n    \n    # Model architecture\n    topK_frame = 16\n    objs = 20\n    frames = 16\n    select_frames = 5\n    topK_obj = 12\n    frame_feat_dim = 1024\n    obj_feat_dim = 2053\n    d_model = 768\n    word_dim = 768\n    nheads = 8\n    num_encoder_layers = 2\n    num_decoder_layers = 2\n    normalize_before = True\n    activation = 'gelu'\n    dropout = 0.3\n    encoder_dropout = 0.3\n    \n    # Token Mark (SoM)\n    use_som = True\n    num_marks = 16\n    \n    # Text encoder\n    text_encoder_type = 'microsoft/deberta-base'\n    freeze_text_encoder = False\n    text_encoder_lr = 1e-5\n    text_pool_mode = 1\n    \n    # Eval\n    bs = 1  # Single sample for visualization\n    n_query = 5\n    gpu = 0\n    hard_eval = True  # Use hard topK for clear visualization\n    \n    pos_ratio = 1.0\n    neg_ratio = 1.0\n    a = 1.0\n\nargs = Config()\nset_gpu_devices(args.gpu)\nset_seed(999)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 6: Download Model from W&B\n# ==============================================================================\nprint('=== CELL 6: Download Model ===')\n\n# Initialize temp run to download\ntemp_run = wandb.init(\n    project=WANDB_PROJECT,\n    entity=WANDB_ENTITY,\n    job_type='inference',\n    name='inference-visualization',\n    reinit=True\n)\n\n# Download artifact\nprint(f'Downloading artifact: {ARTIFACT_NAME}')\ntry:\n    artifact = temp_run.use_artifact(ARTIFACT_NAME, type='model')\n    artifact_dir = artifact.download()\n    \n    ckpt_files = [f for f in os.listdir(artifact_dir) if f.endswith('.ckpt') or f.endswith('.pt')]\n    if ckpt_files:\n        CHECKPOINT_PATH = os.path.join(artifact_dir, ckpt_files[0])\n        print(f'‚úÖ Checkpoint: {CHECKPOINT_PATH}')\n        \n        # Get metadata\n        if artifact.metadata:\n            print(f\"   Epoch: {artifact.metadata.get('epoch', 'N/A')}\")\n            print(f\"   Val Acc: {artifact.metadata.get('val_acc', 'N/A'):.2f}%\")\n    else:\n        raise FileNotFoundError(\"No checkpoint file found\")\n        \nexcept Exception as e:\n    print(f'‚ùå Error: {e}')\n    CHECKPOINT_PATH = None",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 7: Create Test Dataset\n# ==============================================================================\nprint('=== CELL 7: Test Dataset ===')\n\ndef collate_fn_som(batch):\n    ff = torch.stack([item[0] for item in batch])\n    of = torch.stack([item[1] for item in batch])\n    qns = [item[2] for item in batch]\n    ans = [item[3] for item in batch]\n    ans_id = torch.tensor([item[4] for item in batch])\n    qns_key = [item[5] for item in batch]\n    som_data = [item[6] for item in batch]\n    return ff, of, qns, ans, ans_id, qns_key, som_data\n\ntest_ds = VideoQADataset(\n    split='test', n_query=args.n_query, obj_num=args.objs,\n    sample_list_path=args.sample_list_path,\n    video_feature_path=args.video_feature_root,\n    object_feature_path=args.object_feature_path,\n    split_dir=args.split_dir_txt, topK_frame=args.topK_frame,\n    max_samples=100, verbose=True,  # Limit for faster loading\n    som_feature_path=args.som_feature_path\n)\n\ntest_loader = DataLoader(\n    test_ds, batch_size=1, shuffle=False,\n    num_workers=0, collate_fn=collate_fn_som\n)\n\nprint(f'Test samples: {len(test_ds)}')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 8: Load Models (with and without SoM)\n# ==============================================================================\nprint('=== CELL 8: Load Models ===')\n\n# Create model WITH SoM\ncfg_som = {k: v for k, v in Config.__dict__.items() if not k.startswith('_')}\ncfg_som['device'] = device\ncfg_som['topK_frame'] = args.select_frames\ncfg_som['use_som'] = True\ncfg_som['num_marks'] = args.num_marks\ncfg_som['hard_eval'] = True  # For clear visualization\n\nmodel_som = VideoQAmodel(**cfg_som)\nmodel_som.to(device)\n\n# Create model WITHOUT SoM\ncfg_no_som = cfg_som.copy()\ncfg_no_som['use_som'] = False\n\nmodel_no_som = VideoQAmodel(**cfg_no_som)\nmodel_no_som.to(device)\n\n# Load weights\nif CHECKPOINT_PATH and os.path.exists(CHECKPOINT_PATH):\n    state_dict = torch.load(CHECKPOINT_PATH, map_location=device)\n    \n    # Load into SoM model\n    model_som.load_state_dict(state_dict)\n    print('‚úÖ Model WITH SoM loaded')\n    \n    # Load into non-SoM model (ignore som_injector keys)\n    filtered_state = {k: v for k, v in state_dict.items() if 'som_injector' not in k}\n    model_no_som.load_state_dict(filtered_state, strict=False)\n    print('‚úÖ Model WITHOUT SoM loaded (som_injector ignored)')\n    \nmodel_som.eval()\nmodel_no_som.eval()\n\nprint(f'\\nTotal params: {sum(p.numel() for p in model_som.parameters())/1e6:.1f}M')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 9: Video Frame Extraction Utilities\n# ==============================================================================\nprint('=== CELL 9: Video Utils ===')\n\ndef load_video_frames(video_path, num_frames=16):\n    \"\"\"Load uniformly sampled frames from video.\"\"\"\n    if not os.path.exists(video_path):\n        return None\n    \n    if USE_DECORD:\n        vr = VideoReader(video_path, ctx=cpu(0))\n        total_frames = len(vr)\n        indices = np.linspace(0, total_frames - 1, num_frames).astype(int)\n        frames = vr.get_batch(indices).asnumpy()  # [N, H, W, C]\n        return frames\n    else:\n        cap = cv2.VideoCapture(video_path)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        indices = np.linspace(0, total_frames - 1, num_frames).astype(int)\n        \n        frames = []\n        for idx in indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frames.append(frame)\n        cap.release()\n        return np.array(frames) if frames else None\n\ndef find_video_file(video_id, video_dir):\n    \"\"\"Find video file with various extensions.\"\"\"\n    if not os.path.exists(video_dir):\n        return None\n    \n    extensions = ['.mp4', '.avi', '.mkv', '.webm', '.mov']\n    for ext in extensions:\n        path = os.path.join(video_dir, f\"{video_id}{ext}\")\n        if os.path.exists(path):\n            return path\n    \n    # Try without extension match\n    for f in os.listdir(video_dir):\n        if f.startswith(video_id):\n            return os.path.join(video_dir, f)\n    \n    return None\n\ndef load_object_boxes(video_id, obj_feature_path, frame_idx=0):\n    \"\"\"Load object bounding boxes from feature files.\"\"\"\n    boxes_list = []\n    \n    for fidx in range(16):\n        feat_path = os.path.join(obj_feature_path, video_id, f'frame{fidx}.npy')\n        if os.path.exists(feat_path):\n            feat = np.load(feat_path)  # [N, 2053] where last 4 are [x1,y1,x2,y2] or similar\n            # Usually object features contain bbox info\n            # Format may vary - adjust based on actual data\n            boxes_list.append(feat)\n        else:\n            boxes_list.append(None)\n    \n    return boxes_list\n\nprint('‚úÖ Video utilities defined')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 10: Inference with Attention Extraction\n# ==============================================================================\nprint('=== CELL 10: Inference Functions ===')\n\ndef inference_with_attention(model, ff, of, qns, ans, som_data, device, use_som=True):\n    \"\"\"\n    Run inference and extract frame/object selection indices.\n    Returns prediction and selection info.\n    \"\"\"\n    model.eval()\n    B, F, O = of.size()[:3]\n    \n    with torch.no_grad():\n        ff = ff.to(device)\n        of = of.to(device)\n        \n        # Manual forward to extract indices\n        frame_feat = model.frame_resize(ff)\n        q_local, q_mask = model.forward_text(list(qns), device)\n        \n        frame_mask = torch.ones(B, F).bool().to(device)\n        frame_local, frame_att = model.frame_decoder(\n            frame_feat, q_local,\n            memory_key_padding_mask=q_mask,\n            query_pos=model.pos_encoder_1d(frame_mask, model.d_model),\n            output_attentions=True\n        )\n        \n        # Get frame selection indices\n        from networks.topk import HardtopK\n        idx_frame = rearrange(\n            HardtopK(frame_att.flatten(1,2), model.frame_topK), \n            'b (f q) k -> b f q k', f=F\n        ).sum(-2)  # [B, F, frame_topK]\n        \n        # Selected frame indices (which original frames were chosen)\n        selected_frame_indices = idx_frame[0].argmax(dim=0).cpu().numpy()  # [frame_topK]\n        frame_weights = idx_frame[0].sum(dim=1).cpu().numpy()  # [F] total weight per frame\n        \n        frame_local = (frame_local.transpose(1,2) @ idx_frame).transpose(1,2)\n        \n        # Object processing\n        obj_feat = (of.flatten(-2,-1).transpose(1,2) @ idx_frame).transpose(1,2)\n        obj_feat = obj_feat.view(B, model.frame_topK, O, -1)\n        obj_local = model.obj_resize(obj_feat)\n        \n        # Apply SoM if enabled\n        if use_som and hasattr(model, 'som_injector') and som_data[0] is not None:\n            frame_local, obj_local = model.som_injector(\n                frame_local, obj_local, som_data, idx_frame=idx_frame\n            )\n        \n        # Object selection\n        q_local_rep = q_local.repeat_interleave(model.frame_topK, dim=0)\n        q_mask_rep = q_mask.repeat_interleave(model.frame_topK, dim=0) if q_mask is not None else None\n        \n        obj_local_flat, obj_att = model.obj_decoder(\n            obj_local.flatten(0,1), q_local_rep,\n            memory_key_padding_mask=q_mask_rep,\n            output_attentions=True\n        )\n        \n        idx_obj = rearrange(\n            HardtopK(obj_att.flatten(1,2), model.obj_topK),\n            'b (o q) k -> b o q k', o=O\n        ).sum(-2)\n        \n        # Selected object indices per frame\n        selected_obj_indices = []\n        for f_idx in range(model.frame_topK):\n            obj_w = idx_obj[f_idx].sum(dim=1).cpu().numpy()  # [O]\n            top_objs = np.argsort(obj_w)[-model.obj_topK:][::-1]\n            selected_obj_indices.append(top_objs)\n        \n        # Full forward for prediction\n        if use_som and som_data[0] is not None:\n            out = model(ff, of, qns, ans, som_data=som_data)\n        else:\n            # Need to handle differently for no-som model\n            out = model(ff, of, qns, ans)\n        \n        pred = out.argmax(-1).item()\n        probs = torch.softmax(out, dim=-1)[0].cpu().numpy()\n        \n    return {\n        'pred': pred,\n        'probs': probs,\n        'selected_frames': selected_frame_indices,\n        'frame_weights': frame_weights,\n        'selected_objects': selected_obj_indices,\n    }\n\nprint('‚úÖ Inference function defined')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 11: Visualization Functions\n# ==============================================================================\nprint('=== CELL 11: Visualization ===')\n\ndef visualize_sample(sample_data, result_som, result_no_som, video_frames=None, som_masks=None):\n    \"\"\"\n    Create comprehensive visualization for a sample.\n    \"\"\"\n    qns_key = sample_data['qns_key']\n    question = sample_data['question']\n    answers = sample_data['answers']\n    correct_ans = sample_data['correct_ans']\n    \n    # Create figure\n    fig = plt.figure(figsize=(20, 16))\n    gs = GridSpec(4, 4, figure=fig, hspace=0.3, wspace=0.2)\n    \n    # Title\n    fig.suptitle(f\"Sample: {qns_key}\", fontsize=16, fontweight='bold')\n    \n    # ============================================\n    # Row 1: 16 Sampled Frames\n    # ============================================\n    if video_frames is not None:\n        for i in range(min(16, len(video_frames))):\n            row = i // 8\n            col = i % 8\n            ax = fig.add_subplot(gs[row, col // 2] if col < 4 else gs[row, col // 2])\n            \n            if row == 0:\n                ax = fig.add_subplot(4, 8, i + 1)\n            else:\n                ax = fig.add_subplot(4, 8, i + 1)\n            \n            ax.imshow(video_frames[i])\n            \n            # Highlight selected frames\n            if i in result_som['selected_frames']:\n                ax.patch.set_edgecolor('lime')\n                ax.patch.set_linewidth(4)\n                ax.set_title(f'F{i}‚úì', fontsize=8, color='lime')\n            else:\n                ax.set_title(f'F{i}', fontsize=8)\n            \n            ax.axis('off')\n    else:\n        ax = fig.add_subplot(gs[0:2, :])\n        ax.text(0.5, 0.5, 'Video frames not available', \n                ha='center', va='center', fontsize=14)\n        ax.axis('off')\n    \n    # ============================================\n    # Row 3: Q&A Info and Comparison\n    # ============================================\n    ax_qa = fig.add_subplot(gs[2, :2])\n    ax_qa.axis('off')\n    \n    qa_text = f\"\"\"QUESTION:\n{question}\n\nANSWERS:\n\"\"\"\n    for i, ans in enumerate(answers):\n        marker = ''\n        if i == correct_ans:\n            marker = ' ‚úì (correct)'\n        if i == result_som['pred']:\n            marker += ' ‚Üê SoM pred'\n        if i == result_no_som['pred']:\n            marker += ' ‚Üê No-SoM pred'\n        qa_text += f\"  [{i}] {ans}{marker}\\n\"\n    \n    ax_qa.text(0.02, 0.98, qa_text, transform=ax_qa.transAxes,\n               fontsize=10, verticalalignment='top', fontfamily='monospace',\n               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    # ============================================\n    # Row 3: Prediction Comparison\n    # ============================================\n    ax_comp = fig.add_subplot(gs[2, 2:])\n    \n    x = np.arange(5)\n    width = 0.35\n    \n    bars1 = ax_comp.bar(x - width/2, result_som['probs'], width, label='With SoM', color='green', alpha=0.7)\n    bars2 = ax_comp.bar(x + width/2, result_no_som['probs'], width, label='Without SoM', color='red', alpha=0.7)\n    \n    ax_comp.set_ylabel('Probability')\n    ax_comp.set_xlabel('Answer Option')\n    ax_comp.set_title('Prediction Probabilities')\n    ax_comp.set_xticks(x)\n    ax_comp.set_xticklabels(['A0', 'A1', 'A2', 'A3', 'A4'])\n    ax_comp.legend()\n    ax_comp.axhline(y=0.2, color='gray', linestyle='--', alpha=0.5)\n    \n    # Highlight correct answer\n    ax_comp.get_xticklabels()[correct_ans].set_color('blue')\n    ax_comp.get_xticklabels()[correct_ans].set_fontweight('bold')\n    \n    # ============================================\n    # Row 4: Selected Frames Detail\n    # ============================================\n    ax_sel = fig.add_subplot(gs[3, :2])\n    \n    # Frame attention weights\n    frame_w = result_som['frame_weights']\n    colors = ['lime' if i in result_som['selected_frames'] else 'gray' for i in range(16)]\n    ax_sel.bar(range(16), frame_w, color=colors)\n    ax_sel.set_xlabel('Frame Index')\n    ax_sel.set_ylabel('Attention Weight')\n    ax_sel.set_title(f'Frame Selection (Top {len(result_som[\"selected_frames\"])} selected in green)')\n    ax_sel.set_xticks(range(16))\n    \n    # ============================================\n    # Row 4: Summary\n    # ============================================\n    ax_sum = fig.add_subplot(gs[3, 2:])\n    ax_sum.axis('off')\n    \n    som_correct = result_som['pred'] == correct_ans\n    no_som_correct = result_no_som['pred'] == correct_ans\n    \n    summary = f\"\"\"RESULTS SUMMARY\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nWith Token Mark (SoM):\n  Prediction: {result_som['pred']} {'‚úÖ CORRECT' if som_correct else '‚ùå WRONG'}\n  Confidence: {result_som['probs'][result_som['pred']]*100:.1f}%\n  Selected Frames: {list(result_som['selected_frames'])}\n\nWithout Token Mark:\n  Prediction: {result_no_som['pred']} {'‚úÖ CORRECT' if no_som_correct else '‚ùå WRONG'}\n  Confidence: {result_no_som['probs'][result_no_som['pred']]*100:.1f}%\n  Selected Frames: {list(result_no_som['selected_frames'])}\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nCorrect Answer: {correct_ans}\n\"\"\"\n    \n    ax_sum.text(0.02, 0.98, summary, transform=ax_sum.transAxes,\n               fontsize=10, verticalalignment='top', fontfamily='monospace',\n               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n    \n    plt.tight_layout()\n    return fig\n\nprint('‚úÖ Visualization function defined')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 12: Run Inference on Selected Samples\n# ==============================================================================\nprint('=== CELL 12: Run Inference ===')\n\n# Get annotation data for question/answer text\nannotation_files = {\n    'descriptive': os.path.join(ANNOTATION_PATH, 'descriptive.csv'),\n    'explanatory': os.path.join(ANNOTATION_PATH, 'explanatory.csv'),\n    'predictive': os.path.join(ANNOTATION_PATH, 'predictive.csv'),\n    'counterfactual': os.path.join(ANNOTATION_PATH, 'counterfactual.csv'),\n}\n\n# Load annotations\nannotations = {}\nfor qtype, path in annotation_files.items():\n    if os.path.exists(path):\n        annotations[qtype] = pd.read_csv(path)\n        print(f'Loaded {qtype}: {len(annotations[qtype])} samples')\n\ndef get_sample_info(video_id, qtype):\n    \"\"\"Get question and answers from annotation.\"\"\"\n    qtype_map = {\n        'descriptive': 'descriptive',\n        'explanatory': 'explanatory', \n        'predictive': 'predictive',\n        'predictive_reason': 'predictive',\n        'counterfactual': 'counterfactual',\n        'counterfactual_reason': 'counterfactual'\n    }\n    \n    df_key = qtype_map.get(qtype, qtype)\n    if df_key in annotations:\n        df = annotations[df_key]\n        row = df[df['video_id'] == int(video_id)]\n        if len(row) > 0:\n            row = row.iloc[0]\n            return {\n                'question': row.get('question', 'N/A'),\n                'answers': [row.get(f'a{i}', f'Option {i}') for i in range(5)],\n                'correct': row.get('answer', 0)\n            }\n    return None\n\nprint('\\n‚úÖ Annotation data loaded')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 13: Process Multiple Samples\n# ==============================================================================\nprint('=== CELL 13: Process Samples ===')\n\nNUM_SAMPLES = 5  # Number of samples to visualize\nresults_list = []\n\nfor idx, batch in enumerate(tqdm(test_loader, total=NUM_SAMPLES)):\n    if idx >= NUM_SAMPLES:\n        break\n    \n    ff, of, qns, ans, ans_id, qns_key, som_data = batch\n    qns_key = qns_key[0]\n    \n    # Parse video_id and question type\n    parts = qns_key.rsplit('_', 1)\n    video_id = parts[0] if len(parts) > 1 else qns_key\n    qtype = parts[1] if len(parts) > 1 else 'unknown'\n    \n    print(f'\\n--- Sample {idx+1}: {qns_key} ---')\n    print(f'Video ID: {video_id}, Type: {qtype}')\n    \n    # Get Q&A info\n    qa_info = get_sample_info(video_id, qtype)\n    if qa_info:\n        question = qa_info['question']\n        answers = qa_info['answers']\n    else:\n        question = qns[0]\n        answers = ans[0] if isinstance(ans[0], list) else ['N/A'] * 5\n    \n    # Run inference with SoM\n    result_som = inference_with_attention(\n        model_som, ff, of, qns, ans, som_data, device, use_som=True\n    )\n    \n    # Run inference without SoM\n    result_no_som = inference_with_attention(\n        model_no_som, ff, of, qns, ans, [None], device, use_som=False\n    )\n    \n    # Load video frames\n    video_path = find_video_file(video_id, RAW_VIDEO_PATH)\n    video_frames = load_video_frames(video_path) if video_path else None\n    \n    # Get SoM masks if available\n    som_masks = som_data[0].get('frame_masks', {}) if som_data[0] else None\n    entity_names = som_data[0].get('entity_names', {}) if som_data[0] else None\n    \n    # Store results\n    sample_data = {\n        'idx': idx,\n        'qns_key': qns_key,\n        'video_id': video_id,\n        'qtype': qtype,\n        'question': question,\n        'answers': answers,\n        'correct_ans': ans_id[0].item(),\n        'entity_names': entity_names,\n    }\n    \n    results_list.append({\n        'sample_data': sample_data,\n        'result_som': result_som,\n        'result_no_som': result_no_som,\n        'video_frames': video_frames,\n        'som_masks': som_masks\n    })\n    \n    # Print quick summary\n    print(f'Question: {question[:80]}...' if len(question) > 80 else f'Question: {question}')\n    print(f'Correct: {ans_id[0].item()}, SoM pred: {result_som[\"pred\"]}, No-SoM pred: {result_no_som[\"pred\"]}')\n    print(f'SoM correct: {result_som[\"pred\"] == ans_id[0].item()}, No-SoM correct: {result_no_som[\"pred\"] == ans_id[0].item()}')\n    if entity_names:\n        print(f'Entities: {entity_names}')\n\nprint(f'\\n‚úÖ Processed {len(results_list)} samples')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 14: Visualize All Samples\n# ==============================================================================\nprint('=== CELL 14: Visualizations ===')\n\nfor i, res in enumerate(results_list):\n    print(f'\\nüìä Visualizing sample {i+1}/{len(results_list)}: {res[\"sample_data\"][\"qns_key\"]}')\n    \n    fig = visualize_sample(\n        res['sample_data'],\n        res['result_som'],\n        res['result_no_som'],\n        res['video_frames'],\n        res['som_masks']\n    )\n    \n    # Save figure\n    fig.savefig(f'inference_sample_{i+1}.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    plt.close(fig)\n\nprint('\\n‚úÖ All visualizations complete!')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 15: Summary Statistics\n# ==============================================================================\nprint('=== CELL 15: Summary ===')\n\nsom_correct = sum(1 for r in results_list if r['result_som']['pred'] == r['sample_data']['correct_ans'])\nno_som_correct = sum(1 for r in results_list if r['result_no_som']['pred'] == r['sample_data']['correct_ans'])\ntotal = len(results_list)\n\nprint('\\n' + '='*60)\nprint('INFERENCE SUMMARY')\nprint('='*60)\nprint(f'Total samples: {total}')\nprint(f'\\nWith Token Mark (SoM):')\nprint(f'  Correct: {som_correct}/{total} ({som_correct/total*100:.1f}%)')\nprint(f'\\nWithout Token Mark:')\nprint(f'  Correct: {no_som_correct}/{total} ({no_som_correct/total*100:.1f}%)')\nprint('='*60)\n\n# Detailed breakdown\nprint('\\nPer-sample breakdown:')\nprint('-'*60)\nfor i, res in enumerate(results_list):\n    som_ok = '‚úÖ' if res['result_som']['pred'] == res['sample_data']['correct_ans'] else '‚ùå'\n    no_som_ok = '‚úÖ' if res['result_no_som']['pred'] == res['sample_data']['correct_ans'] else '‚ùå'\n    print(f\"{i+1}. {res['sample_data']['qns_key'][:30]:<30} SoM:{som_ok} No-SoM:{no_som_ok}\")\n\n# Log to W&B\nwandb.log({\n    'inference/som_accuracy': som_correct/total*100,\n    'inference/no_som_accuracy': no_som_correct/total*100,\n    'inference/samples': total\n})\n\nwandb.finish()\nprint('\\n‚úÖ Done!')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# ==============================================================================\n# CELL 16: Display 16 Frames with Entity Masks (if available)\n# ==============================================================================\nprint('=== CELL 16: Entity Mask Visualization ===')\n\n# Select a sample with SoM data\nsample_with_som = None\nfor res in results_list:\n    if res['som_masks'] and res['video_frames'] is not None:\n        sample_with_som = res\n        break\n\nif sample_with_som:\n    print(f\"Visualizing entity masks for: {sample_with_som['sample_data']['qns_key']}\")\n    \n    frames = sample_with_som['video_frames']\n    masks = sample_with_som['som_masks']\n    entities = sample_with_som['sample_data'].get('entity_names', {})\n    \n    fig, axes = plt.subplots(4, 8, figsize=(24, 12))\n    axes = axes.flatten()\n    \n    # Define colors for entities\n    cmap = plt.cm.get_cmap('tab10')\n    \n    for i in range(16):\n        ax = axes[i]\n        \n        if i < len(frames):\n            ax.imshow(frames[i])\n            \n            # Overlay mask if available\n            if i in masks:\n                mask = masks[i].numpy()\n                # Create colored overlay\n                overlay = np.zeros((*mask.shape, 4))\n                for entity_id in np.unique(mask):\n                    if entity_id > 0:  # Skip background\n                        color = cmap(entity_id % 10)\n                        entity_mask = mask == entity_id\n                        overlay[entity_mask] = [*color[:3], 0.4]  # RGBA with alpha\n                \n                ax.imshow(overlay)\n        \n        # Highlight selected frames\n        selected_frames = sample_with_som['result_som']['selected_frames']\n        if i in selected_frames:\n            for spine in ax.spines.values():\n                spine.set_edgecolor('lime')\n                spine.set_linewidth(4)\n            ax.set_title(f'F{i} ‚úì', fontsize=10, color='lime', fontweight='bold')\n        else:\n            ax.set_title(f'F{i}', fontsize=10)\n        \n        ax.axis('off')\n    \n    # Legend\n    if entities:\n        legend_text = 'Entities: ' + ', '.join([f'{k}:{v}' for k, v in entities.items()])\n        fig.text(0.5, 0.02, legend_text, ha='center', fontsize=12, \n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    plt.suptitle(f\"16 Frames with Entity Masks - {sample_with_som['sample_data']['qns_key']}\", \n                 fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig('frames_with_masks.png', dpi=150, bbox_inches='tight')\n    plt.show()\nelse:\n    print('No sample with both video frames and SoM masks found.')\n\nprint('\\n‚úÖ Entity mask visualization complete!')",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}